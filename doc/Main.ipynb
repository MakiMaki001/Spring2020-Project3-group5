{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy.io import loadmat\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load():\n",
    "    def load_data(filename):\n",
    "        raw_data = pd.read_csv(filename)\n",
    "        raw_data['filename'] = [str(i).zfill(4)+'.jpg' for i in raw_data['Index'].tolist()]\n",
    "        raw_data['pointsname'] = [str(i).zfill(4)+'.mat' for i in raw_data['Index'].tolist()]\n",
    "        return raw_data\n",
    "\n",
    "    #read points data from mat data\n",
    "    def load_points(points_path,data):\n",
    "        n = data.shape[0]\n",
    "        points_data = np.zeros([n,3003,2])\n",
    "        start_time = time.time()\n",
    "        for i in range(n):\n",
    "            result = loadmat(points_path+data['pointsname'][i])\n",
    "            key = sorted(result.keys())[-1]\n",
    "            points = result[key]\n",
    "            distance_h = []\n",
    "            distance_v = []\n",
    "            for d in range(points.shape[0]-1):\n",
    "                for j in range(d+1,points.shape[0]):\n",
    "                    distance_h.append(abs(points[d,0]-points[j,0]))\n",
    "                    distance_v.append(abs(points[d,1]-points[j,1]))\n",
    "\n",
    "            points_data[i,:,0]=distance_h\n",
    "            points_data[i,:,1]=distance_v\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        return points_data.reshape([2500,6006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "--- 15.889317274093628 seconds ---\n"
=======
      "--- 35.73706340789795 seconds ---\n"
>>>>>>> 555b9f2c82e2dedff59d52efe835de5f65a4e4ed
     ]
    }
   ],
   "source": [
    "path = '/Users/zhaoziqin/Desktop/train_set/'  # Please modify your own path\n",
    "data = load.load_data(path+'label.csv')\n",
    "points_path = '/Users/zhaoziqin/Desktop/train_set/points/'\n",
    "X = load.load_points(points_path,data)\n",
    "X = np.round(X,0)\n",
    "y= data['emotion_idx'].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y=train_test_split(X,y,test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 6006)\n",
      "(500, 6006)\n",
      "(2000,)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Baseline Model\n",
    "\n",
    "We used Cross-Validation to find the more efficient estimator combination for baseline model and fitted by using this combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2388.476354122162 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "baseline = GradientBoostingClassifier(n_estimators=100,max_depth= 1,learning_rate=0.1)\n",
    "gbm_model = baseline.fit(train_x,train_y) \n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on the baseline model: 0.8380\n",
      "Testing Accuracy on the baseline model: 0.3820\n",
      "Prediction on Baseline: 0.048870086669921875 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.87      0.97      0.91        94\n",
      "           2       0.93      0.99      0.96        95\n",
      "           3       0.80      0.90      0.85       110\n",
      "           4       0.80      0.82      0.81        97\n",
      "           5       0.90      0.95      0.92        91\n",
      "           6       0.81      0.83      0.82        78\n",
      "           7       0.88      0.88      0.88        94\n",
      "           8       0.94      0.95      0.95       104\n",
      "           9       0.91      0.95      0.93        84\n",
      "          10       0.79      0.77      0.78        87\n",
      "          11       0.89      0.82      0.85        99\n",
      "          12       0.85      0.75      0.79        80\n",
      "          13       0.81      0.65      0.72        83\n",
      "          14       0.78      0.90      0.84        99\n",
      "          15       0.86      0.77      0.81        73\n",
      "          16       0.82      0.88      0.85        91\n",
      "          17       0.79      0.83      0.81       101\n",
      "          18       0.90      0.77      0.83        84\n",
      "          19       0.70      0.75      0.73        88\n",
      "          20       0.74      0.74      0.74        95\n",
      "          21       0.86      0.78      0.82        83\n",
      "          22       0.85      0.69      0.76        90\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.83      0.83      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.67      0.51        18\n",
      "           2       0.59      0.68      0.63        19\n",
      "           3       0.24      0.36      0.29        25\n",
      "           4       0.38      0.57      0.45        21\n",
      "           5       0.48      0.56      0.51        18\n",
      "           6       0.47      0.35      0.40        26\n",
      "           7       0.42      0.55      0.48        20\n",
      "           8       0.67      0.62      0.65        16\n",
      "           9       0.73      0.44      0.55        25\n",
      "          10       0.44      0.35      0.39        20\n",
      "          11       0.48      0.54      0.51        24\n",
      "          12       0.32      0.22      0.26        32\n",
      "          13       0.24      0.21      0.22        24\n",
      "          14       0.43      0.52      0.47        23\n",
      "          15       0.31      0.23      0.26        22\n",
      "          16       0.55      0.50      0.52        22\n",
      "          17       0.24      0.27      0.25        26\n",
      "          18       0.25      0.23      0.24        22\n",
      "          19       0.28      0.43      0.34        23\n",
      "          20       0.20      0.20      0.20        20\n",
      "          21       0.32      0.21      0.25        34\n",
      "          22       0.17      0.05      0.08        20\n",
      "\n",
      "    accuracy                           0.38       500\n",
      "   macro avg       0.39      0.40      0.38       500\n",
      "weighted avg       0.38      0.38      0.37       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_train_accuracy = gbm_model.score(train_x,train_y)\n",
    "baseline_test_accuracy = gbm_model.score(test_x,test_y)\n",
    "print(\"Training Accuracy on the baseline model: %.4f\" % (baseline_train_accuracy))\n",
    "print(\"Testing Accuracy on the baseline model: %.4f\" % (baseline_test_accuracy))\n",
    "\n",
    "baseline_pred_train = gbm_model.predict(train_x)\n",
    "t1 = time.time()\n",
    "baseline_pred_test = gbm_model.predict(test_x)\n",
    "print(\"Prediction on Baseline: %s seconds\" % (time.time() - t1))\n",
    "\n",
    "print(classification_report(train_y, baseline_pred_train))\n",
    "print(classification_report(test_y, baseline_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21f851c50b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_baseline = confusion_matrix(test_y,baseline_pred_test)\n",
    "sns.heatmap(cm_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 : Candidate Advanced Model --- Part I\n",
    "\n",
    "For our advanced model, we chose different models to fit the train dataset. Here is the following models we picked as our candidate advanced models firstly.\n",
    "\n",
    "    1. XGBoost\n",
    "    2. Logistic Regression\n",
    "    3. Support Vector Machine (SVM)\n",
    "    4. LDA\n",
    "    5. Random Forest\n",
    "\n",
    "For each of these models, We have different python files for each model which contains the completed process. In each of model, we used Cross-Validation to find the best parameter combination and fit the training set.\n",
    "\n",
    "\n",
    "### 1. XGBoost\n",
    "\n",
    "We used GridSearch cross-validation to find the best parameter combination.\n",
    "\n",
    "'max_depth': range (1, 5, 1)\n",
    "\n",
    "'n_estimators': range(1, 200, 20)\n",
    "\n",
    "'learning_rate': [0.1, 0.01, 0.05]\n",
    "\n",
    "Results: Best Max Depth: 2 / Best N.estimators: 181 / Best Learning Rate: 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1064.7244787216187 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xgb = XGBClassifier(n_estimators = 181,max_depth=2,learning_rate=0.1)\n",
    "xgb_model = xgb.fit(train_x,train_y)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on the xgb model: 1.0000\n",
      "Testing Accuracy on the xgb model: 0.4920\n",
      "Prediction on XGBoost: 0.7849006652832031 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        94\n",
      "           2       1.00      1.00      1.00        95\n",
      "           3       1.00      1.00      1.00       110\n",
      "           4       1.00      1.00      1.00        97\n",
      "           5       1.00      1.00      1.00        91\n",
      "           6       1.00      1.00      1.00        78\n",
      "           7       1.00      1.00      1.00        94\n",
      "           8       1.00      1.00      1.00       104\n",
      "           9       1.00      1.00      1.00        84\n",
      "          10       1.00      1.00      1.00        87\n",
      "          11       1.00      1.00      1.00        99\n",
      "          12       1.00      1.00      1.00        80\n",
      "          13       1.00      1.00      1.00        83\n",
      "          14       1.00      1.00      1.00        99\n",
      "          15       1.00      1.00      1.00        73\n",
      "          16       1.00      1.00      1.00        91\n",
      "          17       1.00      1.00      1.00       101\n",
      "          18       1.00      1.00      1.00        84\n",
      "          19       1.00      1.00      1.00        88\n",
      "          20       1.00      1.00      1.00        95\n",
      "          21       1.00      1.00      1.00        83\n",
      "          22       1.00      1.00      1.00        90\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.83      0.68        18\n",
      "           2       0.63      0.63      0.63        19\n",
      "           3       0.46      0.52      0.49        25\n",
      "           4       0.44      0.52      0.48        21\n",
      "           5       0.50      0.61      0.55        18\n",
      "           6       0.73      0.42      0.54        26\n",
      "           7       0.47      0.40      0.43        20\n",
      "           8       0.75      0.75      0.75        16\n",
      "           9       0.68      0.52      0.59        25\n",
      "          10       0.55      0.60      0.57        20\n",
      "          11       0.52      0.58      0.55        24\n",
      "          12       0.43      0.41      0.42        32\n",
      "          13       0.29      0.29      0.29        24\n",
      "          14       0.60      0.65      0.63        23\n",
      "          15       0.56      0.41      0.47        22\n",
      "          16       0.65      0.77      0.71        22\n",
      "          17       0.42      0.50      0.46        26\n",
      "          18       0.50      0.32      0.39        22\n",
      "          19       0.36      0.57      0.44        23\n",
      "          20       0.27      0.35      0.30        20\n",
      "          21       0.46      0.32      0.38        34\n",
      "          22       0.17      0.10      0.12        20\n",
      "\n",
      "    accuracy                           0.49       500\n",
      "   macro avg       0.50      0.50      0.49       500\n",
      "weighted avg       0.50      0.49      0.49       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_train_accuracy = xgb_model.score(train_x,train_y)\n",
    "xgb_test_accuracy = xgb_model.score(test_x,test_y)\n",
    "print(\"Training Accuracy on the xgb model: %.4f\" % (xgb_train_accuracy))\n",
    "print(\"Testing Accuracy on the xgb model: %.4f\" % (xgb_test_accuracy))\n",
    "\n",
    "xgb_pred_train = xgb_model.predict(train_x)\n",
    "t1 = time.time()\n",
    "xgb_pred_test = xgb_model.predict(test_x)\n",
    "print(\"Prediction on XGBoost: %s seconds\" % (time.time() - t1))\n",
    "\n",
    "print(classification_report(train_y, xgb_pred_train))\n",
    "print(classification_report(test_y, xgb_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression\n",
    "\n",
    "#### We used GridSearchCV to fine the best parameter combination. \n",
    "\n",
    "\n",
    "'C': [0.001,0.01, 1, 25,50,100]\n",
    "\n",
    "Result:\n",
    "\n",
    "'C': 0.01, 'dual': False. 'fit_intercept': True, 'intercept_scaling': 1, 'max_iter': 300, 'multi_class': 'multinomial', 'penalty': 'l2', 'solver': 'lbfgs', 'tol': 0.0001}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 17,
>>>>>>> 555b9f2c82e2dedff59d52efe835de5f65a4e4ed
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "--- 14.794970989227295 seconds ---\n"
=======
      "--- 37.261882305145264 seconds ---\n"
>>>>>>> 555b9f2c82e2dedff59d52efe835de5f65a4e4ed
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "lr = LogisticRegression(C=0.01, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, max_iter=300,\n",
    "                   multi_class='multinomial', penalty='l2',\n",
    "                   solver='lbfgs', tol=0.0001)\n",
    "lr_model = lr.fit(train_x,train_y)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on the logistic regression model: 0.7630\n",
      "Testing Accuracy on the logistic regression model: 0.5120\n",
      "Prediction on Logistic Regression: 0.014961481094360352 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.91      0.91        94\n",
      "           2       0.93      0.97      0.95        95\n",
      "           3       0.82      0.84      0.83       110\n",
      "           4       0.71      0.72      0.72        97\n",
      "           5       0.86      0.86      0.86        91\n",
      "           6       0.71      0.73      0.72        78\n",
      "           7       0.76      0.77      0.76        94\n",
      "           8       0.95      0.93      0.94       104\n",
      "           9       0.95      0.92      0.93        84\n",
      "          10       0.64      0.62      0.63        87\n",
      "          11       0.78      0.78      0.78        99\n",
      "          12       0.67      0.66      0.67        80\n",
      "          13       0.61      0.59      0.60        83\n",
      "          14       0.77      0.84      0.80        99\n",
      "          15       0.78      0.68      0.73        73\n",
      "          16       0.82      0.82      0.82        91\n",
      "          17       0.77      0.73      0.75       101\n",
      "          18       0.70      0.75      0.72        84\n",
      "          19       0.60      0.59      0.59        88\n",
      "          20       0.68      0.62      0.65        95\n",
      "          21       0.70      0.69      0.69        83\n",
      "          22       0.60      0.66      0.63        90\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.76      0.76      0.76      2000\n",
      "weighted avg       0.76      0.76      0.76      2000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.78      0.72        18\n",
      "           2       0.73      0.58      0.65        19\n",
      "           3       0.50      0.56      0.53        25\n",
      "           4       0.46      0.52      0.49        21\n",
      "           5       0.55      0.89      0.68        18\n",
      "           6       0.54      0.54      0.54        26\n",
      "           7       0.52      0.60      0.56        20\n",
      "           8       0.65      0.81      0.72        16\n",
      "           9       0.86      0.72      0.78        25\n",
      "          10       0.43      0.50      0.47        20\n",
      "          11       0.55      0.46      0.50        24\n",
      "          12       0.41      0.34      0.37        32\n",
      "          13       0.38      0.38      0.38        24\n",
      "          14       0.60      0.65      0.63        23\n",
      "          15       0.57      0.55      0.56        22\n",
      "          16       0.57      0.73      0.64        22\n",
      "          17       0.58      0.54      0.56        26\n",
      "          18       0.57      0.36      0.44        22\n",
      "          19       0.20      0.17      0.19        23\n",
      "          20       0.30      0.35      0.33        20\n",
      "          21       0.57      0.38      0.46        34\n",
      "          22       0.14      0.15      0.15        20\n",
      "\n",
      "    accuracy                           0.51       500\n",
      "   macro avg       0.52      0.53      0.51       500\n",
      "weighted avg       0.52      0.51      0.51       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_train_accuracy = lr_model.score(train_x,train_y)\n",
    "lr_test_accuracy = lr_model.score(test_x,test_y)\n",
    "print(\"Training Accuracy on the logistic regression model: %.4f\" % (lr_train_accuracy))\n",
    "print(\"Testing Accuracy on the logistic regression model: %.4f\" % (lr_test_accuracy))\n",
    "\n",
    "lr_pred_train = lr_model.predict(train_x)\n",
    "t1 = time.time()\n",
    "lr_pred_test = lr_model.predict(test_x)\n",
    "print(\"Prediction on Logistic Regression: %s seconds\" % (time.time() - t1))\n",
    "\n",
    "print(classification_report(train_y, lr_pred_train))\n",
    "print(classification_report(test_y, lr_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machine (SVM)\n",
    "\n",
    "**We used GridSearchCV to find the best parameter combination of SVM.**\n",
    "\n",
    "param= {'C': [0.0000001,0.000001,0.00001,0.0001,0.001,0.01,1]}\n",
    "\n",
    "Result: 'C':0.00001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 55.175928592681885 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "svc = SVC(kernel= 'linear', random_state = 123, C = 0.00001)\n",
    "svm = svc.fit(train_x,train_y)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on the logistic regression model: 0.8145\n",
      "Testing Accuracy on the logistic regression model: 0.4840\n",
      "Prediction on SVM: 10.659993171691895 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.97      0.92        94\n",
      "           2       0.96      0.96      0.96        95\n",
      "           3       0.87      0.82      0.85       110\n",
      "           4       0.75      0.81      0.78        97\n",
      "           5       0.90      0.95      0.92        91\n",
      "           6       0.79      0.78      0.79        78\n",
      "           7       0.87      0.87      0.87        94\n",
      "           8       0.94      0.98      0.96       104\n",
      "           9       0.93      0.95      0.94        84\n",
      "          10       0.70      0.79      0.74        87\n",
      "          11       0.86      0.76      0.81        99\n",
      "          12       0.66      0.71      0.69        80\n",
      "          13       0.65      0.61      0.63        83\n",
      "          14       0.75      0.89      0.81        99\n",
      "          15       0.85      0.71      0.78        73\n",
      "          16       0.89      0.89      0.89        91\n",
      "          17       0.81      0.78      0.79       101\n",
      "          18       0.82      0.74      0.78        84\n",
      "          19       0.69      0.70      0.70        88\n",
      "          20       0.82      0.68      0.75        95\n",
      "          21       0.72      0.78      0.75        83\n",
      "          22       0.78      0.68      0.73        90\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.81      0.81      0.81      2000\n",
      "weighted avg       0.82      0.81      0.81      2000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.72      0.65        18\n",
      "           2       0.68      0.68      0.68        19\n",
      "           3       0.40      0.56      0.47        25\n",
      "           4       0.41      0.57      0.48        21\n",
      "           5       0.54      0.83      0.65        18\n",
      "           6       0.50      0.46      0.48        26\n",
      "           7       0.45      0.45      0.45        20\n",
      "           8       0.59      0.62      0.61        16\n",
      "           9       0.78      0.72      0.75        25\n",
      "          10       0.38      0.40      0.39        20\n",
      "          11       0.55      0.46      0.50        24\n",
      "          12       0.54      0.44      0.48        32\n",
      "          13       0.36      0.33      0.35        24\n",
      "          14       0.62      0.70      0.65        23\n",
      "          15       0.65      0.59      0.62        22\n",
      "          16       0.48      0.59      0.53        22\n",
      "          17       0.50      0.46      0.48        26\n",
      "          18       0.37      0.32      0.34        22\n",
      "          19       0.17      0.17      0.17        23\n",
      "          20       0.33      0.30      0.32        20\n",
      "          21       0.52      0.35      0.42        34\n",
      "          22       0.14      0.10      0.12        20\n",
      "\n",
      "    accuracy                           0.48       500\n",
      "   macro avg       0.48      0.49      0.48       500\n",
      "weighted avg       0.48      0.48      0.48       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_train_accuracy = svm.score(train_x,train_y)\n",
    "svm_test_accuracy = svm.score(test_x,test_y)\n",
    "print(\"Training Accuracy on the logistic regression model: %.4f\" % (svm_train_accuracy))\n",
    "print(\"Testing Accuracy on the logistic regression model: %.4f\" % (svm_test_accuracy))\n",
    "\n",
    "svm_pred_train = svm.predict(train_x)\n",
    "t1 = time.time()\n",
    "svm_pred_test = svm.predict(test_x)\n",
    "print(\"Prediction on SVM: %s seconds\" % (time.time() - t1))\n",
    "\n",
    "print(classification_report(train_y, svm_pred_train))\n",
    "print(classification_report(test_y, svm_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Linear Discriminant Analysis (LDA)\n",
    "\n",
    "While LDA is often times used for dimensionality reduction, it can also be a powerful classifying model which performed quite well with the image data. \n",
    "\n",
    "For the LDA we used the following key parameters: \n",
    "\n",
    "**n_components: 2**\n",
    "This determines the amount of features that will be created by LDA.\n",
    "\n",
    "**shrinkage:.1** \n",
    "This determines the steps taken by the model to improve covariance matrix estimation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 133.1860227584839 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lda = LDA(solver='eigen', shrinkage=.1, n_components=2)\n",
    "lda_model = lda.fit(train_x, train_y)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on the LDA model: 0.8580\n",
      "Testing Accuracy on the LDA model: 0.5380\n",
      "Prediction on LDA: 0.019947290420532227 minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.99      0.95        94\n",
      "           2       0.93      0.96      0.94        95\n",
      "           3       0.87      0.89      0.88       110\n",
      "           4       0.86      0.84      0.85        97\n",
      "           5       0.96      0.90      0.93        91\n",
      "           6       0.84      0.82      0.83        78\n",
      "           7       0.90      0.87      0.89        94\n",
      "           8       0.95      0.92      0.94       104\n",
      "           9       0.94      0.96      0.95        84\n",
      "          10       0.75      0.86      0.80        87\n",
      "          11       0.96      0.70      0.81        99\n",
      "          12       0.79      0.82      0.80        80\n",
      "          13       0.75      0.81      0.78        83\n",
      "          14       0.84      0.86      0.85        99\n",
      "          15       0.88      0.77      0.82        73\n",
      "          16       0.95      0.84      0.89        91\n",
      "          17       0.89      0.81      0.85       101\n",
      "          18       0.81      0.88      0.85        84\n",
      "          19       0.74      0.89      0.81        88\n",
      "          20       0.86      0.77      0.81        95\n",
      "          21       0.78      0.88      0.83        83\n",
      "          22       0.76      0.82      0.79        90\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.94      0.74        18\n",
      "           2       0.79      0.58      0.67        19\n",
      "           3       0.58      0.60      0.59        25\n",
      "           4       0.50      0.57      0.53        21\n",
      "           5       0.85      0.61      0.71        18\n",
      "           6       0.64      0.54      0.58        26\n",
      "           7       0.60      0.60      0.60        20\n",
      "           8       0.68      0.81      0.74        16\n",
      "           9       0.79      0.88      0.83        25\n",
      "          10       0.60      0.60      0.60        20\n",
      "          11       0.77      0.42      0.54        24\n",
      "          12       0.43      0.50      0.46        32\n",
      "          13       0.37      0.46      0.41        24\n",
      "          14       0.54      0.57      0.55        23\n",
      "          15       0.52      0.50      0.51        22\n",
      "          16       0.73      0.73      0.73        22\n",
      "          17       0.55      0.46      0.50        26\n",
      "          18       0.50      0.45      0.48        22\n",
      "          19       0.25      0.35      0.29        23\n",
      "          20       0.33      0.40      0.36        20\n",
      "          21       0.52      0.35      0.42        34\n",
      "          22       0.17      0.15      0.16        20\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.56      0.55      0.55       500\n",
      "weighted avg       0.55      0.54      0.54       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_train_accuracy = lda_model.score(train_x,train_y)\n",
    "lda_test_accuracy = lda_model.score(test_x,test_y)\n",
    "print(\"Training Accuracy on the LDA model: %.4f\" % (lda_train_accuracy))\n",
    "print(\"Testing Accuracy on the LDA model: %.4f\" % (lda_test_accuracy))\n",
    "\n",
    "lda_pred_train = lda_model.predict(train_x)\n",
    "t1 = time.time()\n",
    "lda_pred_test = lda_model.predict(test_x)\n",
    "print(\"Prediction on LDA: %s minutes\" % (time.time() - t1))\n",
    "\n",
    "print(classification_report(train_y, lda_pred_train))\n",
    "print(classification_report(test_y, lda_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.241256713867188 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(n_estimators = 100, criterion = 'gini', min_samples_leaf=1, max_features='sqrt')\n",
    "rf_model = rf.fit(train_x, train_y)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on the random forest model: 1.0000\n",
      "Testing Accuracy on the random forest model: 0.4300\n",
      "Prediction on Random Forest: 0.06482815742492676 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        94\n",
      "           2       1.00      1.00      1.00        95\n",
      "           3       1.00      1.00      1.00       110\n",
      "           4       1.00      1.00      1.00        97\n",
      "           5       1.00      1.00      1.00        91\n",
      "           6       1.00      1.00      1.00        78\n",
      "           7       1.00      1.00      1.00        94\n",
      "           8       1.00      1.00      1.00       104\n",
      "           9       1.00      1.00      1.00        84\n",
      "          10       1.00      1.00      1.00        87\n",
      "          11       1.00      1.00      1.00        99\n",
      "          12       1.00      1.00      1.00        80\n",
      "          13       1.00      1.00      1.00        83\n",
      "          14       1.00      1.00      1.00        99\n",
      "          15       1.00      1.00      1.00        73\n",
      "          16       1.00      1.00      1.00        91\n",
      "          17       1.00      1.00      1.00       101\n",
      "          18       1.00      1.00      1.00        84\n",
      "          19       1.00      1.00      1.00        88\n",
      "          20       1.00      1.00      1.00        95\n",
      "          21       1.00      1.00      1.00        83\n",
      "          22       1.00      1.00      1.00        90\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.61      0.49        18\n",
      "           2       0.58      0.79      0.67        19\n",
      "           3       0.34      0.56      0.42        25\n",
      "           4       0.43      0.62      0.51        21\n",
      "           5       0.52      0.72      0.60        18\n",
      "           6       0.68      0.50      0.58        26\n",
      "           7       0.40      0.30      0.34        20\n",
      "           8       0.50      0.75      0.60        16\n",
      "           9       0.64      0.56      0.60        25\n",
      "          10       0.45      0.45      0.45        20\n",
      "          11       0.38      0.50      0.43        24\n",
      "          12       0.54      0.41      0.46        32\n",
      "          13       0.20      0.12      0.15        24\n",
      "          14       0.38      0.65      0.48        23\n",
      "          15       0.35      0.32      0.33        22\n",
      "          16       0.68      0.59      0.63        22\n",
      "          17       0.35      0.42      0.39        26\n",
      "          18       0.36      0.18      0.24        22\n",
      "          19       0.23      0.22      0.22        23\n",
      "          20       0.30      0.15      0.20        20\n",
      "          21       0.47      0.26      0.34        34\n",
      "          22       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.43       500\n",
      "   macro avg       0.42      0.44      0.42       500\n",
      "weighted avg       0.42      0.43      0.41       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_train_accuracy = rf_model.score(train_x,train_y)\n",
    "rf_test_accuracy = rf_model.score(test_x,test_y)\n",
    "print(\"Training Accuracy on the random forest model: %.4f\" % (rf_train_accuracy))\n",
    "print(\"Testing Accuracy on the random forest model: %.4f\" % (rf_test_accuracy))\n",
    "\n",
    "rf_pred_train = rf_model.predict(train_x)\n",
    "t1 = time.time()\n",
    "rf_pred_test = rf_model.predict(test_x)\n",
    "print(\"Prediction on Random Forest: %s seconds\" % (time.time() - t1))\n",
    "\n",
    "print(classification_report(train_y, rf_pred_train))\n",
    "print(classification_report(test_y, rf_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "\n",
    "### Ensemle with VotingClassifier : Combining the effective models to get the better prediction\n",
    "\n",
    "**From our candidate advanced models 1-5, summary is in the following :**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate Model</th>\n",
       "      <th>Training Accuracy(%)</th>\n",
       "      <th>Testing Accuracy(%)</th>\n",
       "      <th>Fitting Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline(GBM)</td>\n",
       "      <td>83.8</td>\n",
       "      <td>38.2</td>\n",
       "      <td>521s</td>\n",
       "      <td>0.036s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>100</td>\n",
       "      <td>49.2</td>\n",
       "      <td>1987s</td>\n",
       "      <td>1.18s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>77.25</td>\n",
       "      <td>51.4</td>\n",
       "      <td>15s</td>\n",
       "      <td>0.0084s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDA</td>\n",
       "      <td>85.8</td>\n",
       "      <td>53.8</td>\n",
       "      <td>68.77s</td>\n",
       "      <td>0.0077s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>81.45</td>\n",
       "      <td>48.4</td>\n",
       "      <td>18.18s</td>\n",
       "      <td>5.26s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>100</td>\n",
       "      <td>40.4</td>\n",
       "      <td>7.9s</td>\n",
       "      <td>0.04s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Candidate Model Training Accuracy(%) Testing Accuracy(%) Fitting Time  \\\n",
       "1        Baseline(GBM)                 83.8                38.2         521s   \n",
       "2              XGBoost                  100                49.2        1987s   \n",
       "3  Logistic Regression                77.25                51.4          15s   \n",
       "4                  LDA                 85.8                53.8       68.77s   \n",
       "5                  SVM                81.45                48.4       18.18s   \n",
       "6        Random Forest                  100                40.4         7.9s   \n",
       "\n",
       "  Prediction Time  \n",
       "1          0.036s  \n",
       "2           1.18s  \n",
       "3         0.0084s  \n",
       "4         0.0077s  \n",
       "5           5.26s  \n",
       "6           0.04s  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = {'Candidate Model':['Baseline(GBM)', 'XGBoost', 'Logistic Regression', 'LDA','SVM','Random Forest'],\n",
    "         'Training Accuracy(%)':['83.8','100','77.25','85.8','81.45','100'],\n",
    "        'Testing Accuracy(%)':['38.2','49.2','51.4','53.8','48.4','40.4'],\n",
    "         'Fitting Time':['521s','1987s','15s','68.77s','18.18s','7.9s'],\n",
    "         'Prediction Time':['0.036s','1.18s','0.0084s','0.0077s','5.26s','0.04s']\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(chart,index = ['1','2','3','4','5','6'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to combine the effiective models showing above and to do a ensemble prediction.\n",
    "\n",
    "From all the candidate models, XGBoost, Logistic Regression, LDA and SVM have the higher testing accuracy. At the same time, XGBoost has the longer fitting time, so we did not pick XGBoost in our ensemble model.\n",
    "\n",
    "### Ensemble : Logistic Regression, LDA and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 184.04435682296753 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', lr), ('lda', lda), ('svm', svc)], \n",
    "    voting='hard',\n",
    "    weights=[1.5,1.5,1])\n",
    "eclf_model = eclf.fit(train_x, train_y)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy on the Ensemle model: 0.8395\n",
      "Testing Accuracy on the Ensemle model: 0.5460\n",
      "Prediction on Ensemle model: 7.404771566390991 seconds\n"
     ]
    }
   ],
   "source": [
    "eclf_train_accuracy = eclf_model.score(train_x,train_y)\n",
    "eclf_test_accuracy = eclf_model.score(test_x,test_y)\n",
    "print(\"Training Accuracy on the Ensemble model: %.4f\" % (eclf_train_accuracy))\n",
    "print(\"Testing Accuracy on the Ensemble model: %.4f\" % (eclf_test_accuracy))\n",
    "\n",
    "eclf_pred_train = eclf_model.predict(train_x)\n",
    "t1 = time.time()\n",
    "eclf_pred_test = eclf_model.predict(test_x)\n",
    "print(\"Prediction on Ensemble model: %s seconds\" % (time.time() - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21f9db83470>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_eclf = confusion_matrix(test_y,eclf_pred_test)\n",
    "sns.heatmap(cm_eclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Candidate Advanced Model --- Deep Learning\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "At the beginning of this model, install the required packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(y)\n",
    "Y = Y[:,1:]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=1106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [6006] \n",
    "input_layer = Input(input_shape)\n",
    "x = BatchNormalization(momentum = 0.88)(input_layer) \n",
    "x = Dense(22*10,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = Dense(22*8,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*4,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*2,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "output_layer = Dense(22,activation='softmax',kernel_initializer=initializers.glorot_normal(seed=6))(x) \n",
    "model = Model(input_layer,output_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 3.0537 - accuracy: 0.0850\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.6401 - accuracy: 0.1750\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.3390 - accuracy: 0.2390\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.1587 - accuracy: 0.2860\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9740 - accuracy: 0.3305\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 1.9239 - accuracy: 0.3590\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.7919 - accuracy: 0.3870\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 1.7435 - accuracy: 0.3965\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 1.6664 - accuracy: 0.4300\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 1.6220 - accuracy: 0.4595\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 1.5656 - accuracy: 0.4710\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 1.5085 - accuracy: 0.4785\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 1.4754 - accuracy: 0.4960\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.4553 - accuracy: 0.5065\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.4272 - accuracy: 0.5140\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.3589 - accuracy: 0.5180\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.3797 - accuracy: 0.5135: 2s\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.2852 - accuracy: 0.5580\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.2575 - accuracy: 0.5570: 0s - loss:\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.2168 - accuracy: 0.5770\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1835 - accuracy: 0.6055\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.2186 - accuracy: 0.5890\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.1353 - accuracy: 0.6080: 0s - los\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.1108 - accuracy: 0.6240\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.0993 - accuracy: 0.6105\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.1070 - accuracy: 0.6160\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.0618 - accuracy: 0.6390\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 1.1090 - accuracy: 0.6195\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.0028 - accuracy: 0.6550\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.0084 - accuracy: 0.6470\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.9898 - accuracy: 0.6430\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.9536 - accuracy: 0.6720\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.9896 - accuracy: 0.6595\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.9562 - accuracy: 0.6715\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.9224 - accuracy: 0.6740\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.8914 - accuracy: 0.6830\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.8897 - accuracy: 0.6925\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.8638 - accuracy: 0.7070\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.8707 - accuracy: 0.6980\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.7874 - accuracy: 0.7240\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.8451 - accuracy: 0.7095\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.7906 - accuracy: 0.7265: 0s - loss: 0.7911 - accuracy\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.7643 - accuracy: 0.7425\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.8121 - accuracy: 0.7205\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.7456 - accuracy: 0.7310: 0s - loss: 0.7307 - \n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.7967 - accuracy: 0.7125: 1s\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.7990 - accuracy: 0.7310\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.7651 - accuracy: 0.7250\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.7739 - accuracy: 0.7335\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.6962 - accuracy: 0.7505\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.7170 - accuracy: 0.7525\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6771 - accuracy: 0.7765\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.6795 - accuracy: 0.7535\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.6461 - accuracy: 0.7760\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.7031 - accuracy: 0.7675\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.6330 - accuracy: 0.7905\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.6050 - accuracy: 0.7920\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.6295 - accuracy: 0.7885\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5960 - accuracy: 0.7975\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5769 - accuracy: 0.8035\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.6247 - accuracy: 0.7820\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5956 - accuracy: 0.7885\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5619 - accuracy: 0.8065\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5641 - accuracy: 0.8095: \n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5907 - accuracy: 0.7975\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5317 - accuracy: 0.8100: 1s - l\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5696 - accuracy: 0.8200\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5812 - accuracy: 0.8040\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5511 - accuracy: 0.8140\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5362 - accuracy: 0.8205\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5184 - accuracy: 0.8240\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5424 - accuracy: 0.8145\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5034 - accuracy: 0.8310\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5127 - accuracy: 0.8260\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.5003 - accuracy: 0.8400\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4804 - accuracy: 0.8330\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4810 - accuracy: 0.8405\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4581 - accuracy: 0.8415: 1s - l\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4836 - accuracy: 0.8300\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4831 - accuracy: 0.8405\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4505 - accuracy: 0.8395\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4601 - accuracy: 0.8470\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4741 - accuracy: 0.8395\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4342 - accuracy: 0.8545\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4397 - accuracy: 0.8565\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4414 - accuracy: 0.8570\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4227 - accuracy: 0.8610\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4289 - accuracy: 0.8610\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4139 - accuracy: 0.8545\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4423 - accuracy: 0.8535\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4093 - accuracy: 0.8635\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4298 - accuracy: 0.8625\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4034 - accuracy: 0.8695\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4409 - accuracy: 0.8505\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3747 - accuracy: 0.8745\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3568 - accuracy: 0.8715\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3575 - accuracy: 0.8900\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4057 - accuracy: 0.8615\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3975 - accuracy: 0.8645\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3923 - accuracy: 0.8670\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3899 - accuracy: 0.8745\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.4063 - accuracy: 0.8635\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3346 - accuracy: 0.8895\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3409 - accuracy: 0.8780\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2986 - accuracy: 0.9030: 0s - loss: 0.2912 - accu\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3178 - accuracy: 0.8970\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3507 - accuracy: 0.8760\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3489 - accuracy: 0.8785\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3146 - accuracy: 0.8990\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3036 - accuracy: 0.8945\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.3299 - accuracy: 0.8890\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3161 - accuracy: 0.9005\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.3018 - accuracy: 0.8990\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.2852 - accuracy: 0.9055: 1s - loss:\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3365 - accuracy: 0.8940\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3308 - accuracy: 0.8945\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3210 - accuracy: 0.9010\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3088 - accuracy: 0.9000\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3145 - accuracy: 0.8930\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2915 - accuracy: 0.9070\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2738 - accuracy: 0.9035\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2417 - accuracy: 0.9205\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.3220 - accuracy: 0.8950\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2852 - accuracy: 0.9060\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2852 - accuracy: 0.9095: 0s - loss:\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3267 - accuracy: 0.9005\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2684 - accuracy: 0.9130\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2726 - accuracy: 0.9080\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2621 - accuracy: 0.9210\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2687 - accuracy: 0.9140\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2977 - accuracy: 0.8990\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2675 - accuracy: 0.9140\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2268 - accuracy: 0.9230\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2305 - accuracy: 0.9325\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.2717 - accuracy: 0.9100\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2731 - accuracy: 0.9135\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2597 - accuracy: 0.9160\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2840 - accuracy: 0.9095\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2490 - accuracy: 0.9165\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2360 - accuracy: 0.9200\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2195 - accuracy: 0.9290\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2512 - accuracy: 0.9160\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2562 - accuracy: 0.9210\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2104 - accuracy: 0.9300: 0s - loss: 0.2014 - accuracy\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2083 - accuracy: 0.9270\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2535 - accuracy: 0.9280\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2449 - accuracy: 0.9220\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2714 - accuracy: 0.9135\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2097 - accuracy: 0.9260\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1879 - accuracy: 0.9390\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2585 - accuracy: 0.9110\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2400 - accuracy: 0.9220\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2342 - accuracy: 0.9245\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2339 - accuracy: 0.9315\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2123 - accuracy: 0.9275\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2122 - accuracy: 0.9350\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2397 - accuracy: 0.9195\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2075 - accuracy: 0.9355\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2147 - accuracy: 0.9385\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1815 - accuracy: 0.9425\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2266 - accuracy: 0.9290\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2080 - accuracy: 0.9340\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1986 - accuracy: 0.9370\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1735 - accuracy: 0.9440\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2249 - accuracy: 0.9335\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1878 - accuracy: 0.9425\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1895 - accuracy: 0.9420\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2194 - accuracy: 0.9335\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2162 - accuracy: 0.9335\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1957 - accuracy: 0.9345\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1947 - accuracy: 0.9405\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2162 - accuracy: 0.9330\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1849 - accuracy: 0.9400\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1837 - accuracy: 0.9490: \n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1722 - accuracy: 0.9355\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1574 - accuracy: 0.9510\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1627 - accuracy: 0.9450\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2011 - accuracy: 0.9380\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2105 - accuracy: 0.9360\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1914 - accuracy: 0.9400: 0s - loss: 0.1742 \n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1933 - accuracy: 0.9410\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2069 - accuracy: 0.9395\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2026 - accuracy: 0.9345\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.2016 - accuracy: 0.9340\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.1853 - accuracy: 0.9470\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1599 - accuracy: 0.9480\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1733 - accuracy: 0.9450\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1625 - accuracy: 0.9455\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1783 - accuracy: 0.9425\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1574 - accuracy: 0.9500\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1571 - accuracy: 0.9480\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1982 - accuracy: 0.9420\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1600 - accuracy: 0.9490\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1889 - accuracy: 0.9430\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1370 - accuracy: 0.9560\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1549 - accuracy: 0.9520\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1609 - accuracy: 0.9465\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1765 - accuracy: 0.9415\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1394 - accuracy: 0.9535\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 0.1717 - accuracy: 0.9460\n",
      "--- 647.6277086734772 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
    "model_nn = model.fit(X_train,y_train,epochs=200)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy Curves')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(model_nn.history['accuracy'],'r',linewidth=1.0)\n",
    "plt.legend(['Training Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 58.0 percent\n",
      "testing model takes 0.21555399894714355 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred = model.predict(X_test)\n",
    "pred_list = [] \n",
    "for i in range(len(pred)):\n",
    "    arr = pred[i]\n",
    "    idx = np.argwhere(arr == np.max(arr))\n",
    "    pred_list.append(idx[0][0])\n",
    "tst_labl = np.argmax(y_test, axis=-1)\n",
    "accuracy = accuracy_score(pred_list, tst_labl)\n",
    "print(\"Test accuracy is %s percent\" % round(accuracy*100,3))\n",
    "print(\"testing model takes %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing : Ensemble Model VS Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate Model</th>\n",
       "      <th>Testing Accuracy(%)</th>\n",
       "      <th>Fitting Time</th>\n",
       "      <th>Prediction Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ensemble Model</td>\n",
       "      <td>54.6</td>\n",
       "      <td>106s</td>\n",
       "      <td>6.7s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neutral Network</td>\n",
       "      <td>55</td>\n",
       "      <td>994s</td>\n",
       "      <td>0.35s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Candidate Model Testing Accuracy(%) Fitting Time Prediction Time\n",
       "1   Ensemble Model                54.6         106s            6.7s\n",
       "2  Neutral Network                  55         994s           0.35s"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare = {'Candidate Model':['Ensemble Model','Neutral Network'],\n",
    "        'Testing Accuracy(%)':['54.6','55'],\n",
    "         'Fitting Time':['106s','994s'],\n",
    "         'Prediction Time':['6.7s','0.35s']}\n",
    "df_c = pd.DataFrame(compare,index = ['1','2'])\n",
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../output/eclf.m'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-ea1d9682aa8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save ensemble model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meclf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'../output/eclf.m'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Python 2\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../output/eclf.m'"
     ]
    }
   ],
   "source": [
    "# save ensemble model\n",
    "joblib.dump(eclf,'../output/eclf.m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save nn model\n",
    "joblib.dump(model_nn,'../output/nn.m')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
