{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import numpy as np \n",
    "from scipy.spatial.distance import pdist\n",
    "import time \n",
    "import math\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Input, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "%run \"OneDrive/Documents/Columbia/Applied Data Science/lib/load.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 33.069143295288086 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## change the root to your own path \n",
    "\n",
    "#root = sys.path[0]\n",
    "#train_dir =  os.path.join(root,  '../data/train_set')  \n",
    "#train_image_dir =  os.path.join(train_dir, 'images')\n",
    "#train_pt_dir =  os.path.join(train_dir, 'points' )\n",
    "#train_label_path =  os.path.join(train_dir,  \"label.csv\")\n",
    "\n",
    "path = \"C:/Users/marko/OneDrive/Documents/Columbia/Applied Data Science/data/train_set/\"\n",
    "data = load.load_data(path+'label.csv')\n",
    "points_path = \"C:/Users/marko/OneDrive/Documents/Columbia/Applied Data Science/data/train_set/points/\"\n",
    "X = load.load_points(points_path,data)\n",
    "y= data['Index'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mat file changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mat file and store coordinates in mat \n",
    "m = []\n",
    "for idx in data['Index']: \n",
    "    file = \"%04d.mat\"%(idx)\n",
    "    m.append( scipy.io.loadmat( os.path.join( points_path, file )))\n",
    "\n",
    "mat = [x[[i for i in x.keys() if not i in ['__header__', '__version__', '__globals__']][0]] for x in m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_idx, test_idx = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_idx, test_idx = train_test_split(y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = [ mat[i-1] for i in train_idx ] \n",
    "test_mat = [ mat[i-1] for i in test_idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = data.emotion_idx[train_idx-1]\n",
    "test_labels = data.emotion_idx[test_idx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_cat = to_categorical(train_labels)\n",
    "train_label_cat= train_label_cat[:,1:]\n",
    "test_label_cat = to_categorical(test_labels)\n",
    "test_label_cat= test_label_cat[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1ï¼š  pairwise_dist_cal  \n",
    "#def pairwise_dist_cal(xy_cord):\n",
    "    #p_dist =[]\n",
    "    #for i in range(xy_cord.shape[0]):\n",
    "        #for j in range(i+1,xy_cord.shape[0]):\n",
    "               # p_dist.append( abs(round(xy_cord[i,0]) - round(xy_cord[j,0] )) )\n",
    "               # p_dist.append( abs(round(xy_cord[i,1]) - round(xy_cord[j,1] ) ) )\n",
    "    #return p_dist \n",
    "\n",
    "\n",
    "\n",
    "#### updated methods with selected fiducial points; this reduce 78 poins to 50  \n",
    "''' \n",
    "feature selection : 1. remove  points P64 - 70 and P72 - 78  \n",
    "                 2. remove P51,53,55,57,58,60, 61, 63\n",
    "                 3. calculate midpints between  upper and lower eyebrow lines and replace P20-22,P24-16 with midpoints               \n",
    "So there is in total 50 points left, which gives 50*49 pairwise distance \n",
    "'''\n",
    "\n",
    "def pairwise_dist_cal_updt(mt):\n",
    "    t0 = time.time()\n",
    "    p_dist_updt =np.zeros([len(mt),1225,2])\n",
    "    n = len(mt)\n",
    "    for k in range(n):\n",
    "        xy_cord = mt[k]\n",
    "\n",
    "        xy_cord_cpy  = xy_cord\n",
    "        \n",
    "        #  eye_brow midpoint \n",
    "        to_add_brl = (xy_cord_cpy[19:22]+ xy_cord_cpy[23:26])/2\n",
    "        to_add_brr = (xy_cord_cpy[27:30]+ xy_cord_cpy[31:34])/2\n",
    "\n",
    "        # index to remove \n",
    "        rm_idx = np.append(np.arange(63,70), np.arange(71,78))\n",
    "        rm_idx = np.append(rm_idx,np.arange(50,57,2) )\n",
    "        rm_idx = np.append(rm_idx, [57,59,60,62])\n",
    "        rm_idx = np.append(rm_idx, np.arange(19,22))\n",
    "        rm_idx = np.append(rm_idx, np.arange(23,26))\n",
    "        rm_idx = np.append(rm_idx, np.arange(27,30))\n",
    "        rm_idx = np.append(rm_idx, np.arange(31,34))\n",
    "        xy_cord = np.delete(xy_cord, rm_idx, axis = 0) \n",
    "        xy_cord = np.concatenate((xy_cord,to_add_brl,to_add_brr))\n",
    "        \n",
    "\n",
    "        dist_h = [] \n",
    "        dist_v = []\n",
    "        for i in range(xy_cord.shape[0]):\n",
    "            for j in range(i+1,xy_cord.shape[0]):\n",
    "                dist_h.append( abs(round(xy_cord[i,0]) - round(xy_cord[j,0] )) )\n",
    "                dist_v.append( abs(round(xy_cord[i,1]) - round(xy_cord[j,1] ) ) )\n",
    "        p_dist_updt[k,:,0]= dist_h\n",
    "        p_dist_updt[k,:,1]= dist_v\n",
    "    \n",
    "\n",
    "    print(\"feature constructions takes %s seconds\" % (time.time() - t0))\n",
    "    return p_dist_updt.reshape([n,2450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: \n",
      "feature constructions takes 43.68259143829346 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"training: \")\n",
    "train_data = pairwise_dist_cal_updt(train_mat[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: \n",
      "feature constructions takes 11.231314182281494 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"testing: \")\n",
    "test_data = pairwise_dist_cal_updt(test_mat[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_lda = [ x  - 1 for x in train_labels ] \n",
    "test_labels_lda = [ x  - 1 for x in test_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(train_data, train_labels_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lda = lda.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lda_test_pred = lda.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the LDA mode: 0.3600\n"
     ]
    }
   ],
   "source": [
    "lda_accuracy = lda.score(test_data, test_labels_lda)\n",
    "print(\"Accuracy of the LDA mode: %.4f\" % (lda_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_xgb = [ x  - 1 for x in train_labels ] \n",
    "test_labels_xgb = [ x  - 1 for x in test_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  model takes 804.773 seconds\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators= 200,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softmax',  # for multi-labels classification \n",
    " num_class = 22, \n",
    " scale_pos_weight=1,\n",
    " seed=123)\n",
    "start_time=time.time()\n",
    "xgb.fit(train_data, train_labels_xgb ,eval_metric='auc')\n",
    "print(\"training  model takes %s seconds\" % round((time.time() - start_time),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model takes 0.418 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_xgb = xgb.predict(test_data)\n",
    "print(\"testing model takes %s seconds\" % round((time.time() - start_time),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 49.6 percent\n"
     ]
    }
   ],
   "source": [
    "acc_xgb = accuracy_score(pred_xgb,test_labels_xgb )\n",
    "print(\"Test accuracy is %s percent\" %(acc_xgb*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "estimator = XGBClassifier(objective='multi:softprob',seed=36)\n",
    "parameters = {\n",
    "   'max_depth': range (1, 5, 1),\n",
    "   'n_estimators': range(1, 200, 20),\n",
    "   'learning_rate': [0.1, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv1 = GridSearchCV(estimator = estimator , \n",
    "                        param_grid = parameters, \n",
    "                        scoring ='accuracy',\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "gscv1.fit(train_data, train_labels_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accuracy score\n",
    "print('Best score for data:', gscv1.best_score_) \n",
    "\n",
    "# Find the best parameters for the model by using grid search\n",
    "print('Best Max Depth:',gscv1.best_estimator_.max_depth) \n",
    "print('Best N.estimators:',gscv1.best_estimator_.n_estimators)\n",
    "print('Best Learning Rate:',gscv1.best_estimator_.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set predictors\n",
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(y)\n",
    "Y = Y[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=123)\n",
    "\n",
    "train_x, test_x, train_idx, test_idx = train_test_split(X, Y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels_cnn = [ x  - 1 for x in train_labels ] \n",
    "#test_labels_cnn = [ x  - 1 for x in test_labels ]\n",
    "\n",
    "train_labels_cnn = [ x  for x in train_labels ] \n",
    "test_labels_cnn = [ x  for x in test_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_cat = to_categorical(train_labels_cnn)\n",
    "train_label_cat= train_label_cat[:,1:]\n",
    "test_label_cat = to_categorical(test_labels_cnn)\n",
    "test_label_cat= test_label_cat[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Input,Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "input_shape = [6006] \n",
    "input_layer = Input(input_shape)\n",
    "x = BatchNormalization()(input_layer) \n",
    "x = Dense(96,activation='relu',kernel_initializer=initializers.glorot_normal(seed=None))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = Dense(16,activation='relu',kernel_initializer=initializers.glorot_normal(seed=None))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(22,activation='softmax',kernel_initializer=initializers.glorot_normal(seed=None))(x) \n",
    "model = Model(input_layer,output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 3.4590 - accuracy: 0.0590\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 699us/step - loss: 3.2104 - accuracy: 0.0765\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 703us/step - loss: 3.0991 - accuracy: 0.1000\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 708us/step - loss: 3.0061 - accuracy: 0.1065\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 714us/step - loss: 2.8994 - accuracy: 0.1290\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 729us/step - loss: 2.8402 - accuracy: 0.1410\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 2s 759us/step - loss: 2.7576 - accuracy: 0.1470\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 2s 824us/step - loss: 2.6887 - accuracy: 0.16700s -\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 2s 817us/step - loss: 2.6133 - accuracy: 0.1735\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 2s 752us/step - loss: 2.5473 - accuracy: 0.1960\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 2s 793us/step - loss: 2.4785 - accuracy: 0.1995\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 2s 760us/step - loss: 2.4959 - accuracy: 0.2040\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 2s 754us/step - loss: 2.3906 - accuracy: 0.2255\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 698us/step - loss: 2.3683 - accuracy: 0.2250\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 700us/step - loss: 2.3303 - accuracy: 0.2300\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 709us/step - loss: 2.3023 - accuracy: 0.2515\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 732us/step - loss: 2.3124 - accuracy: 0.2480\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 2s 752us/step - loss: 2.2552 - accuracy: 0.2580\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 2s 792us/step - loss: 2.2485 - accuracy: 0.2575\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 2s 803us/step - loss: 2.2591 - accuracy: 0.2575\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 710us/step - loss: 2.1881 - accuracy: 0.2810\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 715us/step - loss: 2.2278 - accuracy: 0.2690\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 704us/step - loss: 2.1374 - accuracy: 0.2930\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 729us/step - loss: 2.1940 - accuracy: 0.2685\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 2s 750us/step - loss: 2.1306 - accuracy: 0.2885\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 2s 883us/step - loss: 2.1764 - accuracy: 0.2850\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.1045 - accuracy: 0.2985\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 2s 920us/step - loss: 2.0979 - accuracy: 0.2945\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.1275 - accuracy: 0.2965\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 2s 951us/step - loss: 2.1026 - accuracy: 0.27901s - loss: 2.1329 - accuracy -\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 2.0966 - accuracy: 0.2875\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 2s 922us/step - loss: 2.0830 - accuracy: 0.3115\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 2.0348 - accuracy: 0.3195\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 2.0468 - accuracy: 0.3005\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 2.0444 - accuracy: 0.2965\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 2.0186 - accuracy: 0.3050\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9989 - accuracy: 0.3190\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 2.0027 - accuracy: 0.3100\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.9947 - accuracy: 0.3110\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9876 - accuracy: 0.3195\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9721 - accuracy: 0.3270\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9844 - accuracy: 0.3195\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9604 - accuracy: 0.3395\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9808 - accuracy: 0.3280\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9530 - accuracy: 0.3145\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.9610 - accuracy: 0.3280\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.9354 - accuracy: 0.3245\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 2s 961us/step - loss: 1.9144 - accuracy: 0.3430\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.9544 - accuracy: 0.3350\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8970 - accuracy: 0.3580\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8900 - accuracy: 0.3545\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.9444 - accuracy: 0.3335\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 2s 946us/step - loss: 1.9571 - accuracy: 0.3265\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 2s 966us/step - loss: 1.8810 - accuracy: 0.3585\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8854 - accuracy: 0.3445\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.9102 - accuracy: 0.3360\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.9101 - accuracy: 0.3615\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.9255 - accuracy: 0.3300\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 2s 904us/step - loss: 1.9264 - accuracy: 0.3425\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 2s 890us/step - loss: 1.8805 - accuracy: 0.3580\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 2s 983us/step - loss: 1.9129 - accuracy: 0.3440\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8654 - accuracy: 0.3500\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 2s 910us/step - loss: 1.8936 - accuracy: 0.3490\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 2s 889us/step - loss: 1.8391 - accuracy: 0.3660\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 2s 932us/step - loss: 1.8472 - accuracy: 0.3580\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 2s 858us/step - loss: 1.8549 - accuracy: 0.3645\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 2s 896us/step - loss: 1.8514 - accuracy: 0.3580\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8306 - accuracy: 0.3695\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8405 - accuracy: 0.3820\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8293 - accuracy: 0.3610\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8386 - accuracy: 0.3755\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8212 - accuracy: 0.3835\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8529 - accuracy: 0.3660\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8498 - accuracy: 0.3580\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8276 - accuracy: 0.3735\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8133 - accuracy: 0.3700\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8212 - accuracy: 0.3605\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8367 - accuracy: 0.3805\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 1.8151 - accuracy: 0.35 - 2s 1ms/step - loss: 1.8138 - accuracy: 0.3540\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8064 - accuracy: 0.3820\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8139 - accuracy: 0.3755\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7640 - accuracy: 0.3945\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8125 - accuracy: 0.3570\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7912 - accuracy: 0.3780\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 2s 863us/step - loss: 1.7884 - accuracy: 0.3775\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 2s 851us/step - loss: 1.8020 - accuracy: 0.3725\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 2s 819us/step - loss: 1.7790 - accuracy: 0.3855\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.8310 - accuracy: 0.3675\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7925 - accuracy: 0.3720\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 2s 859us/step - loss: 1.7760 - accuracy: 0.3825\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 2s 867us/step - loss: 1.7799 - accuracy: 0.3955\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 2s 820us/step - loss: 1.7597 - accuracy: 0.3815\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 2s 830us/step - loss: 1.7371 - accuracy: 0.3965\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 2s 836us/step - loss: 1.7625 - accuracy: 0.3860\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 2s 852us/step - loss: 1.7731 - accuracy: 0.3885\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 2s 997us/step - loss: 1.7510 - accuracy: 0.3880\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7525 - accuracy: 0.3820\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7351 - accuracy: 0.3925\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7516 - accuracy: 0.3915\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7614 - accuracy: 0.4030\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7441 - accuracy: 0.3900\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7513 - accuracy: 0.3760\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7669 - accuracy: 0.4015\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7636 - accuracy: 0.3865\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7818 - accuracy: 0.3935\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7379 - accuracy: 0.3845\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7267 - accuracy: 0.3855\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7485 - accuracy: 0.3935\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7482 - accuracy: 0.4045\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7720 - accuracy: 0.3860\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7567 - accuracy: 0.3850\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7689 - accuracy: 0.3815\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7192 - accuracy: 0.3965\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.7281 - accuracy: 0.3885\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7180 - accuracy: 0.3935\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 2s 834us/step - loss: 1.7141 - accuracy: 0.3920\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 2s 861us/step - loss: 1.7127 - accuracy: 0.4130\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 2s 825us/step - loss: 1.7009 - accuracy: 0.3900\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 2s 851us/step - loss: 1.7822 - accuracy: 0.3865\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 2s 826us/step - loss: 1.7374 - accuracy: 0.39950s - loss: 1.7286 - accura\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 2s 827us/step - loss: 1.6997 - accuracy: 0.3905\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 2s 882us/step - loss: 1.6727 - accuracy: 0.4075\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 2s 903us/step - loss: 1.7021 - accuracy: 0.4050\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 2s 843us/step - loss: 1.6622 - accuracy: 0.4145\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 2s 959us/step - loss: 1.7260 - accuracy: 0.4025\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7005 - accuracy: 0.3980\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7278 - accuracy: 0.4020\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 2s 838us/step - loss: 1.7794 - accuracy: 0.3850\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 2s 972us/step - loss: 1.7136 - accuracy: 0.3990\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7219 - accuracy: 0.3950\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6790 - accuracy: 0.4050: 1s - loss: - ETA: 0s - loss: 1.6677 - accura\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7106 - accuracy: 0.4015\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.6966 - accuracy: 0.4045\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.6999 - accuracy: 0.3985\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.6729 - accuracy: 0.4155\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.6539 - accuracy: 0.4195\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.6846 - accuracy: 0.4130\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.6685 - accuracy: 0.4220\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6792 - accuracy: 0.4110\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 2s 999us/step - loss: 1.7194 - accuracy: 0.4030\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 2s 919us/step - loss: 1.7045 - accuracy: 0.4075\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 2s 832us/step - loss: 1.6377 - accuracy: 0.4175\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 2s 776us/step - loss: 1.6683 - accuracy: 0.4140\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 2s 949us/step - loss: 1.6864 - accuracy: 0.3990\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6714 - accuracy: 0.4165\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.6708 - accuracy: 0.4030\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6620 - accuracy: 0.4115\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 2s 940us/step - loss: 1.6509 - accuracy: 0.4210\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 2s 961us/step - loss: 1.6537 - accuracy: 0.4115\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6748 - accuracy: 0.4055\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 2s 836us/step - loss: 1.6838 - accuracy: 0.4235\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.7242 - accuracy: 0.4065: 0s - loss: 1.7199 - accuracy: 0.\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6753 - accuracy: 0.4000\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 2s 930us/step - loss: 1.6586 - accuracy: 0.4225\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 2s 999us/step - loss: 1.6973 - accuracy: 0.4040\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6702 - accuracy: 0.4080\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 2s 974us/step - loss: 1.6680 - accuracy: 0.4115\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 2s 881us/step - loss: 1.6856 - accuracy: 0.3985\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 2s 796us/step - loss: 1.6602 - accuracy: 0.4200\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.6975 - accuracy: 0.4100\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 2s 996us/step - loss: 1.6299 - accuracy: 0.4165\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6487 - accuracy: 0.4180\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 2s 778us/step - loss: 1.6441 - accuracy: 0.4235\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 2s 829us/step - loss: 1.6866 - accuracy: 0.4110\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 736us/step - loss: 1.6834 - accuracy: 0.4075\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 731us/step - loss: 1.6693 - accuracy: 0.4130\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6424 - accuracy: 0.4195\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.6366 - accuracy: 0.4265\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 2s 756us/step - loss: 1.6058 - accuracy: 0.43350s - loss: 1.6\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 2s 762us/step - loss: 1.6298 - accuracy: 0.4270\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 2s 798us/step - loss: 1.6011 - accuracy: 0.4490\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 2s 954us/step - loss: 1.6574 - accuracy: 0.4190\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 1.6176 - accuracy: 0.41 - 2s 785us/step - loss: 1.6232 - accuracy: 0.4135\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 2s 783us/step - loss: 1.6660 - accuracy: 0.4200\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 2s 768us/step - loss: 1.6517 - accuracy: 0.4210\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 2s 787us/step - loss: 1.6399 - accuracy: 0.4155\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 2s 797us/step - loss: 1.6542 - accuracy: 0.4270\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 2s 821us/step - loss: 1.6489 - accuracy: 0.41700s - loss: 1.604\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 2s 798us/step - loss: 1.6756 - accuracy: 0.4260\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 2s 800us/step - loss: 1.6265 - accuracy: 0.4165\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 2s 819us/step - loss: 1.6421 - accuracy: 0.4225\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 2s 785us/step - loss: 1.6117 - accuracy: 0.4300\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 2s 788us/step - loss: 1.5677 - accuracy: 0.4490\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 2s 781us/step - loss: 1.5958 - accuracy: 0.4420\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6342 - accuracy: 0.4260\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6210 - accuracy: 0.4245\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 2s 783us/step - loss: 1.6652 - accuracy: 0.4125\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6750 - accuracy: 0.4050\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6132 - accuracy: 0.4410: 0s - loss: 1.6078 - \n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 2s 927us/step - loss: 1.6383 - accuracy: 0.4340\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 2s 863us/step - loss: 1.5968 - accuracy: 0.4310\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 2s 818us/step - loss: 1.6547 - accuracy: 0.4280\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 2s 800us/step - loss: 1.6441 - accuracy: 0.4240\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 2s 850us/step - loss: 1.6386 - accuracy: 0.4205\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 2s 875us/step - loss: 1.5786 - accuracy: 0.4485\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 2s 895us/step - loss: 1.6351 - accuracy: 0.4305\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 1.6435 - accuracy: 0.4320\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.5718 - accuracy: 0.4380\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 3s 1ms/step - loss: 1.5980 - accuracy: 0.4395\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 2s 962us/step - loss: 1.6331 - accuracy: 0.4265\n",
      "training  model takes 406.98 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
    "model_nn = model.fit(train_x, train_label_cat, epochs=200)\n",
    "print(\"training  model takes %s seconds\" % round((time.time() - start_time),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy Curves')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(model_nn.history['accuracy'],'r',linewidth=1.0)\n",
    "plt.legend(['Training Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 53.0 percent\n",
      "testing model takes 0.128 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred = model.predict(test_x)\n",
    "pred_list = [] \n",
    "for i in range(len(pred)):\n",
    "    arr = pred[i]\n",
    "    idx = np.argwhere(arr == np.max(arr))\n",
    "    pred_list.append(idx[0][0])\n",
    "tst_labl = np.argmax(test_label_cat, axis=-1)\n",
    "accuracy = accuracy_score(pred_list, tst_labl)\n",
    "print(\"Test accuracy is %s percent\" % round(accuracy*100,3))\n",
    "print(\"testing model takes %s seconds\" % round((time.time() - start),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Iterations test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [6006] \n",
    "input_layer = Input(input_shape)\n",
    "x = BatchNormalization(momentum = 0.88)(input_layer) \n",
    "x = Dense(22*10,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = Dense(22*8,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*4,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*2,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "output_layer = Dense(22,activation='softmax',kernel_initializer=initializers.glorot_normal(seed=6))(x) \n",
    "model = Model(input_layer,output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 1s 637us/step - loss: 3.1294 - accuracy: 0.0620 - val_loss: 17.7691 - val_accuracy: 0.0320\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 2.8740 - accuracy: 0.1300 - val_loss: 3.2928 - val_accuracy: 0.1940\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 2.4950 - accuracy: 0.2025 - val_loss: 2.7819 - val_accuracy: 0.2180\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 2.3102 - accuracy: 0.2455 - val_loss: 2.5626 - val_accuracy: 0.2180\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 2.2096 - accuracy: 0.2535 - val_loss: 2.3436 - val_accuracy: 0.2620\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 2.0376 - accuracy: 0.3105 - val_loss: 2.3819 - val_accuracy: 0.2700\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 1.9541 - accuracy: 0.3075 - val_loss: 2.1632 - val_accuracy: 0.3200\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 1.8640 - accuracy: 0.3495 - val_loss: 2.1072 - val_accuracy: 0.3120\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 1.7857 - accuracy: 0.3710 - val_loss: 1.9677 - val_accuracy: 0.3400\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 1.7256 - accuracy: 0.4045 - val_loss: 1.7853 - val_accuracy: 0.3620\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 1.6685 - accuracy: 0.4130 - val_loss: 1.7754 - val_accuracy: 0.3840\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 1.6649 - accuracy: 0.4180 - val_loss: 1.7183 - val_accuracy: 0.4140\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 1.6420 - accuracy: 0.4195 - val_loss: 1.6776 - val_accuracy: 0.4280\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 1.5264 - accuracy: 0.4635 - val_loss: 1.6308 - val_accuracy: 0.4460\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 1.5509 - accuracy: 0.4605 - val_loss: 1.5751 - val_accuracy: 0.4260\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 1.4907 - accuracy: 0.4715 - val_loss: 1.5273 - val_accuracy: 0.4640\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 1.3800 - accuracy: 0.5095 - val_loss: 1.4841 - val_accuracy: 0.4520\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 1.3017 - accuracy: 0.5375 - val_loss: 1.4676 - val_accuracy: 0.4840\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 1.3506 - accuracy: 0.5220 - val_loss: 1.4684 - val_accuracy: 0.4900\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 1.2747 - accuracy: 0.5565 - val_loss: 1.4398 - val_accuracy: 0.4700\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 1s 344us/step - loss: 1.2566 - accuracy: 0.5595 - val_loss: 1.4157 - val_accuracy: 0.4900\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 1.2098 - accuracy: 0.5720 - val_loss: 1.4160 - val_accuracy: 0.5280\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 1s 327us/step - loss: 1.1808 - accuracy: 0.5760 - val_loss: 1.4351 - val_accuracy: 0.5060\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 1.1176 - accuracy: 0.6160 - val_loss: 1.4084 - val_accuracy: 0.5180\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 1.0902 - accuracy: 0.6170 - val_loss: 1.4305 - val_accuracy: 0.5220\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 1.0319 - accuracy: 0.6250 - val_loss: 1.3948 - val_accuracy: 0.5260\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.9835 - accuracy: 0.6420 - val_loss: 1.4666 - val_accuracy: 0.5180\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 1.0151 - accuracy: 0.6390 - val_loss: 1.4324 - val_accuracy: 0.5320\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.9834 - accuracy: 0.6500 - val_loss: 1.3994 - val_accuracy: 0.5360\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 1s 326us/step - loss: 0.9854 - accuracy: 0.6595 - val_loss: 1.4295 - val_accuracy: 0.5180\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.9419 - accuracy: 0.6555 - val_loss: 1.4423 - val_accuracy: 0.5320\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.9260 - accuracy: 0.6740 - val_loss: 1.4441 - val_accuracy: 0.5320\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.9378 - accuracy: 0.6805 - val_loss: 1.4977 - val_accuracy: 0.5060\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.8760 - accuracy: 0.6830 - val_loss: 1.4718 - val_accuracy: 0.5460\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.8142 - accuracy: 0.7115 - val_loss: 1.4834 - val_accuracy: 0.5420\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.7645 - accuracy: 0.7305 - val_loss: 1.4627 - val_accuracy: 0.5500\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.7094 - accuracy: 0.7365 - val_loss: 1.5947 - val_accuracy: 0.5240\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.7611 - accuracy: 0.7265 - val_loss: 1.5637 - val_accuracy: 0.5260\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.7577 - accuracy: 0.7270 - val_loss: 1.6210 - val_accuracy: 0.5280\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.7522 - accuracy: 0.7510 - val_loss: 1.6534 - val_accuracy: 0.5120\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.7578 - accuracy: 0.7335 - val_loss: 1.7172 - val_accuracy: 0.5340\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.7411 - accuracy: 0.7495 - val_loss: 1.6313 - val_accuracy: 0.5500\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.6936 - accuracy: 0.7640 - val_loss: 1.6116 - val_accuracy: 0.5420\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 1s 324us/step - loss: 0.6995 - accuracy: 0.7580 - val_loss: 1.6800 - val_accuracy: 0.5400\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.6260 - accuracy: 0.7755 - val_loss: 1.9248 - val_accuracy: 0.5220\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 1s 324us/step - loss: 0.6041 - accuracy: 0.7910 - val_loss: 1.7814 - val_accuracy: 0.5480\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.5597 - accuracy: 0.8065 - val_loss: 1.8120 - val_accuracy: 0.5440\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.5935 - accuracy: 0.8005 - val_loss: 1.6566 - val_accuracy: 0.5420\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.5681 - accuracy: 0.8115 - val_loss: 1.7115 - val_accuracy: 0.5260\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.5510 - accuracy: 0.8100 - val_loss: 1.8103 - val_accuracy: 0.5320\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 622us/step - loss: 0.4930 - accuracy: 0.8320 - val_loss: 1.7321 - val_accuracy: 0.5500\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.4460 - accuracy: 0.8455 - val_loss: 1.7040 - val_accuracy: 0.5640\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.3822 - accuracy: 0.8690 - val_loss: 1.6813 - val_accuracy: 0.5660\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.3764 - accuracy: 0.8695 - val_loss: 1.6919 - val_accuracy: 0.5760\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 344us/step - loss: 0.4195 - accuracy: 0.8560 - val_loss: 1.7188 - val_accuracy: 0.5620\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.3673 - accuracy: 0.8700 - val_loss: 1.7206 - val_accuracy: 0.5660\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.3679 - accuracy: 0.8750 - val_loss: 1.7186 - val_accuracy: 0.5720\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.3140 - accuracy: 0.8880 - val_loss: 1.7439 - val_accuracy: 0.5620\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.3443 - accuracy: 0.8885 - val_loss: 1.7592 - val_accuracy: 0.5660\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.3234 - accuracy: 0.8830 - val_loss: 1.7468 - val_accuracy: 0.5560\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.2962 - accuracy: 0.8965 - val_loss: 1.7654 - val_accuracy: 0.5660\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.2958 - accuracy: 0.8990 - val_loss: 1.7894 - val_accuracy: 0.5720\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.2792 - accuracy: 0.9105 - val_loss: 1.8289 - val_accuracy: 0.5660\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.2717 - accuracy: 0.9085 - val_loss: 1.8684 - val_accuracy: 0.5540\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.2894 - accuracy: 0.9075 - val_loss: 1.8822 - val_accuracy: 0.5500\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.2660 - accuracy: 0.9120 - val_loss: 1.9026 - val_accuracy: 0.5560\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.2704 - accuracy: 0.9040 - val_loss: 1.9008 - val_accuracy: 0.5520\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.2839 - accuracy: 0.8940 - val_loss: 1.9137 - val_accuracy: 0.5680\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.2544 - accuracy: 0.9130 - val_loss: 1.9518 - val_accuracy: 0.5700\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.2825 - accuracy: 0.9010 - val_loss: 1.9536 - val_accuracy: 0.5680\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.2593 - accuracy: 0.9120 - val_loss: 1.9497 - val_accuracy: 0.5740\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.2495 - accuracy: 0.9140 - val_loss: 1.9495 - val_accuracy: 0.5640\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.2650 - accuracy: 0.9175 - val_loss: 1.9522 - val_accuracy: 0.5580\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.2460 - accuracy: 0.9175 - val_loss: 1.9397 - val_accuracy: 0.5600\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.2335 - accuracy: 0.9200 - val_loss: 1.9450 - val_accuracy: 0.5620\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.2268 - accuracy: 0.9200 - val_loss: 1.9840 - val_accuracy: 0.5660\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.2244 - accuracy: 0.9275 - val_loss: 1.9878 - val_accuracy: 0.5640\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.2193 - accuracy: 0.9285 - val_loss: 2.0098 - val_accuracy: 0.5680\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.2436 - accuracy: 0.9210 - val_loss: 2.0214 - val_accuracy: 0.5740\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.2059 - accuracy: 0.9255 - val_loss: 2.0330 - val_accuracy: 0.5600\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.2300 - accuracy: 0.9170 - val_loss: 2.0504 - val_accuracy: 0.5560\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.1931 - accuracy: 0.9395 - val_loss: 2.0858 - val_accuracy: 0.5700\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.2097 - accuracy: 0.9310 - val_loss: 2.0895 - val_accuracy: 0.5700\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.2005 - accuracy: 0.9295 - val_loss: 2.0816 - val_accuracy: 0.5820\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.2323 - accuracy: 0.9190 - val_loss: 2.0705 - val_accuracy: 0.5680\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.1956 - accuracy: 0.9310 - val_loss: 2.0611 - val_accuracy: 0.5620\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.1958 - accuracy: 0.9365 - val_loss: 2.1042 - val_accuracy: 0.5620\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.2053 - accuracy: 0.9305 - val_loss: 2.1338 - val_accuracy: 0.5700\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.1897 - accuracy: 0.9390 - val_loss: 2.1450 - val_accuracy: 0.5680\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.2067 - accuracy: 0.9265 - val_loss: 2.1352 - val_accuracy: 0.5580\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.1764 - accuracy: 0.9470 - val_loss: 2.1347 - val_accuracy: 0.5620\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.1960 - accuracy: 0.9370 - val_loss: 2.1369 - val_accuracy: 0.5600\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.2008 - accuracy: 0.9320 - val_loss: 2.1777 - val_accuracy: 0.5680\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.1800 - accuracy: 0.9390 - val_loss: 2.1925 - val_accuracy: 0.5580\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.1931 - accuracy: 0.9340 - val_loss: 2.1862 - val_accuracy: 0.5620\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.1931 - accuracy: 0.9400 - val_loss: 2.1583 - val_accuracy: 0.5640\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.1997 - accuracy: 0.9350 - val_loss: 2.1282 - val_accuracy: 0.5600\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.1859 - accuracy: 0.9380 - val_loss: 2.1227 - val_accuracy: 0.5600\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.1637 - accuracy: 0.9490 - val_loss: 2.1593 - val_accuracy: 0.5700\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.1698 - accuracy: 0.9435 - val_loss: 2.1818 - val_accuracy: 0.5780\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.1639 - accuracy: 0.9455 - val_loss: 2.2057 - val_accuracy: 0.5760\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.1671 - accuracy: 0.9460 - val_loss: 2.2672 - val_accuracy: 0.5660\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.1650 - accuracy: 0.9420 - val_loss: 2.2734 - val_accuracy: 0.5660\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.1778 - accuracy: 0.9410 - val_loss: 2.2599 - val_accuracy: 0.5680\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.1530 - accuracy: 0.9430 - val_loss: 2.2915 - val_accuracy: 0.5600\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 441us/step - loss: 0.1766 - accuracy: 0.9435 - val_loss: 2.2898 - val_accuracy: 0.5620\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.1782 - accuracy: 0.9430 - val_loss: 2.2451 - val_accuracy: 0.5680\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.1537 - accuracy: 0.9530 - val_loss: 2.2283 - val_accuracy: 0.5600\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.1562 - accuracy: 0.9550 - val_loss: 2.2338 - val_accuracy: 0.5580\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.1458 - accuracy: 0.9550 - val_loss: 2.2573 - val_accuracy: 0.5600\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.1605 - accuracy: 0.9495 - val_loss: 2.2931 - val_accuracy: 0.5660\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.1435 - accuracy: 0.9500 - val_loss: 2.3294 - val_accuracy: 0.5580\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.1399 - accuracy: 0.9540 - val_loss: 2.3448 - val_accuracy: 0.5600\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.1541 - accuracy: 0.9455 - val_loss: 2.3665 - val_accuracy: 0.5540\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.1709 - accuracy: 0.9425 - val_loss: 2.3434 - val_accuracy: 0.5500\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.1595 - accuracy: 0.9485 - val_loss: 2.3621 - val_accuracy: 0.5600\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.1260 - accuracy: 0.9630 - val_loss: 2.4032 - val_accuracy: 0.5580\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.1422 - accuracy: 0.9530 - val_loss: 2.4307 - val_accuracy: 0.5500\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.1526 - accuracy: 0.9525 - val_loss: 2.4235 - val_accuracy: 0.5580\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.1391 - accuracy: 0.9560 - val_loss: 2.3844 - val_accuracy: 0.5600\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.1324 - accuracy: 0.9550 - val_loss: 2.3918 - val_accuracy: 0.5660\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.1252 - accuracy: 0.9595 - val_loss: 2.3673 - val_accuracy: 0.5720\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.1416 - accuracy: 0.9560 - val_loss: 2.3547 - val_accuracy: 0.5700\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.1284 - accuracy: 0.9575 - val_loss: 2.3985 - val_accuracy: 0.5640\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.1480 - accuracy: 0.9540 - val_loss: 2.4290 - val_accuracy: 0.5520\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.1102 - accuracy: 0.9615 - val_loss: 2.4119 - val_accuracy: 0.5580\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.1335 - accuracy: 0.9565 - val_loss: 2.4254 - val_accuracy: 0.5580\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.1114 - accuracy: 0.9665 - val_loss: 2.4605 - val_accuracy: 0.5520\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.1379 - accuracy: 0.9525 - val_loss: 2.4970 - val_accuracy: 0.5620\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.1504 - accuracy: 0.9555 - val_loss: 2.4575 - val_accuracy: 0.5660\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.1406 - accuracy: 0.9510 - val_loss: 2.4240 - val_accuracy: 0.5700\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.1500 - accuracy: 0.9545 - val_loss: 2.4180 - val_accuracy: 0.5580\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.1230 - accuracy: 0.9640 - val_loss: 2.3877 - val_accuracy: 0.5580\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.1201 - accuracy: 0.9605 - val_loss: 2.4067 - val_accuracy: 0.5560\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.1263 - accuracy: 0.9600 - val_loss: 2.4004 - val_accuracy: 0.5500\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.1252 - accuracy: 0.9610 - val_loss: 2.4042 - val_accuracy: 0.5480\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.1277 - accuracy: 0.9635 - val_loss: 2.4067 - val_accuracy: 0.5460\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.1115 - accuracy: 0.9715 - val_loss: 2.4069 - val_accuracy: 0.5480\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.1180 - accuracy: 0.9605 - val_loss: 2.4446 - val_accuracy: 0.5520\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.1328 - accuracy: 0.9585 - val_loss: 2.4409 - val_accuracy: 0.5580\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.1183 - accuracy: 0.9600 - val_loss: 2.4565 - val_accuracy: 0.5620\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.1096 - accuracy: 0.9640 - val_loss: 2.4670 - val_accuracy: 0.5540\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.1025 - accuracy: 0.9680 - val_loss: 2.4798 - val_accuracy: 0.5600\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.1276 - accuracy: 0.9605 - val_loss: 2.4491 - val_accuracy: 0.5640\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0972 - accuracy: 0.9680 - val_loss: 2.4134 - val_accuracy: 0.5620\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.1098 - accuracy: 0.9675 - val_loss: 2.3966 - val_accuracy: 0.5500\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.1090 - accuracy: 0.9635 - val_loss: 2.4313 - val_accuracy: 0.5580\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.1333 - accuracy: 0.9590 - val_loss: 2.4735 - val_accuracy: 0.5580\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.1027 - accuracy: 0.9700 - val_loss: 2.4945 - val_accuracy: 0.5660\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.1102 - accuracy: 0.9665 - val_loss: 2.4979 - val_accuracy: 0.5560\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.1381 - accuracy: 0.9535 - val_loss: 2.4652 - val_accuracy: 0.5620\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0884 - accuracy: 0.9725 - val_loss: 2.4874 - val_accuracy: 0.5600\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.1308 - accuracy: 0.9590 - val_loss: 2.4796 - val_accuracy: 0.5640\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.1099 - accuracy: 0.9625 - val_loss: 2.4609 - val_accuracy: 0.5720\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.1012 - accuracy: 0.9675 - val_loss: 2.4727 - val_accuracy: 0.5600\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0945 - accuracy: 0.9715 - val_loss: 2.5038 - val_accuracy: 0.5440\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.1381 - accuracy: 0.9620 - val_loss: 2.5113 - val_accuracy: 0.5580\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.0867 - accuracy: 0.9715 - val_loss: 2.4824 - val_accuracy: 0.5580\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.1089 - accuracy: 0.9625 - val_loss: 2.5171 - val_accuracy: 0.5600\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.1058 - accuracy: 0.9700 - val_loss: 2.5303 - val_accuracy: 0.5660\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0983 - accuracy: 0.9695 - val_loss: 2.5628 - val_accuracy: 0.5640\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.1092 - accuracy: 0.9690 - val_loss: 2.5709 - val_accuracy: 0.5620\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.1213 - accuracy: 0.9600 - val_loss: 2.5592 - val_accuracy: 0.5680\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.1026 - accuracy: 0.9680 - val_loss: 2.5809 - val_accuracy: 0.5880\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0893 - accuracy: 0.9725 - val_loss: 2.5867 - val_accuracy: 0.5720\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0889 - accuracy: 0.9700 - val_loss: 2.5813 - val_accuracy: 0.5620\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0886 - accuracy: 0.9735 - val_loss: 2.5832 - val_accuracy: 0.5540\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0846 - accuracy: 0.9725 - val_loss: 2.5853 - val_accuracy: 0.5680\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0898 - accuracy: 0.9695 - val_loss: 2.5742 - val_accuracy: 0.5520\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0942 - accuracy: 0.9700 - val_loss: 2.6054 - val_accuracy: 0.5540\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0939 - accuracy: 0.9690 - val_loss: 2.5855 - val_accuracy: 0.5520\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0965 - accuracy: 0.9715 - val_loss: 2.5960 - val_accuracy: 0.5640\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0814 - accuracy: 0.9745 - val_loss: 2.5969 - val_accuracy: 0.5540\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0991 - accuracy: 0.9675 - val_loss: 2.6460 - val_accuracy: 0.5540\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0820 - accuracy: 0.9775 - val_loss: 2.6447 - val_accuracy: 0.5540\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0772 - accuracy: 0.9765 - val_loss: 2.6272 - val_accuracy: 0.5560\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0865 - accuracy: 0.9760 - val_loss: 2.6214 - val_accuracy: 0.5540\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0896 - accuracy: 0.9775 - val_loss: 2.6284 - val_accuracy: 0.5600\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0883 - accuracy: 0.9770 - val_loss: 2.6242 - val_accuracy: 0.5580\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0875 - accuracy: 0.9695 - val_loss: 2.6416 - val_accuracy: 0.5540\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0922 - accuracy: 0.9730 - val_loss: 2.6143 - val_accuracy: 0.5560\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.0674 - accuracy: 0.9760 - val_loss: 2.6211 - val_accuracy: 0.5560\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0814 - accuracy: 0.9730 - val_loss: 2.6057 - val_accuracy: 0.5500\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0711 - accuracy: 0.9805 - val_loss: 2.6345 - val_accuracy: 0.5440\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0822 - accuracy: 0.9755 - val_loss: 2.6216 - val_accuracy: 0.5560\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0806 - accuracy: 0.9765 - val_loss: 2.6205 - val_accuracy: 0.5480\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0846 - accuracy: 0.9745 - val_loss: 2.6349 - val_accuracy: 0.5520\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0732 - accuracy: 0.9720 - val_loss: 2.6306 - val_accuracy: 0.5540\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0897 - accuracy: 0.9705 - val_loss: 2.6571 - val_accuracy: 0.5460\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0740 - accuracy: 0.9760 - val_loss: 2.7027 - val_accuracy: 0.5620\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0636 - accuracy: 0.9790 - val_loss: 2.6852 - val_accuracy: 0.5540\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0922 - accuracy: 0.9700 - val_loss: 2.6637 - val_accuracy: 0.5580\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0890 - accuracy: 0.9700 - val_loss: 2.6523 - val_accuracy: 0.5640\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0699 - accuracy: 0.9795 - val_loss: 2.6194 - val_accuracy: 0.5760\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 344us/step - loss: 0.0824 - accuracy: 0.9745 - val_loss: 2.6342 - val_accuracy: 0.5620\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0731 - accuracy: 0.9780 - val_loss: 2.6673 - val_accuracy: 0.5580\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 2.6603 - val_accuracy: 0.5580\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0863 - accuracy: 0.9700 - val_loss: 2.6700 - val_accuracy: 0.5620\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0896 - accuracy: 0.9755 - val_loss: 2.6861 - val_accuracy: 0.5660\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0795 - accuracy: 0.9740 - val_loss: 2.6851 - val_accuracy: 0.5600\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0656 - accuracy: 0.9770 - val_loss: 2.6795 - val_accuracy: 0.5600\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.0612 - accuracy: 0.9795 - val_loss: 2.6680 - val_accuracy: 0.5600\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.0615 - accuracy: 0.9805 - val_loss: 2.6813 - val_accuracy: 0.5560\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0696 - accuracy: 0.9780 - val_loss: 2.7255 - val_accuracy: 0.5540\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0609 - accuracy: 0.9815 - val_loss: 2.7240 - val_accuracy: 0.5460\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0734 - accuracy: 0.9780 - val_loss: 2.7168 - val_accuracy: 0.5500\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0755 - accuracy: 0.9745 - val_loss: 2.7453 - val_accuracy: 0.5540\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0837 - accuracy: 0.9730 - val_loss: 2.7286 - val_accuracy: 0.5560\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - 1s 423us/step - loss: 0.0851 - accuracy: 0.9775 - val_loss: 2.7482 - val_accuracy: 0.5560\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0775 - accuracy: 0.9750 - val_loss: 2.7552 - val_accuracy: 0.5580\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0788 - accuracy: 0.9760 - val_loss: 2.7387 - val_accuracy: 0.5580\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0822 - accuracy: 0.9740 - val_loss: 2.7422 - val_accuracy: 0.5620\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0760 - accuracy: 0.9760 - val_loss: 2.7812 - val_accuracy: 0.5580\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0884 - accuracy: 0.9770 - val_loss: 2.8380 - val_accuracy: 0.5560\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.0828 - accuracy: 0.9755 - val_loss: 2.9209 - val_accuracy: 0.5500\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0902 - accuracy: 0.9735 - val_loss: 2.8353 - val_accuracy: 0.5540\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0806 - accuracy: 0.9760 - val_loss: 2.6897 - val_accuracy: 0.5540\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0612 - accuracy: 0.9805 - val_loss: 2.6415 - val_accuracy: 0.5480\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0610 - accuracy: 0.9820 - val_loss: 2.6924 - val_accuracy: 0.5520\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0644 - accuracy: 0.9780 - val_loss: 2.7159 - val_accuracy: 0.5520\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0700 - accuracy: 0.9770 - val_loss: 2.6924 - val_accuracy: 0.5420\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 2.6655 - val_accuracy: 0.5440\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.0677 - accuracy: 0.9765 - val_loss: 2.6924 - val_accuracy: 0.5560\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0878 - accuracy: 0.9775 - val_loss: 2.7193 - val_accuracy: 0.5620\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0663 - accuracy: 0.9815 - val_loss: 2.7279 - val_accuracy: 0.5580\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0492 - accuracy: 0.9835 - val_loss: 2.7722 - val_accuracy: 0.5560\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0567 - accuracy: 0.9810 - val_loss: 2.7622 - val_accuracy: 0.5620\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0823 - accuracy: 0.9755 - val_loss: 2.7835 - val_accuracy: 0.5660\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0659 - accuracy: 0.9795 - val_loss: 2.7674 - val_accuracy: 0.5680\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.98 - 1s 411us/step - loss: 0.0473 - accuracy: 0.9870 - val_loss: 2.8029 - val_accuracy: 0.5540\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0662 - accuracy: 0.9790 - val_loss: 2.8292 - val_accuracy: 0.5620\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0601 - accuracy: 0.9800 - val_loss: 2.8948 - val_accuracy: 0.5660\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0733 - accuracy: 0.9770 - val_loss: 2.8821 - val_accuracy: 0.5680\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0670 - accuracy: 0.9800 - val_loss: 2.8198 - val_accuracy: 0.5540\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0664 - accuracy: 0.9790 - val_loss: 2.8367 - val_accuracy: 0.5600\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0570 - accuracy: 0.9855 - val_loss: 2.8380 - val_accuracy: 0.5520\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0650 - accuracy: 0.9780 - val_loss: 2.8218 - val_accuracy: 0.5520\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0764 - accuracy: 0.9775 - val_loss: 2.7831 - val_accuracy: 0.5660\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0638 - accuracy: 0.9830 - val_loss: 2.7653 - val_accuracy: 0.5740\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0537 - accuracy: 0.9835 - val_loss: 2.7460 - val_accuracy: 0.5800\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0635 - accuracy: 0.9820 - val_loss: 2.7475 - val_accuracy: 0.5720\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0731 - accuracy: 0.9805 - val_loss: 2.7467 - val_accuracy: 0.5740\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0630 - accuracy: 0.9830 - val_loss: 2.7411 - val_accuracy: 0.5700\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0730 - accuracy: 0.9785 - val_loss: 2.7337 - val_accuracy: 0.5640\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.0779 - accuracy: 0.9765 - val_loss: 2.7254 - val_accuracy: 0.5680\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0720 - accuracy: 0.9785 - val_loss: 2.7012 - val_accuracy: 0.5700\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0643 - accuracy: 0.9775 - val_loss: 2.7152 - val_accuracy: 0.5680\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0583 - accuracy: 0.9820 - val_loss: 2.7284 - val_accuracy: 0.5760\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0541 - accuracy: 0.9800 - val_loss: 2.7375 - val_accuracy: 0.5800\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0769 - accuracy: 0.9785 - val_loss: 2.7601 - val_accuracy: 0.5780\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 2s 756us/step - loss: 0.0489 - accuracy: 0.9865 - val_loss: 2.7612 - val_accuracy: 0.5720\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 2.7661 - val_accuracy: 0.5740\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0615 - accuracy: 0.9800 - val_loss: 2.7508 - val_accuracy: 0.5780\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0654 - accuracy: 0.9830 - val_loss: 2.7620 - val_accuracy: 0.5740\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0464 - accuracy: 0.9855 - val_loss: 2.7696 - val_accuracy: 0.5720\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0554 - accuracy: 0.9815 - val_loss: 2.7527 - val_accuracy: 0.5700\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0776 - accuracy: 0.9805 - val_loss: 2.7442 - val_accuracy: 0.5760\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0537 - accuracy: 0.9815 - val_loss: 2.7482 - val_accuracy: 0.5700\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0622 - accuracy: 0.9820 - val_loss: 2.7224 - val_accuracy: 0.5740\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0517 - accuracy: 0.9855 - val_loss: 2.7538 - val_accuracy: 0.5720\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 2.7524 - val_accuracy: 0.5720\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0649 - accuracy: 0.9820 - val_loss: 2.7627 - val_accuracy: 0.5760\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 421us/step - loss: 0.0625 - accuracy: 0.9845 - val_loss: 2.7594 - val_accuracy: 0.5740\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0568 - accuracy: 0.9820 - val_loss: 2.7728 - val_accuracy: 0.5760\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0517 - accuracy: 0.9850 - val_loss: 2.7713 - val_accuracy: 0.5740\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0532 - accuracy: 0.9850 - val_loss: 2.7439 - val_accuracy: 0.5720\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0702 - accuracy: 0.9840 - val_loss: 2.7681 - val_accuracy: 0.5780\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0633 - accuracy: 0.9815 - val_loss: 2.7679 - val_accuracy: 0.5720\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0501 - accuracy: 0.9810 - val_loss: 2.7898 - val_accuracy: 0.5740\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 451us/step - loss: 0.0511 - accuracy: 0.9860 - val_loss: 2.7845 - val_accuracy: 0.5740\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 457us/step - loss: 0.0516 - accuracy: 0.9855 - val_loss: 2.7603 - val_accuracy: 0.5760\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0578 - accuracy: 0.9845 - val_loss: 2.7762 - val_accuracy: 0.5780\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.0759 - accuracy: 0.9740 - val_loss: 2.7715 - val_accuracy: 0.5760\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0476 - accuracy: 0.9865 - val_loss: 2.7769 - val_accuracy: 0.5760\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0445 - accuracy: 0.9845 - val_loss: 2.7705 - val_accuracy: 0.5760\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0579 - accuracy: 0.9835 - val_loss: 2.7803 - val_accuracy: 0.5780\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0468 - accuracy: 0.9870 - val_loss: 2.7607 - val_accuracy: 0.5740\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0540 - accuracy: 0.9840 - val_loss: 2.7753 - val_accuracy: 0.5740\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0643 - accuracy: 0.9810 - val_loss: 2.7822 - val_accuracy: 0.5760\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0463 - accuracy: 0.9835 - val_loss: 2.7834 - val_accuracy: 0.5780\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0606 - accuracy: 0.9810 - val_loss: 2.7896 - val_accuracy: 0.5820\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0536 - accuracy: 0.9850 - val_loss: 2.7901 - val_accuracy: 0.5780\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0459 - accuracy: 0.9845 - val_loss: 2.7983 - val_accuracy: 0.5800\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0456 - accuracy: 0.9835 - val_loss: 2.7980 - val_accuracy: 0.5760\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0569 - accuracy: 0.9810 - val_loss: 2.7849 - val_accuracy: 0.5780\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0548 - accuracy: 0.9815 - val_loss: 2.7822 - val_accuracy: 0.5760\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0450 - accuracy: 0.9855 - val_loss: 2.7919 - val_accuracy: 0.5740\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 2.8066 - val_accuracy: 0.5760\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 458us/step - loss: 0.0443 - accuracy: 0.9835 - val_loss: 2.8107 - val_accuracy: 0.5740\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 446us/step - loss: 0.0559 - accuracy: 0.9825 - val_loss: 2.8084 - val_accuracy: 0.5740\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0570 - accuracy: 0.9840 - val_loss: 2.7917 - val_accuracy: 0.5740\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0408 - accuracy: 0.9870 - val_loss: 2.8149 - val_accuracy: 0.5760\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0490 - accuracy: 0.9870 - val_loss: 2.8143 - val_accuracy: 0.5700\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0409 - accuracy: 0.9900 - val_loss: 2.8202 - val_accuracy: 0.5720\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0582 - accuracy: 0.9815 - val_loss: 2.8199 - val_accuracy: 0.5760\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0432 - accuracy: 0.9895 - val_loss: 2.8238 - val_accuracy: 0.5700\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0541 - accuracy: 0.9840 - val_loss: 2.8333 - val_accuracy: 0.5720\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0289 - accuracy: 0.9925 - val_loss: 2.8352 - val_accuracy: 0.5720\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0539 - accuracy: 0.9850 - val_loss: 2.8527 - val_accuracy: 0.5780\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0598 - accuracy: 0.9830 - val_loss: 2.8266 - val_accuracy: 0.5760\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0474 - accuracy: 0.9870 - val_loss: 2.8447 - val_accuracy: 0.5760\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 2.8415 - val_accuracy: 0.5780\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0507 - accuracy: 0.9850 - val_loss: 2.8116 - val_accuracy: 0.5720\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0589 - accuracy: 0.9830 - val_loss: 2.8164 - val_accuracy: 0.5740\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0480 - accuracy: 0.9885 - val_loss: 2.8205 - val_accuracy: 0.5720\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0462 - accuracy: 0.9865 - val_loss: 2.8434 - val_accuracy: 0.5700\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0581 - accuracy: 0.9860 - val_loss: 2.8518 - val_accuracy: 0.5760\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0374 - accuracy: 0.9900 - val_loss: 2.8401 - val_accuracy: 0.5720\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0364 - accuracy: 0.9900 - val_loss: 2.8426 - val_accuracy: 0.5680\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0462 - accuracy: 0.9860 - val_loss: 2.8232 - val_accuracy: 0.5720\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0516 - accuracy: 0.9855 - val_loss: 2.8376 - val_accuracy: 0.5700\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 2.8490 - val_accuracy: 0.5660\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0443 - accuracy: 0.9855 - val_loss: 2.8362 - val_accuracy: 0.5680\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 421us/step - loss: 0.0467 - accuracy: 0.9860 - val_loss: 2.8393 - val_accuracy: 0.5660\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0634 - accuracy: 0.9790 - val_loss: 2.8561 - val_accuracy: 0.5660\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0411 - accuracy: 0.9855 - val_loss: 2.8500 - val_accuracy: 0.5720\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0420 - accuracy: 0.9885 - val_loss: 2.8614 - val_accuracy: 0.5740\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0485 - accuracy: 0.9870 - val_loss: 2.8437 - val_accuracy: 0.5680\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0427 - accuracy: 0.9830 - val_loss: 2.8507 - val_accuracy: 0.5700\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0391 - accuracy: 0.9875 - val_loss: 2.8525 - val_accuracy: 0.5660\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0524 - accuracy: 0.9840 - val_loss: 2.8560 - val_accuracy: 0.5660\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0573 - accuracy: 0.9825 - val_loss: 2.8464 - val_accuracy: 0.5640\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0433 - accuracy: 0.9895 - val_loss: 2.8537 - val_accuracy: 0.5720\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0498 - accuracy: 0.9885 - val_loss: 2.8299 - val_accuracy: 0.5620\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0348 - accuracy: 0.9880 - val_loss: 2.8350 - val_accuracy: 0.5660\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.0443 - accuracy: 0.9850 - val_loss: 2.8422 - val_accuracy: 0.5680\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0319 - accuracy: 0.9915 - val_loss: 2.8419 - val_accuracy: 0.5660\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 473us/step - loss: 0.0430 - accuracy: 0.9870 - val_loss: 2.8422 - val_accuracy: 0.5680\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 489us/step - loss: 0.0451 - accuracy: 0.9855 - val_loss: 2.8501 - val_accuracy: 0.5700\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 454us/step - loss: 0.0385 - accuracy: 0.9900 - val_loss: 2.8487 - val_accuracy: 0.5680\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0378 - accuracy: 0.9895 - val_loss: 2.8473 - val_accuracy: 0.5660\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0514 - accuracy: 0.9855 - val_loss: 2.8452 - val_accuracy: 0.5660\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 440us/step - loss: 0.0356 - accuracy: 0.9915 - val_loss: 2.8502 - val_accuracy: 0.5680\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0440 - accuracy: 0.9860 - val_loss: 2.8562 - val_accuracy: 0.5740\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0330 - accuracy: 0.9905 - val_loss: 2.8651 - val_accuracy: 0.5720\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 452us/step - loss: 0.0493 - accuracy: 0.9820 - val_loss: 2.8570 - val_accuracy: 0.5740\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0482 - accuracy: 0.9835 - val_loss: 2.8527 - val_accuracy: 0.5700\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0405 - accuracy: 0.9890 - val_loss: 2.8555 - val_accuracy: 0.5700\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0437 - accuracy: 0.9870 - val_loss: 2.8581 - val_accuracy: 0.5680\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0477 - accuracy: 0.9860 - val_loss: 2.8587 - val_accuracy: 0.5740\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0445 - accuracy: 0.9875 - val_loss: 2.8484 - val_accuracy: 0.5720\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0402 - accuracy: 0.9895 - val_loss: 2.8737 - val_accuracy: 0.5700\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0548 - accuracy: 0.9830 - val_loss: 2.8771 - val_accuracy: 0.5700\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 448us/step - loss: 0.0454 - accuracy: 0.9855 - val_loss: 2.8689 - val_accuracy: 0.5720\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0447 - accuracy: 0.9890 - val_loss: 2.8877 - val_accuracy: 0.5740\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 446us/step - loss: 0.0277 - accuracy: 0.9910 - val_loss: 2.8784 - val_accuracy: 0.5740\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0485 - accuracy: 0.9870 - val_loss: 2.9021 - val_accuracy: 0.5720\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 451us/step - loss: 0.0478 - accuracy: 0.9860 - val_loss: 2.8871 - val_accuracy: 0.5700\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 461us/step - loss: 0.0342 - accuracy: 0.9885 - val_loss: 2.9053 - val_accuracy: 0.5700\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0530 - accuracy: 0.9860 - val_loss: 2.8758 - val_accuracy: 0.5660\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 2.8880 - val_accuracy: 0.5720\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0518 - accuracy: 0.9870 - val_loss: 2.9117 - val_accuracy: 0.5740\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0365 - accuracy: 0.9910 - val_loss: 2.9228 - val_accuracy: 0.5760\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0400 - accuracy: 0.9855 - val_loss: 2.9036 - val_accuracy: 0.5760\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0572 - accuracy: 0.9830 - val_loss: 2.8935 - val_accuracy: 0.5740\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0395 - accuracy: 0.9885 - val_loss: 2.8919 - val_accuracy: 0.5720\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0498 - accuracy: 0.9845 - val_loss: 2.9004 - val_accuracy: 0.5720\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0504 - accuracy: 0.9860 - val_loss: 2.8966 - val_accuracy: 0.5720\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 2.9027 - val_accuracy: 0.5680\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0401 - accuracy: 0.9870 - val_loss: 2.9021 - val_accuracy: 0.5680\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0546 - accuracy: 0.9810 - val_loss: 2.9140 - val_accuracy: 0.5740\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0366 - accuracy: 0.9915 - val_loss: 2.8942 - val_accuracy: 0.5760\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0397 - accuracy: 0.9860 - val_loss: 2.8852 - val_accuracy: 0.5780\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0367 - accuracy: 0.9905 - val_loss: 2.8914 - val_accuracy: 0.5760\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0377 - accuracy: 0.9890 - val_loss: 2.9207 - val_accuracy: 0.5780\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 462us/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 2.9257 - val_accuracy: 0.5720\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 2.9139 - val_accuracy: 0.5760\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0451 - accuracy: 0.9840 - val_loss: 2.9167 - val_accuracy: 0.5780\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0538 - accuracy: 0.9850 - val_loss: 2.9092 - val_accuracy: 0.5680\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0466 - accuracy: 0.9875 - val_loss: 2.9006 - val_accuracy: 0.5680\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0396 - accuracy: 0.9885 - val_loss: 2.8675 - val_accuracy: 0.5640\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0558 - accuracy: 0.9830 - val_loss: 2.8965 - val_accuracy: 0.5640\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0399 - accuracy: 0.9865 - val_loss: 2.8791 - val_accuracy: 0.5700\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0517 - accuracy: 0.9845 - val_loss: 2.9116 - val_accuracy: 0.5700\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 2.9032 - val_accuracy: 0.5640\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0390 - accuracy: 0.9875 - val_loss: 2.9128 - val_accuracy: 0.5660\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0278 - accuracy: 0.9915 - val_loss: 2.9059 - val_accuracy: 0.5680\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 2.9210 - val_accuracy: 0.5680\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 2.9466 - val_accuracy: 0.5640\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 2.9272 - val_accuracy: 0.5660\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0294 - accuracy: 0.9900 - val_loss: 2.9304 - val_accuracy: 0.5660\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0524 - accuracy: 0.9860 - val_loss: 2.9384 - val_accuracy: 0.5660\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 2.9413 - val_accuracy: 0.5640\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 435us/step - loss: 0.0508 - accuracy: 0.9855 - val_loss: 2.9260 - val_accuracy: 0.5660\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0301 - accuracy: 0.9905 - val_loss: 2.9285 - val_accuracy: 0.5620\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0358 - accuracy: 0.9890 - val_loss: 2.9214 - val_accuracy: 0.5640\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0585 - accuracy: 0.9830 - val_loss: 2.9285 - val_accuracy: 0.5660\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0462 - accuracy: 0.9840 - val_loss: 2.9430 - val_accuracy: 0.5640\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0474 - accuracy: 0.9860 - val_loss: 2.9567 - val_accuracy: 0.5700\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0353 - accuracy: 0.9890 - val_loss: 2.9527 - val_accuracy: 0.5680\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0371 - accuracy: 0.9880 - val_loss: 2.9451 - val_accuracy: 0.5660\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0420 - accuracy: 0.9875 - val_loss: 2.9468 - val_accuracy: 0.5660\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0415 - accuracy: 0.9865 - val_loss: 2.9344 - val_accuracy: 0.5660\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0409 - accuracy: 0.9865 - val_loss: 2.9402 - val_accuracy: 0.5600\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0405 - accuracy: 0.9850 - val_loss: 2.9498 - val_accuracy: 0.5620\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0398 - accuracy: 0.9880 - val_loss: 2.9176 - val_accuracy: 0.5720\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0317 - accuracy: 0.9910 - val_loss: 2.9218 - val_accuracy: 0.5680\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0503 - accuracy: 0.9815 - val_loss: 2.9314 - val_accuracy: 0.5700\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0421 - accuracy: 0.9885 - val_loss: 2.9528 - val_accuracy: 0.5620\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0337 - accuracy: 0.9905 - val_loss: 2.9441 - val_accuracy: 0.5660\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0387 - accuracy: 0.9905 - val_loss: 2.9547 - val_accuracy: 0.5680\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 2.9567 - val_accuracy: 0.5660\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0282 - accuracy: 0.9930 - val_loss: 2.9430 - val_accuracy: 0.5700\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0430 - accuracy: 0.9915 - val_loss: 2.9539 - val_accuracy: 0.5680\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 438us/step - loss: 0.0371 - accuracy: 0.9900 - val_loss: 2.9648 - val_accuracy: 0.5640\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 2.9471 - val_accuracy: 0.5660\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0444 - accuracy: 0.9875 - val_loss: 2.9485 - val_accuracy: 0.5680\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0489 - accuracy: 0.9855 - val_loss: 2.9602 - val_accuracy: 0.5700\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0240 - accuracy: 0.9930 - val_loss: 2.9534 - val_accuracy: 0.5640\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0466 - accuracy: 0.9875 - val_loss: 2.9447 - val_accuracy: 0.5700\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0335 - accuracy: 0.9895 - val_loss: 2.9521 - val_accuracy: 0.5660\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0487 - accuracy: 0.9845 - val_loss: 2.9592 - val_accuracy: 0.5660\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 2.9490 - val_accuracy: 0.5680\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 2.9363 - val_accuracy: 0.5620\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0455 - accuracy: 0.9855 - val_loss: 2.9456 - val_accuracy: 0.5620\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 2.9390 - val_accuracy: 0.5640\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 437us/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 2.9126 - val_accuracy: 0.5700\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0354 - accuracy: 0.9865 - val_loss: 2.9150 - val_accuracy: 0.5740\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0523 - accuracy: 0.9860 - val_loss: 2.9492 - val_accuracy: 0.5720\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0458 - accuracy: 0.9845 - val_loss: 2.9329 - val_accuracy: 0.5740\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0291 - accuracy: 0.9915 - val_loss: 2.9507 - val_accuracy: 0.5760\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0557 - accuracy: 0.9855 - val_loss: 2.9518 - val_accuracy: 0.5780\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0478 - accuracy: 0.9890 - val_loss: 2.9480 - val_accuracy: 0.5740\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 461us/step - loss: 0.0534 - accuracy: 0.9875 - val_loss: 2.9449 - val_accuracy: 0.5680\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 425us/step - loss: 0.0308 - accuracy: 0.9900 - val_loss: 2.9399 - val_accuracy: 0.5640\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 2.9430 - val_accuracy: 0.5620\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0483 - accuracy: 0.9870 - val_loss: 2.9208 - val_accuracy: 0.5640\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0435 - accuracy: 0.9870 - val_loss: 2.9394 - val_accuracy: 0.5660\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0481 - accuracy: 0.9895 - val_loss: 2.9692 - val_accuracy: 0.5780\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 2.9537 - val_accuracy: 0.5700\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0316 - accuracy: 0.9910 - val_loss: 2.9562 - val_accuracy: 0.5760\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0390 - accuracy: 0.9885 - val_loss: 2.9509 - val_accuracy: 0.5700\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0393 - accuracy: 0.9905 - val_loss: 2.9552 - val_accuracy: 0.5760\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 2.9727 - val_accuracy: 0.5740\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0428 - accuracy: 0.9850 - val_loss: 2.9575 - val_accuracy: 0.5700\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0285 - accuracy: 0.9900 - val_loss: 2.9809 - val_accuracy: 0.5700\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0425 - accuracy: 0.9860 - val_loss: 2.9670 - val_accuracy: 0.5660\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0440 - accuracy: 0.9845 - val_loss: 2.9762 - val_accuracy: 0.5720\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0407 - accuracy: 0.9850 - val_loss: 2.9596 - val_accuracy: 0.5640\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 2.9585 - val_accuracy: 0.5660\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0312 - accuracy: 0.9925 - val_loss: 2.9470 - val_accuracy: 0.5660\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0365 - accuracy: 0.9890 - val_loss: 2.9461 - val_accuracy: 0.5640\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 453us/step - loss: 0.0424 - accuracy: 0.9895 - val_loss: 2.9574 - val_accuracy: 0.5660\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0441 - accuracy: 0.9880 - val_loss: 2.9432 - val_accuracy: 0.5700\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0464 - accuracy: 0.9865 - val_loss: 2.9817 - val_accuracy: 0.5640\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0345 - accuracy: 0.9880 - val_loss: 2.9825 - val_accuracy: 0.5660\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0518 - accuracy: 0.9890 - val_loss: 2.9775 - val_accuracy: 0.5640\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0373 - accuracy: 0.9910 - val_loss: 2.9727 - val_accuracy: 0.5700\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0400 - accuracy: 0.9855 - val_loss: 2.9845 - val_accuracy: 0.5740\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0339 - accuracy: 0.9905 - val_loss: 2.9633 - val_accuracy: 0.5680\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 691us/step - loss: 0.0447 - accuracy: 0.9865 - val_loss: 2.9785 - val_accuracy: 0.5700\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0507 - accuracy: 0.9830 - val_loss: 2.9515 - val_accuracy: 0.5700\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0509 - accuracy: 0.9850 - val_loss: 2.9494 - val_accuracy: 0.5680\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0357 - accuracy: 0.9880 - val_loss: 2.9597 - val_accuracy: 0.5720\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 2.9624 - val_accuracy: 0.5720\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0367 - accuracy: 0.9885 - val_loss: 2.9486 - val_accuracy: 0.5660\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 2.9646 - val_accuracy: 0.5700\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0406 - accuracy: 0.9865 - val_loss: 2.9539 - val_accuracy: 0.5720\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 2.9593 - val_accuracy: 0.5700\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0341 - accuracy: 0.9880 - val_loss: 2.9683 - val_accuracy: 0.5660\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0347 - accuracy: 0.9880 - val_loss: 2.9756 - val_accuracy: 0.5680\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 449us/step - loss: 0.0398 - accuracy: 0.9860 - val_loss: 2.9607 - val_accuracy: 0.5680\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0368 - accuracy: 0.9895 - val_loss: 2.9708 - val_accuracy: 0.5680\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0454 - accuracy: 0.9875 - val_loss: 2.9716 - val_accuracy: 0.5700\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0435 - accuracy: 0.9860 - val_loss: 2.9737 - val_accuracy: 0.5680\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 2.9566 - val_accuracy: 0.5680\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0332 - accuracy: 0.9865 - val_loss: 2.9364 - val_accuracy: 0.5700\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0427 - accuracy: 0.9875 - val_loss: 2.9449 - val_accuracy: 0.5720\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0372 - accuracy: 0.9890 - val_loss: 2.9590 - val_accuracy: 0.5680\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.0382 - accuracy: 0.9870 - val_loss: 2.9588 - val_accuracy: 0.5660\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0413 - accuracy: 0.9865 - val_loss: 2.9654 - val_accuracy: 0.5680\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0455 - accuracy: 0.9875 - val_loss: 2.9524 - val_accuracy: 0.5620\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0461 - accuracy: 0.9840 - val_loss: 2.9552 - val_accuracy: 0.5660\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0342 - accuracy: 0.9905 - val_loss: 2.9360 - val_accuracy: 0.5640\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0351 - accuracy: 0.9895 - val_loss: 2.9348 - val_accuracy: 0.5700\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0347 - accuracy: 0.9910 - val_loss: 2.9351 - val_accuracy: 0.5620\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 2.9558 - val_accuracy: 0.5660\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0310 - accuracy: 0.9900 - val_loss: 2.9592 - val_accuracy: 0.5680\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 490us/step - loss: 0.0561 - accuracy: 0.9850 - val_loss: 2.9515 - val_accuracy: 0.5660\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 425us/step - loss: 0.0430 - accuracy: 0.9865 - val_loss: 2.9625 - val_accuracy: 0.5680\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 2.9615 - val_accuracy: 0.5640\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 451us/step - loss: 0.0484 - accuracy: 0.9900 - val_loss: 2.9711 - val_accuracy: 0.5700\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0323 - accuracy: 0.9910 - val_loss: 2.9686 - val_accuracy: 0.5680\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0364 - accuracy: 0.9885 - val_loss: 2.9811 - val_accuracy: 0.5680\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0402 - accuracy: 0.9865 - val_loss: 2.9727 - val_accuracy: 0.5660\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0385 - accuracy: 0.9895 - val_loss: 2.9777 - val_accuracy: 0.5660\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0366 - accuracy: 0.9865 - val_loss: 2.9627 - val_accuracy: 0.5660\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0350 - accuracy: 0.9885 - val_loss: 2.9656 - val_accuracy: 0.5640\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0428 - accuracy: 0.9870 - val_loss: 2.9784 - val_accuracy: 0.5660\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0310 - accuracy: 0.9920 - val_loss: 2.9898 - val_accuracy: 0.5680\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0330 - accuracy: 0.9890 - val_loss: 2.9817 - val_accuracy: 0.5660\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0392 - accuracy: 0.9900 - val_loss: 2.9739 - val_accuracy: 0.5680\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0569 - accuracy: 0.9820 - val_loss: 2.9736 - val_accuracy: 0.5660\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0365 - accuracy: 0.9875 - val_loss: 2.9803 - val_accuracy: 0.5680\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0276 - accuracy: 0.9895 - val_loss: 2.9629 - val_accuracy: 0.5680\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0261 - accuracy: 0.9915 - val_loss: 2.9634 - val_accuracy: 0.5660\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0325 - accuracy: 0.9895 - val_loss: 2.9555 - val_accuracy: 0.5660\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0360 - accuracy: 0.9905 - val_loss: 2.9581 - val_accuracy: 0.5700\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0417 - accuracy: 0.9895 - val_loss: 2.9589 - val_accuracy: 0.5660\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0299 - accuracy: 0.9915 - val_loss: 2.9567 - val_accuracy: 0.5660\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 437us/step - loss: 0.0375 - accuracy: 0.9895 - val_loss: 2.9711 - val_accuracy: 0.5680\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0432 - accuracy: 0.9870 - val_loss: 2.9638 - val_accuracy: 0.5620\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0353 - accuracy: 0.9865 - val_loss: 2.9583 - val_accuracy: 0.5660\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 2.9684 - val_accuracy: 0.5680\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0356 - accuracy: 0.9910 - val_loss: 2.9630 - val_accuracy: 0.5660\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0469 - accuracy: 0.9860 - val_loss: 2.9623 - val_accuracy: 0.5680\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0332 - accuracy: 0.9885 - val_loss: 2.9721 - val_accuracy: 0.5660\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0410 - accuracy: 0.9885 - val_loss: 2.9776 - val_accuracy: 0.5700\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0415 - accuracy: 0.9845 - val_loss: 2.9592 - val_accuracy: 0.5680\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0354 - accuracy: 0.9890 - val_loss: 2.9689 - val_accuracy: 0.5660\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0288 - accuracy: 0.9930 - val_loss: 2.9828 - val_accuracy: 0.5680\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0424 - accuracy: 0.9875 - val_loss: 2.9910 - val_accuracy: 0.5680\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0275 - accuracy: 0.9915 - val_loss: 2.9903 - val_accuracy: 0.5720\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0368 - accuracy: 0.9900 - val_loss: 2.9724 - val_accuracy: 0.5680\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0273 - accuracy: 0.9925 - val_loss: 2.9750 - val_accuracy: 0.5680\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0293 - accuracy: 0.9895 - val_loss: 2.9855 - val_accuracy: 0.5700\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0453 - accuracy: 0.9875 - val_loss: 2.9671 - val_accuracy: 0.5740\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0476 - accuracy: 0.9890 - val_loss: 2.9762 - val_accuracy: 0.5700\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0532 - accuracy: 0.9880 - val_loss: 2.9753 - val_accuracy: 0.5660\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0382 - accuracy: 0.9895 - val_loss: 2.9564 - val_accuracy: 0.5720\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0253 - accuracy: 0.9940 - val_loss: 2.9781 - val_accuracy: 0.5720\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0469 - accuracy: 0.9870 - val_loss: 2.9639 - val_accuracy: 0.5640\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0270 - accuracy: 0.9925 - val_loss: 2.9714 - val_accuracy: 0.5660\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0455 - accuracy: 0.9855 - val_loss: 2.9668 - val_accuracy: 0.5660\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0421 - accuracy: 0.9875 - val_loss: 2.9714 - val_accuracy: 0.5700\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 2.9655 - val_accuracy: 0.5700\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 2.9695 - val_accuracy: 0.5740\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0686 - accuracy: 0.9835 - val_loss: 2.9610 - val_accuracy: 0.5640\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0501 - accuracy: 0.9855 - val_loss: 2.9586 - val_accuracy: 0.5680\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0376 - accuracy: 0.9885 - val_loss: 2.9702 - val_accuracy: 0.5620\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0337 - accuracy: 0.9880 - val_loss: 2.9612 - val_accuracy: 0.5680\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0539 - accuracy: 0.9855 - val_loss: 2.9667 - val_accuracy: 0.5640\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0459 - accuracy: 0.9895 - val_loss: 2.9720 - val_accuracy: 0.5700\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0333 - accuracy: 0.9895 - val_loss: 2.9764 - val_accuracy: 0.5680\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0364 - accuracy: 0.9895 - val_loss: 2.9593 - val_accuracy: 0.5660\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0391 - accuracy: 0.9845 - val_loss: 2.9772 - val_accuracy: 0.5660\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0509 - accuracy: 0.9830 - val_loss: 2.9697 - val_accuracy: 0.5700\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0418 - accuracy: 0.9860 - val_loss: 2.9523 - val_accuracy: 0.5640\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0335 - accuracy: 0.9925 - val_loss: 2.9832 - val_accuracy: 0.5660\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 445us/step - loss: 0.0488 - accuracy: 0.9835 - val_loss: 2.9687 - val_accuracy: 0.5640\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.0375 - accuracy: 0.9865 - val_loss: 2.9654 - val_accuracy: 0.5660\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0290 - accuracy: 0.9910 - val_loss: 2.9716 - val_accuracy: 0.5620\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0434 - accuracy: 0.9870 - val_loss: 2.9639 - val_accuracy: 0.5640\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0365 - accuracy: 0.9885 - val_loss: 2.9698 - val_accuracy: 0.5640\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0400 - accuracy: 0.9870 - val_loss: 2.9684 - val_accuracy: 0.5680\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 2.9609 - val_accuracy: 0.5680\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0385 - accuracy: 0.9905 - val_loss: 2.9674 - val_accuracy: 0.5660\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0439 - accuracy: 0.9870 - val_loss: 2.9805 - val_accuracy: 0.5660\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0395 - accuracy: 0.9865 - val_loss: 2.9809 - val_accuracy: 0.5660\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0339 - accuracy: 0.9890 - val_loss: 2.9807 - val_accuracy: 0.5680\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0481 - accuracy: 0.9850 - val_loss: 2.9829 - val_accuracy: 0.5700\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 2.9823 - val_accuracy: 0.5680\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0423 - accuracy: 0.9875 - val_loss: 2.9735 - val_accuracy: 0.5680\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0229 - accuracy: 0.9935 - val_loss: 2.9738 - val_accuracy: 0.5660\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0433 - accuracy: 0.9870 - val_loss: 2.9718 - val_accuracy: 0.5660\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0450 - accuracy: 0.9845 - val_loss: 2.9754 - val_accuracy: 0.5660\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 2.9826 - val_accuracy: 0.5660\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0384 - accuracy: 0.9840 - val_loss: 2.9564 - val_accuracy: 0.5660\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0350 - accuracy: 0.9905 - val_loss: 2.9865 - val_accuracy: 0.5660\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0342 - accuracy: 0.9885 - val_loss: 2.9755 - val_accuracy: 0.5660\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0385 - accuracy: 0.9915 - val_loss: 2.9979 - val_accuracy: 0.5680\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 441us/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 2.9897 - val_accuracy: 0.5720\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0464 - accuracy: 0.9855 - val_loss: 2.9669 - val_accuracy: 0.5640\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0311 - accuracy: 0.9880 - val_loss: 2.9596 - val_accuracy: 0.5660\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 2.9786 - val_accuracy: 0.5660\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0302 - accuracy: 0.9925 - val_loss: 2.9774 - val_accuracy: 0.5640\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0557 - accuracy: 0.9855 - val_loss: 2.9648 - val_accuracy: 0.5660\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0367 - accuracy: 0.9885 - val_loss: 2.9717 - val_accuracy: 0.5680\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0364 - accuracy: 0.9880 - val_loss: 2.9821 - val_accuracy: 0.5620\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 425us/step - loss: 0.0339 - accuracy: 0.9900 - val_loss: 2.9680 - val_accuracy: 0.5620\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0245 - accuracy: 0.9935 - val_loss: 2.9802 - val_accuracy: 0.5640\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0440 - accuracy: 0.9890 - val_loss: 2.9796 - val_accuracy: 0.5660\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0372 - accuracy: 0.9875 - val_loss: 2.9851 - val_accuracy: 0.5660\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0360 - accuracy: 0.9880 - val_loss: 2.9868 - val_accuracy: 0.5660\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0342 - accuracy: 0.9910 - val_loss: 2.9961 - val_accuracy: 0.5680\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0337 - accuracy: 0.9870 - val_loss: 2.9727 - val_accuracy: 0.5680\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0299 - accuracy: 0.9895 - val_loss: 2.9761 - val_accuracy: 0.5660\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0442 - accuracy: 0.9870 - val_loss: 2.9805 - val_accuracy: 0.5660\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 446us/step - loss: 0.0371 - accuracy: 0.9890 - val_loss: 2.9881 - val_accuracy: 0.5660\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0364 - accuracy: 0.9890 - val_loss: 2.9841 - val_accuracy: 0.5660\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0292 - accuracy: 0.9910 - val_loss: 2.9840 - val_accuracy: 0.5660\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0349 - accuracy: 0.9865 - val_loss: 2.9714 - val_accuracy: 0.5660\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0347 - accuracy: 0.9900 - val_loss: 2.9910 - val_accuracy: 0.5660\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 2.9859 - val_accuracy: 0.5640\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 2.9813 - val_accuracy: 0.5660\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0482 - accuracy: 0.9850 - val_loss: 2.9716 - val_accuracy: 0.5660\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 2.9784 - val_accuracy: 0.5660\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0338 - accuracy: 0.9910 - val_loss: 2.9973 - val_accuracy: 0.5660\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0308 - accuracy: 0.9910 - val_loss: 2.9755 - val_accuracy: 0.5660\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0320 - accuracy: 0.9925 - val_loss: 2.9790 - val_accuracy: 0.5660\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0348 - accuracy: 0.9875 - val_loss: 2.9764 - val_accuracy: 0.5660\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0303 - accuracy: 0.9915 - val_loss: 2.9613 - val_accuracy: 0.5700\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0455 - accuracy: 0.9910 - val_loss: 2.9700 - val_accuracy: 0.5680\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0457 - accuracy: 0.9845 - val_loss: 2.9739 - val_accuracy: 0.5680\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0461 - accuracy: 0.9895 - val_loss: 2.9772 - val_accuracy: 0.5660\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0273 - accuracy: 0.9930 - val_loss: 2.9859 - val_accuracy: 0.5660\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 2.9877 - val_accuracy: 0.5640\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0448 - accuracy: 0.9920 - val_loss: 2.9720 - val_accuracy: 0.5620\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0374 - accuracy: 0.9900 - val_loss: 2.9846 - val_accuracy: 0.5660\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0276 - accuracy: 0.9900 - val_loss: 2.9847 - val_accuracy: 0.5640\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 421us/step - loss: 0.0476 - accuracy: 0.9880 - val_loss: 2.9978 - val_accuracy: 0.5680\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 447us/step - loss: 0.0470 - accuracy: 0.9880 - val_loss: 2.9822 - val_accuracy: 0.5660\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0389 - accuracy: 0.9875 - val_loss: 2.9937 - val_accuracy: 0.5660\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 2.9733 - val_accuracy: 0.5640\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 2.9692 - val_accuracy: 0.5660\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0468 - accuracy: 0.9850 - val_loss: 2.9701 - val_accuracy: 0.5660\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0419 - accuracy: 0.9855 - val_loss: 2.9715 - val_accuracy: 0.5680\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0375 - accuracy: 0.9905 - val_loss: 2.9657 - val_accuracy: 0.5680\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0312 - accuracy: 0.9895 - val_loss: 2.9747 - val_accuracy: 0.5700\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0334 - accuracy: 0.9880 - val_loss: 2.9573 - val_accuracy: 0.5640\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0343 - accuracy: 0.9900 - val_loss: 2.9723 - val_accuracy: 0.5680\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0305 - accuracy: 0.9890 - val_loss: 2.9854 - val_accuracy: 0.5660\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0284 - accuracy: 0.9930 - val_loss: 2.9762 - val_accuracy: 0.5640\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0348 - accuracy: 0.9910 - val_loss: 2.9628 - val_accuracy: 0.5640\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 2.9542 - val_accuracy: 0.5660\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0324 - accuracy: 0.9885 - val_loss: 2.9830 - val_accuracy: 0.5680\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0308 - accuracy: 0.9880 - val_loss: 2.9818 - val_accuracy: 0.5680\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0397 - accuracy: 0.9875 - val_loss: 2.9736 - val_accuracy: 0.5640\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 2.9897 - val_accuracy: 0.5680\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0316 - accuracy: 0.9900 - val_loss: 2.9585 - val_accuracy: 0.5640\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 445us/step - loss: 0.0358 - accuracy: 0.9930 - val_loss: 2.9755 - val_accuracy: 0.5660\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 443us/step - loss: 0.0295 - accuracy: 0.9900 - val_loss: 2.9641 - val_accuracy: 0.5660\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0354 - accuracy: 0.9870 - val_loss: 2.9740 - val_accuracy: 0.5660\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0292 - accuracy: 0.9905 - val_loss: 2.9634 - val_accuracy: 0.5660\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0313 - accuracy: 0.9890 - val_loss: 2.9616 - val_accuracy: 0.5640\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0358 - accuracy: 0.9880 - val_loss: 2.9962 - val_accuracy: 0.5660\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0447 - accuracy: 0.9845 - val_loss: 2.9984 - val_accuracy: 0.5660\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0395 - accuracy: 0.9895 - val_loss: 2.9771 - val_accuracy: 0.5680\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0374 - accuracy: 0.9885 - val_loss: 2.9711 - val_accuracy: 0.5660\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0505 - accuracy: 0.9870 - val_loss: 2.9901 - val_accuracy: 0.5660\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0349 - accuracy: 0.9890 - val_loss: 2.9899 - val_accuracy: 0.5660\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0340 - accuracy: 0.9895 - val_loss: 2.9814 - val_accuracy: 0.5680\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0335 - accuracy: 0.9875 - val_loss: 2.9889 - val_accuracy: 0.5640\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0415 - accuracy: 0.9885 - val_loss: 2.9847 - val_accuracy: 0.5640\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0392 - accuracy: 0.9870 - val_loss: 2.9846 - val_accuracy: 0.5640\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0304 - accuracy: 0.9915 - val_loss: 2.9847 - val_accuracy: 0.5680\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0382 - accuracy: 0.9895 - val_loss: 2.9806 - val_accuracy: 0.5640\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0279 - accuracy: 0.9925 - val_loss: 2.9923 - val_accuracy: 0.5660\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 2.9732 - val_accuracy: 0.5660\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0382 - accuracy: 0.9855 - val_loss: 2.9792 - val_accuracy: 0.5660\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 447us/step - loss: 0.0347 - accuracy: 0.9880 - val_loss: 2.9835 - val_accuracy: 0.5640\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0340 - accuracy: 0.9905 - val_loss: 2.9704 - val_accuracy: 0.5660\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 2.9887 - val_accuracy: 0.5680\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0302 - accuracy: 0.9885 - val_loss: 2.9873 - val_accuracy: 0.5660\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0319 - accuracy: 0.9905 - val_loss: 2.9961 - val_accuracy: 0.5680\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0464 - accuracy: 0.9840 - val_loss: 2.9837 - val_accuracy: 0.5700\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0355 - accuracy: 0.9885 - val_loss: 2.9677 - val_accuracy: 0.5640\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0401 - accuracy: 0.9850 - val_loss: 2.9618 - val_accuracy: 0.5620\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0355 - accuracy: 0.9870 - val_loss: 2.9896 - val_accuracy: 0.5620\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0348 - accuracy: 0.9890 - val_loss: 2.9872 - val_accuracy: 0.5640\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 581us/step - loss: 0.0461 - accuracy: 0.9875 - val_loss: 3.1538 - val_accuracy: 0.5560\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0561 - accuracy: 0.9845 - val_loss: 3.1612 - val_accuracy: 0.5680\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0411 - accuracy: 0.9855 - val_loss: 3.2383 - val_accuracy: 0.5560\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0503 - accuracy: 0.9855 - val_loss: 3.2377 - val_accuracy: 0.5500\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0479 - accuracy: 0.9855 - val_loss: 3.2666 - val_accuracy: 0.5560\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0501 - accuracy: 0.9850 - val_loss: 3.2489 - val_accuracy: 0.5520\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0530 - accuracy: 0.9815 - val_loss: 3.1644 - val_accuracy: 0.5580\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0499 - accuracy: 0.9830 - val_loss: 3.1289 - val_accuracy: 0.5580\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0583 - accuracy: 0.9830 - val_loss: 3.1609 - val_accuracy: 0.5540\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0466 - accuracy: 0.9855 - val_loss: 3.1706 - val_accuracy: 0.5560\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0695 - accuracy: 0.9800 - val_loss: 3.1294 - val_accuracy: 0.5440\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0438 - accuracy: 0.9900 - val_loss: 3.0896 - val_accuracy: 0.5520\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 327us/step - loss: 0.0407 - accuracy: 0.9865 - val_loss: 3.0706 - val_accuracy: 0.5640\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.0491 - accuracy: 0.9855 - val_loss: 3.1040 - val_accuracy: 0.5620\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0724 - accuracy: 0.9800 - val_loss: 3.0459 - val_accuracy: 0.5580\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0497 - accuracy: 0.9840 - val_loss: 3.0278 - val_accuracy: 0.5540\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0556 - accuracy: 0.9835 - val_loss: 2.9882 - val_accuracy: 0.5640\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0421 - accuracy: 0.9895 - val_loss: 2.9814 - val_accuracy: 0.5640\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0438 - accuracy: 0.9885 - val_loss: 3.0270 - val_accuracy: 0.5560\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0547 - accuracy: 0.9800 - val_loss: 3.0294 - val_accuracy: 0.5600\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0434 - accuracy: 0.9870 - val_loss: 3.0567 - val_accuracy: 0.5540\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 326us/step - loss: 0.0396 - accuracy: 0.9880 - val_loss: 3.1049 - val_accuracy: 0.5480\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0458 - accuracy: 0.9870 - val_loss: 3.0962 - val_accuracy: 0.5500\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0408 - accuracy: 0.9885 - val_loss: 3.1283 - val_accuracy: 0.5600\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0480 - accuracy: 0.9865 - val_loss: 3.1519 - val_accuracy: 0.5640\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.0469 - accuracy: 0.9865 - val_loss: 3.1232 - val_accuracy: 0.5580\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.0453 - accuracy: 0.9840 - val_loss: 3.0848 - val_accuracy: 0.5680\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0338 - accuracy: 0.9870 - val_loss: 3.0857 - val_accuracy: 0.5680\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0502 - accuracy: 0.9815 - val_loss: 3.0578 - val_accuracy: 0.5660\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 3.0339 - val_accuracy: 0.5620\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0383 - accuracy: 0.9910 - val_loss: 3.0134 - val_accuracy: 0.5580\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0415 - accuracy: 0.9845 - val_loss: 3.0092 - val_accuracy: 0.5620\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0474 - accuracy: 0.9870 - val_loss: 2.9978 - val_accuracy: 0.5640\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0428 - accuracy: 0.9880 - val_loss: 3.0256 - val_accuracy: 0.5660\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.0469 - accuracy: 0.9840 - val_loss: 3.0574 - val_accuracy: 0.5600\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.0486 - accuracy: 0.9850 - val_loss: 3.0944 - val_accuracy: 0.5540\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0311 - accuracy: 0.9890 - val_loss: 3.1225 - val_accuracy: 0.5560\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0610 - accuracy: 0.9790 - val_loss: 3.0521 - val_accuracy: 0.5600\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0440 - accuracy: 0.9860 - val_loss: 3.0358 - val_accuracy: 0.5620\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 327us/step - loss: 0.0641 - accuracy: 0.9845 - val_loss: 3.0420 - val_accuracy: 0.5600\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 3.0738 - val_accuracy: 0.5600\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0458 - accuracy: 0.9860 - val_loss: 3.0879 - val_accuracy: 0.5680\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0479 - accuracy: 0.9850 - val_loss: 3.0342 - val_accuracy: 0.5620\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 3.0271 - val_accuracy: 0.5640\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.0487 - accuracy: 0.9900 - val_loss: 3.0152 - val_accuracy: 0.5660\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0450 - accuracy: 0.9850 - val_loss: 3.0141 - val_accuracy: 0.5660\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 3.0000 - val_accuracy: 0.5620\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.0436 - accuracy: 0.9855 - val_loss: 3.0206 - val_accuracy: 0.5520\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 3.0107 - val_accuracy: 0.5480\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0381 - accuracy: 0.9895 - val_loss: 2.9800 - val_accuracy: 0.5460\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0495 - accuracy: 0.9870 - val_loss: 3.0512 - val_accuracy: 0.5500\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.0497 - accuracy: 0.9855 - val_loss: 3.0802 - val_accuracy: 0.5640\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0565 - accuracy: 0.9835 - val_loss: 3.1218 - val_accuracy: 0.5700\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0512 - accuracy: 0.9850 - val_loss: 3.1475 - val_accuracy: 0.5600\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0379 - accuracy: 0.9910 - val_loss: 3.0998 - val_accuracy: 0.5580\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 3.0681 - val_accuracy: 0.5700\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 3.0738 - val_accuracy: 0.5600\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0505 - accuracy: 0.9820 - val_loss: 3.1133 - val_accuracy: 0.5620\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0450 - accuracy: 0.9850 - val_loss: 3.1196 - val_accuracy: 0.5680\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 3.1617 - val_accuracy: 0.5680\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0478 - accuracy: 0.9875 - val_loss: 3.1561 - val_accuracy: 0.5640\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0319 - accuracy: 0.9895 - val_loss: 3.1158 - val_accuracy: 0.5620\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0326 - accuracy: 0.9875 - val_loss: 3.0613 - val_accuracy: 0.5660\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0439 - accuracy: 0.9870 - val_loss: 3.0586 - val_accuracy: 0.5620\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0536 - accuracy: 0.9840 - val_loss: 3.0018 - val_accuracy: 0.5660\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0575 - accuracy: 0.9850 - val_loss: 2.9547 - val_accuracy: 0.5680\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0467 - accuracy: 0.9875 - val_loss: 2.9867 - val_accuracy: 0.5640\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0441 - accuracy: 0.9890 - val_loss: 3.0012 - val_accuracy: 0.5640\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0484 - accuracy: 0.9850 - val_loss: 3.0124 - val_accuracy: 0.5500\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0435 - accuracy: 0.9880 - val_loss: 3.0623 - val_accuracy: 0.5580\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0321 - accuracy: 0.9915 - val_loss: 3.0860 - val_accuracy: 0.5540\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.0450 - accuracy: 0.9860 - val_loss: 3.0989 - val_accuracy: 0.5540\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0476 - accuracy: 0.9870 - val_loss: 3.1349 - val_accuracy: 0.5540\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0359 - accuracy: 0.9890 - val_loss: 3.1674 - val_accuracy: 0.5580\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0441 - accuracy: 0.9850 - val_loss: 3.1156 - val_accuracy: 0.5620\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0545 - accuracy: 0.9845 - val_loss: 3.0501 - val_accuracy: 0.5600\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0315 - accuracy: 0.9900 - val_loss: 3.0505 - val_accuracy: 0.5580\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0600 - accuracy: 0.9840 - val_loss: 3.0577 - val_accuracy: 0.5680\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0349 - accuracy: 0.9880 - val_loss: 3.0424 - val_accuracy: 0.5580\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0526 - accuracy: 0.9840 - val_loss: 2.9856 - val_accuracy: 0.5540\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0563 - accuracy: 0.9850 - val_loss: 2.9199 - val_accuracy: 0.5600\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0472 - accuracy: 0.9890 - val_loss: 2.9401 - val_accuracy: 0.5580\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0430 - accuracy: 0.9870 - val_loss: 2.8872 - val_accuracy: 0.5520\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.0291 - accuracy: 0.9900 - val_loss: 2.8939 - val_accuracy: 0.5500\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0449 - accuracy: 0.9865 - val_loss: 2.8964 - val_accuracy: 0.5560\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0499 - accuracy: 0.9865 - val_loss: 2.9919 - val_accuracy: 0.5560\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0469 - accuracy: 0.9885 - val_loss: 2.9895 - val_accuracy: 0.5540\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 2.9633 - val_accuracy: 0.5580\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0544 - accuracy: 0.9805 - val_loss: 3.0109 - val_accuracy: 0.5640\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0525 - accuracy: 0.9855 - val_loss: 3.0234 - val_accuracy: 0.5660\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0487 - accuracy: 0.9870 - val_loss: 3.0575 - val_accuracy: 0.5620\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0441 - accuracy: 0.9895 - val_loss: 3.0574 - val_accuracy: 0.5640\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0374 - accuracy: 0.9880 - val_loss: 2.9897 - val_accuracy: 0.5680\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0498 - accuracy: 0.9870 - val_loss: 2.9534 - val_accuracy: 0.5620\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0493 - accuracy: 0.9860 - val_loss: 2.9687 - val_accuracy: 0.5620\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0567 - accuracy: 0.9850 - val_loss: 2.9692 - val_accuracy: 0.5640\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0358 - accuracy: 0.9880 - val_loss: 3.0164 - val_accuracy: 0.5680\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0405 - accuracy: 0.9895 - val_loss: 3.0127 - val_accuracy: 0.5640\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0365 - accuracy: 0.9885 - val_loss: 3.0307 - val_accuracy: 0.5500\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 3.0323 - val_accuracy: 0.5620\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0338 - accuracy: 0.9910 - val_loss: 3.0226 - val_accuracy: 0.5700\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 2.9973 - val_accuracy: 0.5660\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0404 - accuracy: 0.9875 - val_loss: 2.9762 - val_accuracy: 0.5680\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 2.9857 - val_accuracy: 0.5720\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.0295 - accuracy: 0.9900 - val_loss: 2.9802 - val_accuracy: 0.5740\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0370 - accuracy: 0.9870 - val_loss: 2.9990 - val_accuracy: 0.5780\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0321 - accuracy: 0.9900 - val_loss: 3.0433 - val_accuracy: 0.5700\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0336 - accuracy: 0.9905 - val_loss: 3.0575 - val_accuracy: 0.5680\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0277 - accuracy: 0.9920 - val_loss: 3.1200 - val_accuracy: 0.5680\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.0340 - accuracy: 0.9890 - val_loss: 3.1318 - val_accuracy: 0.5780\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0336 - accuracy: 0.9895 - val_loss: 3.1077 - val_accuracy: 0.5740\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0331 - accuracy: 0.9890 - val_loss: 3.0462 - val_accuracy: 0.5780\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0296 - accuracy: 0.9930 - val_loss: 3.0304 - val_accuracy: 0.5760\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0425 - accuracy: 0.9870 - val_loss: 3.0187 - val_accuracy: 0.5700\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 2.9856 - val_accuracy: 0.5680\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0314 - accuracy: 0.9905 - val_loss: 2.9845 - val_accuracy: 0.5720\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 3.0078 - val_accuracy: 0.5640\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0373 - accuracy: 0.9935 - val_loss: 3.0264 - val_accuracy: 0.5660\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0383 - accuracy: 0.9920 - val_loss: 3.0251 - val_accuracy: 0.5680\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0380 - accuracy: 0.9885 - val_loss: 2.9807 - val_accuracy: 0.5680\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 3.0414 - val_accuracy: 0.5600\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0252 - accuracy: 0.9935 - val_loss: 3.1048 - val_accuracy: 0.5540\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0347 - accuracy: 0.9890 - val_loss: 3.1695 - val_accuracy: 0.5600\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0279 - accuracy: 0.9930 - val_loss: 3.2058 - val_accuracy: 0.5620\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0482 - accuracy: 0.9905 - val_loss: 3.1864 - val_accuracy: 0.5720\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0274 - accuracy: 0.9910 - val_loss: 3.1358 - val_accuracy: 0.5680\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0256 - accuracy: 0.9905 - val_loss: 3.0934 - val_accuracy: 0.5720\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.0435 - accuracy: 0.9895 - val_loss: 3.0876 - val_accuracy: 0.5720\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0387 - accuracy: 0.9900 - val_loss: 3.0740 - val_accuracy: 0.5760\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0467 - accuracy: 0.9850 - val_loss: 3.1122 - val_accuracy: 0.5740\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 3.1643 - val_accuracy: 0.5700\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0433 - accuracy: 0.9875 - val_loss: 3.2135 - val_accuracy: 0.5620\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0305 - accuracy: 0.9905 - val_loss: 3.2080 - val_accuracy: 0.5680\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 3.1894 - val_accuracy: 0.5680\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0296 - accuracy: 0.9920 - val_loss: 3.1860 - val_accuracy: 0.5720\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0401 - accuracy: 0.9910 - val_loss: 3.2031 - val_accuracy: 0.5760\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.0250 - accuracy: 0.9925 - val_loss: 3.2490 - val_accuracy: 0.5720\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0422 - accuracy: 0.9890 - val_loss: 3.1837 - val_accuracy: 0.5820\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 3.1911 - val_accuracy: 0.5760\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0438 - accuracy: 0.9880 - val_loss: 3.2212 - val_accuracy: 0.5760\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0370 - accuracy: 0.9905 - val_loss: 3.2319 - val_accuracy: 0.5740\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 3.2260 - val_accuracy: 0.5560\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0272 - accuracy: 0.9930 - val_loss: 3.2184 - val_accuracy: 0.5560\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0277 - accuracy: 0.9955 - val_loss: 3.2074 - val_accuracy: 0.5520\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0484 - accuracy: 0.9850 - val_loss: 3.2028 - val_accuracy: 0.5540\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 321us/step - loss: 0.0252 - accuracy: 0.9935 - val_loss: 3.2462 - val_accuracy: 0.5480\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0342 - accuracy: 0.9915 - val_loss: 3.2391 - val_accuracy: 0.5420\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.0394 - accuracy: 0.9880 - val_loss: 3.2315 - val_accuracy: 0.5480\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 317us/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 3.2159 - val_accuracy: 0.5440\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0264 - accuracy: 0.9930 - val_loss: 3.2124 - val_accuracy: 0.5560\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0380 - accuracy: 0.9860 - val_loss: 3.2043 - val_accuracy: 0.5500\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0310 - accuracy: 0.9905 - val_loss: 3.1854 - val_accuracy: 0.5560\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 3.1664 - val_accuracy: 0.5660\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0548 - accuracy: 0.9870 - val_loss: 3.1342 - val_accuracy: 0.5660\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.0283 - accuracy: 0.9925 - val_loss: 3.1674 - val_accuracy: 0.5540\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 326us/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 3.1432 - val_accuracy: 0.5500\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 349us/step - loss: 0.0334 - accuracy: 0.9900 - val_loss: 3.1161 - val_accuracy: 0.5600\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0215 - accuracy: 0.9930 - val_loss: 3.1378 - val_accuracy: 0.5600\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0403 - accuracy: 0.9880 - val_loss: 3.1358 - val_accuracy: 0.5600\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0294 - accuracy: 0.9925 - val_loss: 3.1085 - val_accuracy: 0.5640\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 349us/step - loss: 0.0491 - accuracy: 0.9885 - val_loss: 3.0932 - val_accuracy: 0.5660\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 318us/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 3.1222 - val_accuracy: 0.5540\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 3.1103 - val_accuracy: 0.5600\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0225 - accuracy: 0.9935 - val_loss: 3.1500 - val_accuracy: 0.5700\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0326 - accuracy: 0.9920 - val_loss: 3.2134 - val_accuracy: 0.5740\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 349us/step - loss: 0.0334 - accuracy: 0.9905 - val_loss: 3.2035 - val_accuracy: 0.5780\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0333 - accuracy: 0.9890 - val_loss: 3.2014 - val_accuracy: 0.5740\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 3.1574 - val_accuracy: 0.5760\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 3.1605 - val_accuracy: 0.5700\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0319 - accuracy: 0.9890 - val_loss: 3.1179 - val_accuracy: 0.5700\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 3.0324 - val_accuracy: 0.5720\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0418 - accuracy: 0.9895 - val_loss: 3.0177 - val_accuracy: 0.5700\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 321us/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 3.0237 - val_accuracy: 0.5720\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0415 - accuracy: 0.9890 - val_loss: 3.0063 - val_accuracy: 0.5700\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 3.0414 - val_accuracy: 0.5620\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0501 - accuracy: 0.9860 - val_loss: 3.0087 - val_accuracy: 0.5700\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 322us/step - loss: 0.0299 - accuracy: 0.9895 - val_loss: 3.0176 - val_accuracy: 0.5800\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 318us/step - loss: 0.0335 - accuracy: 0.9910 - val_loss: 3.0193 - val_accuracy: 0.5820\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 3.0935 - val_accuracy: 0.5680\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 3.1600 - val_accuracy: 0.5660\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.0306 - accuracy: 0.9900 - val_loss: 3.1654 - val_accuracy: 0.5640\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.0368 - accuracy: 0.9920 - val_loss: 3.1468 - val_accuracy: 0.5720\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.0307 - accuracy: 0.9915 - val_loss: 3.1492 - val_accuracy: 0.5760\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0284 - accuracy: 0.9915 - val_loss: 3.1550 - val_accuracy: 0.5740\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0225 - accuracy: 0.9935 - val_loss: 3.2021 - val_accuracy: 0.5780\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0298 - accuracy: 0.9910 - val_loss: 3.2327 - val_accuracy: 0.5820\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0318 - accuracy: 0.9880 - val_loss: 3.2405 - val_accuracy: 0.5760\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0276 - accuracy: 0.9925 - val_loss: 3.2003 - val_accuracy: 0.5680\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0331 - accuracy: 0.9915 - val_loss: 3.2142 - val_accuracy: 0.5660\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0304 - accuracy: 0.9915 - val_loss: 3.1989 - val_accuracy: 0.5620\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 318us/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 3.2355 - val_accuracy: 0.5640\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0386 - accuracy: 0.9875 - val_loss: 3.3034 - val_accuracy: 0.5680\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0288 - accuracy: 0.9920 - val_loss: 3.3332 - val_accuracy: 0.5680\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0274 - accuracy: 0.9915 - val_loss: 3.3307 - val_accuracy: 0.5600\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 321us/step - loss: 0.0350 - accuracy: 0.9910 - val_loss: 3.3302 - val_accuracy: 0.5600\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 3.2972 - val_accuracy: 0.5600\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0357 - accuracy: 0.9915 - val_loss: 3.2825 - val_accuracy: 0.5560\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 3.2261 - val_accuracy: 0.5600\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.0414 - accuracy: 0.9900 - val_loss: 3.1356 - val_accuracy: 0.5740\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0306 - accuracy: 0.9885 - val_loss: 3.1084 - val_accuracy: 0.5680\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 600us/step - loss: 0.0266 - accuracy: 0.9900 - val_loss: 3.1045 - val_accuracy: 0.5660\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0304 - accuracy: 0.9940 - val_loss: 3.1296 - val_accuracy: 0.5740\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 3.1418 - val_accuracy: 0.5700\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 3.1642 - val_accuracy: 0.5740\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0353 - accuracy: 0.9910 - val_loss: 3.1606 - val_accuracy: 0.5760\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0325 - accuracy: 0.9910 - val_loss: 3.1318 - val_accuracy: 0.5720\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 3.1202 - val_accuracy: 0.5780\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 441us/step - loss: 0.0337 - accuracy: 0.9900 - val_loss: 3.1298 - val_accuracy: 0.5760\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0313 - accuracy: 0.9925 - val_loss: 3.1013 - val_accuracy: 0.5720\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0296 - accuracy: 0.9905 - val_loss: 3.1162 - val_accuracy: 0.5740\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0232 - accuracy: 0.9940 - val_loss: 3.1151 - val_accuracy: 0.5720\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0276 - accuracy: 0.9930 - val_loss: 3.1077 - val_accuracy: 0.5700\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0254 - accuracy: 0.9925 - val_loss: 3.1181 - val_accuracy: 0.5720\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0298 - accuracy: 0.9925 - val_loss: 3.1188 - val_accuracy: 0.5720\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0310 - accuracy: 0.9905 - val_loss: 3.1196 - val_accuracy: 0.5700\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0485 - accuracy: 0.9895 - val_loss: 3.1352 - val_accuracy: 0.5680\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0242 - accuracy: 0.9930 - val_loss: 3.1360 - val_accuracy: 0.5680\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0357 - accuracy: 0.9910 - val_loss: 3.1076 - val_accuracy: 0.5740\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0342 - accuracy: 0.9920 - val_loss: 3.1254 - val_accuracy: 0.5720\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0221 - accuracy: 0.9935 - val_loss: 3.1230 - val_accuracy: 0.5720\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 3.1267 - val_accuracy: 0.5780\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0218 - accuracy: 0.9955 - val_loss: 3.1036 - val_accuracy: 0.5780\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0206 - accuracy: 0.9895 - val_loss: 3.1043 - val_accuracy: 0.5760\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0254 - accuracy: 0.9910 - val_loss: 3.0995 - val_accuracy: 0.5780\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0272 - accuracy: 0.9925 - val_loss: 3.1281 - val_accuracy: 0.5760\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 3.1069 - val_accuracy: 0.5740\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0193 - accuracy: 0.9945 - val_loss: 3.0892 - val_accuracy: 0.5740\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0238 - accuracy: 0.9935 - val_loss: 3.1040 - val_accuracy: 0.5760\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0182 - accuracy: 0.9935 - val_loss: 3.1182 - val_accuracy: 0.5800\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.0363 - accuracy: 0.9885 - val_loss: 3.1427 - val_accuracy: 0.5820\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0119 - accuracy: 0.9970 - val_loss: 3.1320 - val_accuracy: 0.5800\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0205 - accuracy: 0.9945 - val_loss: 3.1069 - val_accuracy: 0.5740\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0296 - accuracy: 0.9930 - val_loss: 3.1051 - val_accuracy: 0.5840\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0301 - accuracy: 0.9900 - val_loss: 3.1072 - val_accuracy: 0.5800\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0493 - accuracy: 0.9870 - val_loss: 3.0915 - val_accuracy: 0.5760\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 3.1353 - val_accuracy: 0.5720\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0224 - accuracy: 0.9945 - val_loss: 3.1044 - val_accuracy: 0.5800\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0138 - accuracy: 0.9975 - val_loss: 3.1311 - val_accuracy: 0.5800\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 3.0832 - val_accuracy: 0.5840\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 3.1112 - val_accuracy: 0.5800\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 3.1258 - val_accuracy: 0.5760\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0397 - accuracy: 0.9920 - val_loss: 3.0851 - val_accuracy: 0.5800\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0283 - accuracy: 0.9935 - val_loss: 3.1207 - val_accuracy: 0.5820\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0198 - accuracy: 0.9940 - val_loss: 3.1208 - val_accuracy: 0.5800\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 3.1219 - val_accuracy: 0.5720\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 3.0885 - val_accuracy: 0.5840\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 3.1041 - val_accuracy: 0.5780\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 3.1055 - val_accuracy: 0.5760\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0222 - accuracy: 0.9915 - val_loss: 3.1236 - val_accuracy: 0.5780\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 437us/step - loss: 0.0199 - accuracy: 0.9955 - val_loss: 3.1352 - val_accuracy: 0.5800\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0273 - accuracy: 0.9935 - val_loss: 3.1324 - val_accuracy: 0.5800\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 3.1298 - val_accuracy: 0.5800\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0157 - accuracy: 0.9960 - val_loss: 3.1412 - val_accuracy: 0.5800\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 3.1601 - val_accuracy: 0.5800\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0388 - accuracy: 0.9895 - val_loss: 3.1299 - val_accuracy: 0.5840\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0198 - accuracy: 0.9945 - val_loss: 3.1303 - val_accuracy: 0.5720\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0394 - accuracy: 0.9905 - val_loss: 3.1232 - val_accuracy: 0.5740\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0244 - accuracy: 0.9935 - val_loss: 3.1308 - val_accuracy: 0.5720\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0291 - accuracy: 0.9920 - val_loss: 3.1115 - val_accuracy: 0.5660\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 3.1001 - val_accuracy: 0.5680\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0228 - accuracy: 0.9935 - val_loss: 3.0995 - val_accuracy: 0.5700\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0130 - accuracy: 0.9960 - val_loss: 3.1217 - val_accuracy: 0.5700\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0196 - accuracy: 0.9965 - val_loss: 3.1471 - val_accuracy: 0.5720\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0234 - accuracy: 0.9950 - val_loss: 3.1334 - val_accuracy: 0.5740\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0181 - accuracy: 0.9950 - val_loss: 3.1355 - val_accuracy: 0.5760\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0269 - accuracy: 0.9910 - val_loss: 3.1174 - val_accuracy: 0.5760\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 3.1054 - val_accuracy: 0.5780\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0355 - accuracy: 0.9865 - val_loss: 3.1549 - val_accuracy: 0.5720\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0240 - accuracy: 0.9945 - val_loss: 3.1650 - val_accuracy: 0.5720\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 3.1404 - val_accuracy: 0.5680\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0228 - accuracy: 0.9955 - val_loss: 3.1432 - val_accuracy: 0.5700\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0175 - accuracy: 0.9945 - val_loss: 3.1345 - val_accuracy: 0.5780\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 3.1293 - val_accuracy: 0.5760\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 3.1258 - val_accuracy: 0.5720\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 3.1440 - val_accuracy: 0.5680\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0150 - accuracy: 0.9960 - val_loss: 3.1354 - val_accuracy: 0.5700\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0243 - accuracy: 0.9945 - val_loss: 3.1172 - val_accuracy: 0.5700\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0269 - accuracy: 0.9930 - val_loss: 3.1452 - val_accuracy: 0.5680\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0248 - accuracy: 0.9920 - val_loss: 3.1244 - val_accuracy: 0.5680\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0256 - accuracy: 0.9930 - val_loss: 3.1363 - val_accuracy: 0.5700\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 3.1440 - val_accuracy: 0.5700\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0174 - accuracy: 0.9935 - val_loss: 3.1722 - val_accuracy: 0.5740\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0197 - accuracy: 0.9950 - val_loss: 3.1695 - val_accuracy: 0.5740\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0218 - accuracy: 0.9925 - val_loss: 3.1737 - val_accuracy: 0.5700\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0190 - accuracy: 0.9955 - val_loss: 3.1446 - val_accuracy: 0.5660\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0159 - accuracy: 0.9955 - val_loss: 3.1334 - val_accuracy: 0.5680\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0221 - accuracy: 0.9950 - val_loss: 3.1805 - val_accuracy: 0.5680\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0241 - accuracy: 0.9935 - val_loss: 3.1440 - val_accuracy: 0.5740\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0269 - accuracy: 0.9910 - val_loss: 3.1810 - val_accuracy: 0.5760\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0224 - accuracy: 0.9960 - val_loss: 3.1591 - val_accuracy: 0.5740\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0217 - accuracy: 0.9935 - val_loss: 3.1344 - val_accuracy: 0.5740\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 3.1187 - val_accuracy: 0.5760\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 3.1410 - val_accuracy: 0.5740\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 3.1458 - val_accuracy: 0.5720\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0259 - accuracy: 0.9960 - val_loss: 3.1515 - val_accuracy: 0.5760\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 3.1477 - val_accuracy: 0.5740\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 3.1327 - val_accuracy: 0.5720\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0136 - accuracy: 0.9940 - val_loss: 3.1704 - val_accuracy: 0.5700\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0218 - accuracy: 0.9935 - val_loss: 3.1682 - val_accuracy: 0.5700\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0145 - accuracy: 0.9935 - val_loss: 3.1934 - val_accuracy: 0.5720\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 3.2006 - val_accuracy: 0.5680\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0160 - accuracy: 0.9955 - val_loss: 3.2093 - val_accuracy: 0.5720\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0212 - accuracy: 0.9945 - val_loss: 3.1955 - val_accuracy: 0.5740\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0166 - accuracy: 0.9935 - val_loss: 3.2002 - val_accuracy: 0.5680\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 3.1770 - val_accuracy: 0.5700\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 3.1740 - val_accuracy: 0.5720\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0204 - accuracy: 0.9935 - val_loss: 3.1840 - val_accuracy: 0.5700\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 3.1640 - val_accuracy: 0.5760\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 3.1888 - val_accuracy: 0.5720\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0248 - accuracy: 0.9945 - val_loss: 3.1900 - val_accuracy: 0.5740\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 3.1804 - val_accuracy: 0.5780\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0200 - accuracy: 0.9950 - val_loss: 3.2032 - val_accuracy: 0.5720\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 3.1903 - val_accuracy: 0.5720\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0152 - accuracy: 0.9935 - val_loss: 3.1760 - val_accuracy: 0.5780\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0173 - accuracy: 0.9965 - val_loss: 3.1913 - val_accuracy: 0.5700\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0217 - accuracy: 0.9940 - val_loss: 3.2082 - val_accuracy: 0.5680\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 3.2193 - val_accuracy: 0.5680\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0214 - accuracy: 0.9935 - val_loss: 3.2329 - val_accuracy: 0.5680\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0256 - accuracy: 0.9940 - val_loss: 3.2300 - val_accuracy: 0.5700\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0150 - accuracy: 0.9965 - val_loss: 3.2157 - val_accuracy: 0.5700\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0175 - accuracy: 0.9965 - val_loss: 3.2060 - val_accuracy: 0.5660\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0087 - accuracy: 0.9990 - val_loss: 3.2477 - val_accuracy: 0.5700\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0258 - accuracy: 0.9910 - val_loss: 3.2263 - val_accuracy: 0.5760\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 3.2585 - val_accuracy: 0.5740\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0223 - accuracy: 0.9955 - val_loss: 3.2183 - val_accuracy: 0.5740\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 3.2111 - val_accuracy: 0.5780\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0218 - accuracy: 0.9950 - val_loss: 3.2211 - val_accuracy: 0.5760\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0252 - accuracy: 0.9915 - val_loss: 3.2525 - val_accuracy: 0.5760\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0237 - accuracy: 0.9965 - val_loss: 3.2514 - val_accuracy: 0.5720\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0173 - accuracy: 0.9935 - val_loss: 3.2333 - val_accuracy: 0.5780\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0288 - accuracy: 0.9940 - val_loss: 3.2413 - val_accuracy: 0.5760\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0174 - accuracy: 0.9950 - val_loss: 3.2136 - val_accuracy: 0.5760\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0268 - accuracy: 0.9935 - val_loss: 3.2364 - val_accuracy: 0.5760\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 433us/step - loss: 0.0164 - accuracy: 0.9940 - val_loss: 3.2482 - val_accuracy: 0.5780\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0158 - accuracy: 0.9960 - val_loss: 3.2383 - val_accuracy: 0.5760\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 3.2058 - val_accuracy: 0.5800\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 3.2417 - val_accuracy: 0.5820\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0180 - accuracy: 0.9955 - val_loss: 3.2322 - val_accuracy: 0.5760\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0187 - accuracy: 0.9960 - val_loss: 3.2109 - val_accuracy: 0.5800\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0158 - accuracy: 0.9945 - val_loss: 3.2500 - val_accuracy: 0.5780\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0195 - accuracy: 0.9935 - val_loss: 3.2349 - val_accuracy: 0.5780\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0198 - accuracy: 0.9965 - val_loss: 3.2588 - val_accuracy: 0.5780\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 3.2510 - val_accuracy: 0.5740\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0264 - accuracy: 0.9925 - val_loss: 3.2235 - val_accuracy: 0.5760\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0212 - accuracy: 0.9950 - val_loss: 3.2469 - val_accuracy: 0.5720\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 3.2495 - val_accuracy: 0.5700\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 3.2116 - val_accuracy: 0.5740\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0249 - accuracy: 0.9940 - val_loss: 3.2263 - val_accuracy: 0.5740\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 3.2306 - val_accuracy: 0.5720\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0163 - accuracy: 0.9965 - val_loss: 3.2330 - val_accuracy: 0.5660\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0230 - accuracy: 0.9925 - val_loss: 3.2366 - val_accuracy: 0.5680\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 3.2777 - val_accuracy: 0.5640\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0128 - accuracy: 0.9955 - val_loss: 3.2861 - val_accuracy: 0.5620\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 3.2741 - val_accuracy: 0.5660\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0222 - accuracy: 0.9940 - val_loss: 3.2458 - val_accuracy: 0.5720\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 3.2215 - val_accuracy: 0.5740\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0185 - accuracy: 0.9960 - val_loss: 3.2450 - val_accuracy: 0.5720\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 3.2701 - val_accuracy: 0.5740\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0208 - accuracy: 0.9965 - val_loss: 3.2480 - val_accuracy: 0.5740\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0146 - accuracy: 0.9965 - val_loss: 3.2502 - val_accuracy: 0.5720\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0175 - accuracy: 0.9950 - val_loss: 3.2565 - val_accuracy: 0.5700\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0135 - accuracy: 0.9960 - val_loss: 3.2667 - val_accuracy: 0.5700\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 3.2555 - val_accuracy: 0.5720\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0284 - accuracy: 0.9905 - val_loss: 3.2398 - val_accuracy: 0.5740\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0239 - accuracy: 0.9935 - val_loss: 3.2372 - val_accuracy: 0.5740\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0210 - accuracy: 0.9945 - val_loss: 3.2445 - val_accuracy: 0.5740\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0113 - accuracy: 0.9955 - val_loss: 3.2586 - val_accuracy: 0.5740\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0155 - accuracy: 0.9955 - val_loss: 3.2277 - val_accuracy: 0.5720\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 3.2281 - val_accuracy: 0.5760\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 3.2454 - val_accuracy: 0.5760\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 3.2496 - val_accuracy: 0.5720\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 3.2553 - val_accuracy: 0.5720\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 3.2285 - val_accuracy: 0.5700\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 3.2273 - val_accuracy: 0.5680\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0173 - accuracy: 0.9960 - val_loss: 3.2379 - val_accuracy: 0.5720\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0252 - accuracy: 0.9925 - val_loss: 3.2716 - val_accuracy: 0.5680\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0271 - accuracy: 0.9940 - val_loss: 3.2581 - val_accuracy: 0.5660\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 3.2135 - val_accuracy: 0.5680\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0140 - accuracy: 0.9945 - val_loss: 3.2377 - val_accuracy: 0.5680\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0234 - accuracy: 0.9925 - val_loss: 3.2271 - val_accuracy: 0.5700\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0146 - accuracy: 0.9960 - val_loss: 3.2679 - val_accuracy: 0.5700\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0289 - accuracy: 0.9935 - val_loss: 3.2660 - val_accuracy: 0.5660\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0177 - accuracy: 0.9955 - val_loss: 3.2620 - val_accuracy: 0.5680\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0145 - accuracy: 0.9945 - val_loss: 3.2575 - val_accuracy: 0.5680\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0257 - accuracy: 0.9920 - val_loss: 3.2675 - val_accuracy: 0.5660\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0268 - accuracy: 0.9925 - val_loss: 3.2669 - val_accuracy: 0.5640\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0272 - accuracy: 0.9940 - val_loss: 3.2437 - val_accuracy: 0.5600\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0206 - accuracy: 0.9930 - val_loss: 3.2521 - val_accuracy: 0.5600\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0209 - accuracy: 0.9955 - val_loss: 3.2404 - val_accuracy: 0.5660\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0199 - accuracy: 0.9950 - val_loss: 3.2372 - val_accuracy: 0.5620\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 3.2578 - val_accuracy: 0.5600\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0213 - accuracy: 0.9945 - val_loss: 3.2307 - val_accuracy: 0.5620\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 3.2381 - val_accuracy: 0.5600\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 3.2283 - val_accuracy: 0.5580\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0177 - accuracy: 0.9935 - val_loss: 3.2492 - val_accuracy: 0.5580\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 3.2380 - val_accuracy: 0.5600\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0099 - accuracy: 0.9960 - val_loss: 3.2222 - val_accuracy: 0.5600\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0182 - accuracy: 0.9935 - val_loss: 3.2395 - val_accuracy: 0.5600\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0125 - accuracy: 0.9965 - val_loss: 3.2509 - val_accuracy: 0.5620\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0232 - accuracy: 0.9945 - val_loss: 3.2380 - val_accuracy: 0.5600\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 1s 598us/step - loss: 0.0266 - accuracy: 0.9940 - val_loss: 3.2320 - val_accuracy: 0.5660\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0184 - accuracy: 0.9970 - val_loss: 3.2394 - val_accuracy: 0.5580\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0185 - accuracy: 0.9930 - val_loss: 3.2333 - val_accuracy: 0.5620\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 3.2293 - val_accuracy: 0.5640\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 3.2457 - val_accuracy: 0.5640\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 3.2369 - val_accuracy: 0.5640\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 3.2458 - val_accuracy: 0.5620\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 3.2444 - val_accuracy: 0.5620\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0129 - accuracy: 0.9975 - val_loss: 3.2121 - val_accuracy: 0.5620\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0284 - accuracy: 0.9935 - val_loss: 3.2379 - val_accuracy: 0.5620\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0147 - accuracy: 0.9945 - val_loss: 3.2571 - val_accuracy: 0.5600\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 3.2722 - val_accuracy: 0.5600\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0132 - accuracy: 0.9970 - val_loss: 3.2245 - val_accuracy: 0.5640\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0194 - accuracy: 0.9930 - val_loss: 3.2505 - val_accuracy: 0.5620\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 3.2113 - val_accuracy: 0.5640\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0174 - accuracy: 0.9935 - val_loss: 3.2201 - val_accuracy: 0.5620\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 3.2262 - val_accuracy: 0.5580\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 3.2649 - val_accuracy: 0.5600\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0200 - accuracy: 0.9945 - val_loss: 3.2290 - val_accuracy: 0.5620\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 3.2557 - val_accuracy: 0.5640\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0165 - accuracy: 0.9960 - val_loss: 3.2394 - val_accuracy: 0.5660\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 3.2671 - val_accuracy: 0.5600\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0251 - accuracy: 0.9925 - val_loss: 3.2666 - val_accuracy: 0.5640\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0156 - accuracy: 0.9945 - val_loss: 3.2446 - val_accuracy: 0.5620\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0128 - accuracy: 0.9950 - val_loss: 3.2237 - val_accuracy: 0.5640\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0102 - accuracy: 0.9975 - val_loss: 3.2339 - val_accuracy: 0.5640\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0119 - accuracy: 0.9970 - val_loss: 3.2319 - val_accuracy: 0.5640\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0179 - accuracy: 0.9945 - val_loss: 3.2386 - val_accuracy: 0.5620\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 3.2554 - val_accuracy: 0.5620\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0178 - accuracy: 0.9960 - val_loss: 3.2378 - val_accuracy: 0.5640\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 3.2459 - val_accuracy: 0.5640\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0225 - accuracy: 0.9940 - val_loss: 3.2385 - val_accuracy: 0.5640\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0172 - accuracy: 0.9965 - val_loss: 3.2540 - val_accuracy: 0.5660\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 3.2591 - val_accuracy: 0.5640\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 3.2823 - val_accuracy: 0.5640\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 3.2419 - val_accuracy: 0.5640\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 3.2236 - val_accuracy: 0.5660\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 3.2348 - val_accuracy: 0.5620\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0161 - accuracy: 0.9950 - val_loss: 3.2377 - val_accuracy: 0.5640\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 3.2463 - val_accuracy: 0.5620\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0302 - accuracy: 0.9920 - val_loss: 3.2589 - val_accuracy: 0.5640\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 3.2383 - val_accuracy: 0.5620\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0236 - accuracy: 0.9915 - val_loss: 3.2315 - val_accuracy: 0.5600\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 3.2211 - val_accuracy: 0.5620\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 3.2083 - val_accuracy: 0.5640\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0131 - accuracy: 0.9945 - val_loss: 3.2333 - val_accuracy: 0.5640\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 3.2517 - val_accuracy: 0.5620\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 3.2434 - val_accuracy: 0.5640\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 3.2457 - val_accuracy: 0.5640\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0218 - accuracy: 0.9935 - val_loss: 3.2457 - val_accuracy: 0.5620\n",
      "--- 856.139769077301 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.01),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat, validation_data=[test_x, test_label_cat],epochs=50, batch_size=300)\n",
    "# print('lr=0.01, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.fit(train_x,train_label_cat,validation_data=[test_x,test_label_cat],epochs=200, batch_size=250)\n",
    "# print('lr=0.001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat,validation_data=[test_x,test_label_cat],epochs=200, batch_size=200)\n",
    "# print('lr=0.0001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.00001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat, validation_data=[test_x,test_label_cat],epochs=200, batch_size=200)\n",
    "# print('lr=0.000001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.fit(train_x,train_label_cat,validation_data=[test_x, test_label_cat],epochs=200, batch_size=250)\n",
    "# print('lr=0.001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat,validation_data=[test_x, test_label_cat],epochs=200, batch_size=200)\n",
    "# print('lr=0.0001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.00001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat, validation_data=[test_x, test_label_cat],epochs=50, batch_size=200)\n",
    "# print('lr=0.0001, train complete')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy Curves')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(model_nn.history['accuracy'],'r',linewidth=1.0)\n",
    "plt.legend(['Training Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 56.2 percent\n",
      "testing model takes 0.206 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred = model.predict(test_x)\n",
    "pred_list = [] \n",
    "for i in range(len(pred)):\n",
    "    arr = pred[i]\n",
    "    idx = np.argwhere(arr == np.max(arr))\n",
    "    pred_list.append(idx[0][0])\n",
    "tst_labl = np.argmax(test_label_cat, axis=-1)\n",
    "accuracy = accuracy_score(pred_list, tst_labl)\n",
    "print(\"Test accuracy is %s percent\" % round(accuracy*100,3))\n",
    "print(\"testing model takes %s seconds\" % round((time.time() - start),3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
