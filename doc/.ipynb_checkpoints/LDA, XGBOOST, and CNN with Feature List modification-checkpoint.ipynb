{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import numpy as np \n",
    "from scipy.spatial.distance import pdist\n",
    "import time \n",
    "import math\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Flatten, Input, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "%run ../lib/load.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 14.118982076644897 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## change the root to your own path \n",
    "\n",
    "#root = sys.path[0]\n",
    "#train_dir =  os.path.join(root,  '../data/train_set')  \n",
    "#train_image_dir =  os.path.join(train_dir, 'images')\n",
    "#train_pt_dir =  os.path.join(train_dir, 'points' )\n",
    "#train_label_path =  os.path.join(train_dir,  \"label.csv\")\n",
    "path = '/Users/zhaoziqin/Desktop/train_set/'\n",
    "data = load.load_data(path+'label.csv')\n",
    "points_path = '/Users/zhaoziqin/Desktop/train_set/points/'\n",
    "X = load.load_points(points_path,data)\n",
    "y= data['emotion_idx'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mat file changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mat file and store coordinates in mat \n",
    "m = []\n",
    "for idx in data['Index']: \n",
    "    file = \"%04d.mat\"%(idx)\n",
    "    m.append( scipy.io.loadmat( os.path.join( points_path, file )))\n",
    "\n",
    "mat = [x[[i for i in x.keys() if not i in ['__header__', '__version__', '__globals__']][0]] for x in m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_idx, test_idx = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_idx, test_idx = train_test_split(y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = [ mat[i-1] for i in train_idx ] \n",
    "test_mat = [ mat[i-1] for i in test_idx ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = data.emotion_idx[train_idx-1]\n",
    "test_labels = data.emotion_idx[test_idx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_cat = to_categorical(train_labels)\n",
    "train_label_cat= train_label_cat[:,1:]\n",
    "test_label_cat = to_categorical(test_labels)\n",
    "test_label_cat= test_label_cat[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1：  pairwise_dist_cal  \n",
    "#def pairwise_dist_cal(xy_cord):\n",
    "    #p_dist =[]\n",
    "    #for i in range(xy_cord.shape[0]):\n",
    "        #for j in range(i+1,xy_cord.shape[0]):\n",
    "               # p_dist.append( abs(round(xy_cord[i,0]) - round(xy_cord[j,0] )) )\n",
    "               # p_dist.append( abs(round(xy_cord[i,1]) - round(xy_cord[j,1] ) ) )\n",
    "    #return p_dist \n",
    "\n",
    "\n",
    "\n",
    "#### updated methods with selected fiducial points; this reduce 78 poins to 50  \n",
    "''' \n",
    "feature selection : 1. remove  points P64 - 70 and P72 - 78  \n",
    "                 2. remove P51,53,55,57,58,60, 61, 63\n",
    "                 3. calculate midpints between  upper and lower eyebrow lines and replace P20-22,P24-16 with midpoints               \n",
    "So there is in total 50 points left, which gives 50*49 pairwise distance \n",
    "'''\n",
    "\n",
    "def pairwise_dist_cal_updt(mt):\n",
    "    t0 = time.time()\n",
    "    p_dist_updt =np.zeros([len(mt),1225,2])\n",
    "    n = len(mt)\n",
    "    for k in range(n):\n",
    "        xy_cord = mt[k]\n",
    "\n",
    "        xy_cord_cpy  = xy_cord\n",
    "        \n",
    "        #  eye_brow midpoint \n",
    "        to_add_brl = (xy_cord_cpy[19:22]+ xy_cord_cpy[23:26])/2\n",
    "        to_add_brr = (xy_cord_cpy[27:30]+ xy_cord_cpy[31:34])/2\n",
    "\n",
    "        # index to remove \n",
    "        rm_idx = np.append(np.arange(63,70), np.arange(71,78))\n",
    "        rm_idx = np.append(rm_idx,np.arange(50,57,2) )\n",
    "        rm_idx = np.append(rm_idx, [57,59,60,62])\n",
    "        rm_idx = np.append(rm_idx, np.arange(19,22))\n",
    "        rm_idx = np.append(rm_idx, np.arange(23,26))\n",
    "        rm_idx = np.append(rm_idx, np.arange(27,30))\n",
    "        rm_idx = np.append(rm_idx, np.arange(31,34))\n",
    "        xy_cord = np.delete(xy_cord, rm_idx, axis = 0) \n",
    "        xy_cord = np.concatenate((xy_cord,to_add_brl,to_add_brr))\n",
    "        \n",
    "\n",
    "        dist_h = [] \n",
    "        dist_v = []\n",
    "        for i in range(xy_cord.shape[0]):\n",
    "            for j in range(i+1,xy_cord.shape[0]):\n",
    "                dist_h.append( abs(round(xy_cord[i,0]) - round(xy_cord[j,0] )) )\n",
    "                dist_v.append( abs(round(xy_cord[i,1]) - round(xy_cord[j,1] ) ) )\n",
    "        p_dist_updt[k,:,0]= dist_h\n",
    "        p_dist_updt[k,:,1]= dist_v\n",
    "    \n",
    "\n",
    "    print(\"feature constructions takes %s seconds\" % (time.time() - t0))\n",
    "    return p_dist_updt.reshape([n,2450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: \n",
      "feature constructions takes 18.77232003211975 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"training: \")\n",
    "train_data = pairwise_dist_cal_updt(train_mat[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing: \n",
      "feature constructions takes 4.432502031326294 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"testing: \")\n",
    "test_data = pairwise_dist_cal_updt(test_mat[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_lda = [ x  - 1 for x in train_labels ] \n",
    "test_labels_lda = [ x  - 1 for x in test_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:402: RuntimeWarning: invalid value encountered in true_divide\n",
      "  S**2))[:self._max_components]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA()\n",
    "X_train_lda = lda.fit_transform(train_data, train_labels_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lda = lda.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lda_test_pred = lda.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the LDA mode: 1.0000\n"
     ]
    }
   ],
   "source": [
    "lda_accuracy = lda.score(test_data, test_labels_lda)\n",
    "print(\"Accuracy of the LDA mode: %.4f\" % (lda_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_xgb = [ x  - 1 for x in train_labels ] \n",
    "test_labels_xgb = [ x  - 1 for x in test_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  model takes 804.773 seconds\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators= 200,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softmax',  # for multi-labels classification \n",
    " num_class = 22, \n",
    " scale_pos_weight=1,\n",
    " seed=123)\n",
    "start_time=time.time()\n",
    "xgb.fit(train_data, train_labels_xgb ,eval_metric='auc')\n",
    "print(\"training  model takes %s seconds\" % round((time.time() - start_time),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model takes 0.418 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "pred_xgb = xgb.predict(test_data)\n",
    "print(\"testing model takes %s seconds\" % round((time.time() - start_time),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 49.6 percent\n"
     ]
    }
   ],
   "source": [
    "acc_xgb = accuracy_score(pred_xgb,test_labels_xgb )\n",
    "print(\"Test accuracy is %s percent\" %(acc_xgb*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "estimator = XGBClassifier(objective='multi:softprob',seed=36)\n",
    "parameters = {\n",
    "   'max_depth': range (1, 5, 1),\n",
    "   'n_estimators': range(1, 200, 20),\n",
    "   'learning_rate': [0.1, 0.01, 0.05]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv1 = GridSearchCV(estimator = estimator , \n",
    "                        param_grid = parameters, \n",
    "                        scoring ='accuracy',\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1)\n",
    "gscv1.fit(train_data, train_labels_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accuracy score\n",
    "print('Best score for data:', gscv1.best_score_) \n",
    "\n",
    "# Find the best parameters for the model by using grid search\n",
    "print('Best Max Depth:',gscv1.best_estimator_.max_depth) \n",
    "print('Best N.estimators:',gscv1.best_estimator_.n_estimators)\n",
    "print('Best Learning Rate:',gscv1.best_estimator_.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set predictors\n",
    "from keras.utils import to_categorical\n",
    "Y = to_categorical(y)\n",
    "Y = Y[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=123)\n",
    "\n",
    "train_x, test_x, train_idx, test_idx = train_test_split(X, Y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels_cnn = [ x  - 1 for x in train_labels ] \n",
    "#test_labels_cnn = [ x  - 1 for x in test_labels ]\n",
    "\n",
    "# train_labels_cnn = [ x  for x in train_labels ] \n",
    "# test_labels_cnn = [ x  for x in test_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_cat = to_categorical(train_labels_cnn)\n",
    "# train_label_cat= train_label_cat[:,1:]\n",
    "# test_label_cat = to_categorical(test_labels_cnn)\n",
    "# test_label_cat= test_label_cat[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [6006] \n",
    "input_layer = Input(input_shape)\n",
    "x = BatchNormalization(momentum = 0.88)(input_layer) \n",
    "x = Dense(22*10,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = Dense(22*8,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*4,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*2,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "output_layer = Dense(22,activation='softmax',kernel_initializer=initializers.glorot_normal(seed=6))(x) \n",
    "model = Model(input_layer,output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 3.0523 - accuracy: 0.0850\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 2.6775 - accuracy: 0.1770\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 2.3641 - accuracy: 0.2475\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 2.2165 - accuracy: 0.2710\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 2.0055 - accuracy: 0.3355\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.9002 - accuracy: 0.3665\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.8323 - accuracy: 0.3785\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.7076 - accuracy: 0.4295\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 1.6688 - accuracy: 0.4325\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.5925 - accuracy: 0.4720\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.5046 - accuracy: 0.4900\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.5318 - accuracy: 0.4700\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.4485 - accuracy: 0.4970\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.4192 - accuracy: 0.5005\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.4406 - accuracy: 0.5025\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.3383 - accuracy: 0.5305\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.3200 - accuracy: 0.5440\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.3077 - accuracy: 0.5530\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.2525 - accuracy: 0.5620\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.2280 - accuracy: 0.5775\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1965 - accuracy: 0.5930\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1906 - accuracy: 0.5895\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.1175 - accuracy: 0.6095\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.1552 - accuracy: 0.5980\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.1189 - accuracy: 0.6055\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 1.0951 - accuracy: 0.6230\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 1.0867 - accuracy: 0.6170\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.0544 - accuracy: 0.6290\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.0374 - accuracy: 0.6325\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 1.0469 - accuracy: 0.6295\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 1.0263 - accuracy: 0.6420\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.9494 - accuracy: 0.6625\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.9995 - accuracy: 0.6530\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.9653 - accuracy: 0.6645\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.9190 - accuracy: 0.6840\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.9159 - accuracy: 0.6815\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.8848 - accuracy: 0.6940\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.8762 - accuracy: 0.7035\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.8597 - accuracy: 0.7050\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.8278 - accuracy: 0.7130\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.8307 - accuracy: 0.7260\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.9003 - accuracy: 0.6920\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7965 - accuracy: 0.7245\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.8466 - accuracy: 0.7175\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7742 - accuracy: 0.7265\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7626 - accuracy: 0.7350\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7249 - accuracy: 0.7520\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7690 - accuracy: 0.7300\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6861 - accuracy: 0.7570\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7008 - accuracy: 0.7570\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6983 - accuracy: 0.7600\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7261 - accuracy: 0.7400\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.7201 - accuracy: 0.7400\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6580 - accuracy: 0.7725\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6328 - accuracy: 0.7825\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6466 - accuracy: 0.7870\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6240 - accuracy: 0.7925\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6597 - accuracy: 0.7700\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5972 - accuracy: 0.7945\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6756 - accuracy: 0.7695\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.6422 - accuracy: 0.7830\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.6195 - accuracy: 0.7870\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.6025 - accuracy: 0.7880\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5428 - accuracy: 0.8175\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5545 - accuracy: 0.8135\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5452 - accuracy: 0.8160\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5452 - accuracy: 0.8160\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5335 - accuracy: 0.8160\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5290 - accuracy: 0.8160\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5459 - accuracy: 0.8145\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5410 - accuracy: 0.8135\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5469 - accuracy: 0.8090\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5065 - accuracy: 0.8260\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5227 - accuracy: 0.8170\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.5170 - accuracy: 0.8250\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4914 - accuracy: 0.8215\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4781 - accuracy: 0.8315\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4400 - accuracy: 0.8630\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4629 - accuracy: 0.8410\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4860 - accuracy: 0.8320\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4826 - accuracy: 0.8340\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4562 - accuracy: 0.8490\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4883 - accuracy: 0.8275\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4560 - accuracy: 0.8400\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4655 - accuracy: 0.8470\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4636 - accuracy: 0.8450\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4211 - accuracy: 0.8590\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4012 - accuracy: 0.8710\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.4890 - accuracy: 0.8370\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4361 - accuracy: 0.8530\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3948 - accuracy: 0.8710\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.4138 - accuracy: 0.8540\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3875 - accuracy: 0.8640\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3668 - accuracy: 0.8775\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3839 - accuracy: 0.8710\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3837 - accuracy: 0.8790\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3597 - accuracy: 0.8770\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3710 - accuracy: 0.8770\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3865 - accuracy: 0.8670\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3989 - accuracy: 0.8690\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3811 - accuracy: 0.8815\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3429 - accuracy: 0.8880\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3240 - accuracy: 0.8885\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3684 - accuracy: 0.8875\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3724 - accuracy: 0.8735\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3315 - accuracy: 0.8915\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3366 - accuracy: 0.8925\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3644 - accuracy: 0.8755\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3185 - accuracy: 0.8970\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3410 - accuracy: 0.8870\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2913 - accuracy: 0.8985\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3212 - accuracy: 0.8950\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3056 - accuracy: 0.8995\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3002 - accuracy: 0.8905\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3339 - accuracy: 0.8890\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3354 - accuracy: 0.8980\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2697 - accuracy: 0.9115\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2619 - accuracy: 0.9120\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2672 - accuracy: 0.9065\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2938 - accuracy: 0.9030\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2909 - accuracy: 0.9070\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3059 - accuracy: 0.9030\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2924 - accuracy: 0.9105\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.3153 - accuracy: 0.8905\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.2703 - accuracy: 0.9085\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2728 - accuracy: 0.9090\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 5s 3ms/step - loss: 0.2478 - accuracy: 0.9240\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.2546 - accuracy: 0.9155\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2816 - accuracy: 0.9055\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2480 - accuracy: 0.9200\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2530 - accuracy: 0.9125\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.2522 - accuracy: 0.9170\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.2680 - accuracy: 0.9125: 1s -\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2439 - accuracy: 0.9220\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2461 - accuracy: 0.9265\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2470 - accuracy: 0.9235\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2515 - accuracy: 0.9235\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2784 - accuracy: 0.9110\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2362 - accuracy: 0.9235\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2438 - accuracy: 0.9180\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3041 - accuracy: 0.9040\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2809 - accuracy: 0.9105\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2341 - accuracy: 0.9295\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2425 - accuracy: 0.9180\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2279 - accuracy: 0.9280\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2441 - accuracy: 0.9175\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2477 - accuracy: 0.9215\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2655 - accuracy: 0.9135\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2261 - accuracy: 0.9275\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2378 - accuracy: 0.9245\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2345 - accuracy: 0.9260\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2361 - accuracy: 0.9180\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2505 - accuracy: 0.9180\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2133 - accuracy: 0.9285\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2161 - accuracy: 0.9340\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2288 - accuracy: 0.9315\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2203 - accuracy: 0.9245\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2113 - accuracy: 0.9325\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2288 - accuracy: 0.9300\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2024 - accuracy: 0.9325\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2316 - accuracy: 0.9275\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2080 - accuracy: 0.9315\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1843 - accuracy: 0.9435\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2203 - accuracy: 0.9235\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2120 - accuracy: 0.9350\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2174 - accuracy: 0.9245\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2282 - accuracy: 0.9275\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2202 - accuracy: 0.9230\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1940 - accuracy: 0.9405\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2061 - accuracy: 0.9345\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2164 - accuracy: 0.9320\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2399 - accuracy: 0.9280\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2132 - accuracy: 0.9325\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1961 - accuracy: 0.9330\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1845 - accuracy: 0.9420\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1764 - accuracy: 0.9460\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2078 - accuracy: 0.9370\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2007 - accuracy: 0.9340\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1713 - accuracy: 0.9430\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2013 - accuracy: 0.9290\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1802 - accuracy: 0.9420\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1859 - accuracy: 0.9340\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1913 - accuracy: 0.9395\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1763 - accuracy: 0.9430\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1737 - accuracy: 0.9450\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1771 - accuracy: 0.9405\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2170 - accuracy: 0.9290\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1909 - accuracy: 0.9340\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1715 - accuracy: 0.9435\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.2137 - accuracy: 0.9315\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1817 - accuracy: 0.9420\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1599 - accuracy: 0.9460\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1509 - accuracy: 0.9505\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1769 - accuracy: 0.9465\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1586 - accuracy: 0.9540\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1679 - accuracy: 0.9505\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1627 - accuracy: 0.9430\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1703 - accuracy: 0.9465\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1458 - accuracy: 0.9535\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1428 - accuracy: 0.9545\n",
      "training  model takes 788.315 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
    "model_nn = model.fit(train_x,train_idx,epochs=200)\n",
    "print(\"training  model takes %s seconds\" % round((time.time() - start_time),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy Curves')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(model_nn.history['accuracy'],'r',linewidth=1.0)\n",
    "plt.legend(['Training Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 56.6 percent\n",
      "testing model takes 0.288 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred = model.predict(test_x)\n",
    "pred_list = [] \n",
    "for i in range(len(pred)):\n",
    "    arr = pred[i]\n",
    "    idx = np.argwhere(arr == np.max(arr))\n",
    "    pred_list.append(idx[0][0])\n",
    "tst_labl = np.argmax(test_idx, axis=-1)\n",
    "accuracy = accuracy_score(pred_list,tst_labl)\n",
    "print(\"Test accuracy is %s percent\" % round(accuracy*100,3))\n",
    "print(\"testing model takes %s seconds\" % round((time.time() - start),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Iterations test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [6006] \n",
    "input_layer = Input(input_shape)\n",
    "x = BatchNormalization(momentum = 0.88)(input_layer) \n",
    "x = Dense(22*10,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = BatchNormalization()(x) \n",
    "x = Dense(22*8,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*4,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(22*2,activation='relu',kernel_initializer=initializers.glorot_normal(seed=6))(x)\n",
    "output_layer = Dense(22,activation='softmax',kernel_initializer=initializers.glorot_normal(seed=6))(x) \n",
    "model = Model(input_layer,output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 1s 637us/step - loss: 3.1294 - accuracy: 0.0620 - val_loss: 17.7691 - val_accuracy: 0.0320\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 2.8740 - accuracy: 0.1300 - val_loss: 3.2928 - val_accuracy: 0.1940\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 2.4950 - accuracy: 0.2025 - val_loss: 2.7819 - val_accuracy: 0.2180\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 2.3102 - accuracy: 0.2455 - val_loss: 2.5626 - val_accuracy: 0.2180\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 2.2096 - accuracy: 0.2535 - val_loss: 2.3436 - val_accuracy: 0.2620\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 2.0376 - accuracy: 0.3105 - val_loss: 2.3819 - val_accuracy: 0.2700\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 1.9541 - accuracy: 0.3075 - val_loss: 2.1632 - val_accuracy: 0.3200\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 1.8640 - accuracy: 0.3495 - val_loss: 2.1072 - val_accuracy: 0.3120\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 1.7857 - accuracy: 0.3710 - val_loss: 1.9677 - val_accuracy: 0.3400\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 1.7256 - accuracy: 0.4045 - val_loss: 1.7853 - val_accuracy: 0.3620\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 1.6685 - accuracy: 0.4130 - val_loss: 1.7754 - val_accuracy: 0.3840\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 1.6649 - accuracy: 0.4180 - val_loss: 1.7183 - val_accuracy: 0.4140\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 1.6420 - accuracy: 0.4195 - val_loss: 1.6776 - val_accuracy: 0.4280\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 1.5264 - accuracy: 0.4635 - val_loss: 1.6308 - val_accuracy: 0.4460\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 1.5509 - accuracy: 0.4605 - val_loss: 1.5751 - val_accuracy: 0.4260\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 1.4907 - accuracy: 0.4715 - val_loss: 1.5273 - val_accuracy: 0.4640\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 1.3800 - accuracy: 0.5095 - val_loss: 1.4841 - val_accuracy: 0.4520\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 1.3017 - accuracy: 0.5375 - val_loss: 1.4676 - val_accuracy: 0.4840\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 1.3506 - accuracy: 0.5220 - val_loss: 1.4684 - val_accuracy: 0.4900\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 1.2747 - accuracy: 0.5565 - val_loss: 1.4398 - val_accuracy: 0.4700\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 1s 344us/step - loss: 1.2566 - accuracy: 0.5595 - val_loss: 1.4157 - val_accuracy: 0.4900\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 1.2098 - accuracy: 0.5720 - val_loss: 1.4160 - val_accuracy: 0.5280\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 1s 327us/step - loss: 1.1808 - accuracy: 0.5760 - val_loss: 1.4351 - val_accuracy: 0.5060\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 1.1176 - accuracy: 0.6160 - val_loss: 1.4084 - val_accuracy: 0.5180\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 1.0902 - accuracy: 0.6170 - val_loss: 1.4305 - val_accuracy: 0.5220\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 1.0319 - accuracy: 0.6250 - val_loss: 1.3948 - val_accuracy: 0.5260\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.9835 - accuracy: 0.6420 - val_loss: 1.4666 - val_accuracy: 0.5180\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 1.0151 - accuracy: 0.6390 - val_loss: 1.4324 - val_accuracy: 0.5320\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.9834 - accuracy: 0.6500 - val_loss: 1.3994 - val_accuracy: 0.5360\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 1s 326us/step - loss: 0.9854 - accuracy: 0.6595 - val_loss: 1.4295 - val_accuracy: 0.5180\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.9419 - accuracy: 0.6555 - val_loss: 1.4423 - val_accuracy: 0.5320\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.9260 - accuracy: 0.6740 - val_loss: 1.4441 - val_accuracy: 0.5320\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.9378 - accuracy: 0.6805 - val_loss: 1.4977 - val_accuracy: 0.5060\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.8760 - accuracy: 0.6830 - val_loss: 1.4718 - val_accuracy: 0.5460\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.8142 - accuracy: 0.7115 - val_loss: 1.4834 - val_accuracy: 0.5420\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.7645 - accuracy: 0.7305 - val_loss: 1.4627 - val_accuracy: 0.5500\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.7094 - accuracy: 0.7365 - val_loss: 1.5947 - val_accuracy: 0.5240\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.7611 - accuracy: 0.7265 - val_loss: 1.5637 - val_accuracy: 0.5260\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.7577 - accuracy: 0.7270 - val_loss: 1.6210 - val_accuracy: 0.5280\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.7522 - accuracy: 0.7510 - val_loss: 1.6534 - val_accuracy: 0.5120\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.7578 - accuracy: 0.7335 - val_loss: 1.7172 - val_accuracy: 0.5340\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.7411 - accuracy: 0.7495 - val_loss: 1.6313 - val_accuracy: 0.5500\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.6936 - accuracy: 0.7640 - val_loss: 1.6116 - val_accuracy: 0.5420\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 1s 324us/step - loss: 0.6995 - accuracy: 0.7580 - val_loss: 1.6800 - val_accuracy: 0.5400\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.6260 - accuracy: 0.7755 - val_loss: 1.9248 - val_accuracy: 0.5220\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 1s 324us/step - loss: 0.6041 - accuracy: 0.7910 - val_loss: 1.7814 - val_accuracy: 0.5480\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.5597 - accuracy: 0.8065 - val_loss: 1.8120 - val_accuracy: 0.5440\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.5935 - accuracy: 0.8005 - val_loss: 1.6566 - val_accuracy: 0.5420\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.5681 - accuracy: 0.8115 - val_loss: 1.7115 - val_accuracy: 0.5260\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.5510 - accuracy: 0.8100 - val_loss: 1.8103 - val_accuracy: 0.5320\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 622us/step - loss: 0.4930 - accuracy: 0.8320 - val_loss: 1.7321 - val_accuracy: 0.5500\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.4460 - accuracy: 0.8455 - val_loss: 1.7040 - val_accuracy: 0.5640\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.3822 - accuracy: 0.8690 - val_loss: 1.6813 - val_accuracy: 0.5660\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.3764 - accuracy: 0.8695 - val_loss: 1.6919 - val_accuracy: 0.5760\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 344us/step - loss: 0.4195 - accuracy: 0.8560 - val_loss: 1.7188 - val_accuracy: 0.5620\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.3673 - accuracy: 0.8700 - val_loss: 1.7206 - val_accuracy: 0.5660\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.3679 - accuracy: 0.8750 - val_loss: 1.7186 - val_accuracy: 0.5720\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.3140 - accuracy: 0.8880 - val_loss: 1.7439 - val_accuracy: 0.5620\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.3443 - accuracy: 0.8885 - val_loss: 1.7592 - val_accuracy: 0.5660\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.3234 - accuracy: 0.8830 - val_loss: 1.7468 - val_accuracy: 0.5560\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.2962 - accuracy: 0.8965 - val_loss: 1.7654 - val_accuracy: 0.5660\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.2958 - accuracy: 0.8990 - val_loss: 1.7894 - val_accuracy: 0.5720\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.2792 - accuracy: 0.9105 - val_loss: 1.8289 - val_accuracy: 0.5660\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.2717 - accuracy: 0.9085 - val_loss: 1.8684 - val_accuracy: 0.5540\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.2894 - accuracy: 0.9075 - val_loss: 1.8822 - val_accuracy: 0.5500\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.2660 - accuracy: 0.9120 - val_loss: 1.9026 - val_accuracy: 0.5560\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.2704 - accuracy: 0.9040 - val_loss: 1.9008 - val_accuracy: 0.5520\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.2839 - accuracy: 0.8940 - val_loss: 1.9137 - val_accuracy: 0.5680\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.2544 - accuracy: 0.9130 - val_loss: 1.9518 - val_accuracy: 0.5700\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.2825 - accuracy: 0.9010 - val_loss: 1.9536 - val_accuracy: 0.5680\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.2593 - accuracy: 0.9120 - val_loss: 1.9497 - val_accuracy: 0.5740\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.2495 - accuracy: 0.9140 - val_loss: 1.9495 - val_accuracy: 0.5640\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.2650 - accuracy: 0.9175 - val_loss: 1.9522 - val_accuracy: 0.5580\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.2460 - accuracy: 0.9175 - val_loss: 1.9397 - val_accuracy: 0.5600\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.2335 - accuracy: 0.9200 - val_loss: 1.9450 - val_accuracy: 0.5620\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.2268 - accuracy: 0.9200 - val_loss: 1.9840 - val_accuracy: 0.5660\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.2244 - accuracy: 0.9275 - val_loss: 1.9878 - val_accuracy: 0.5640\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.2193 - accuracy: 0.9285 - val_loss: 2.0098 - val_accuracy: 0.5680\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.2436 - accuracy: 0.9210 - val_loss: 2.0214 - val_accuracy: 0.5740\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.2059 - accuracy: 0.9255 - val_loss: 2.0330 - val_accuracy: 0.5600\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.2300 - accuracy: 0.9170 - val_loss: 2.0504 - val_accuracy: 0.5560\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.1931 - accuracy: 0.9395 - val_loss: 2.0858 - val_accuracy: 0.5700\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.2097 - accuracy: 0.9310 - val_loss: 2.0895 - val_accuracy: 0.5700\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.2005 - accuracy: 0.9295 - val_loss: 2.0816 - val_accuracy: 0.5820\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.2323 - accuracy: 0.9190 - val_loss: 2.0705 - val_accuracy: 0.5680\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.1956 - accuracy: 0.9310 - val_loss: 2.0611 - val_accuracy: 0.5620\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.1958 - accuracy: 0.9365 - val_loss: 2.1042 - val_accuracy: 0.5620\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.2053 - accuracy: 0.9305 - val_loss: 2.1338 - val_accuracy: 0.5700\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.1897 - accuracy: 0.9390 - val_loss: 2.1450 - val_accuracy: 0.5680\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.2067 - accuracy: 0.9265 - val_loss: 2.1352 - val_accuracy: 0.5580\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.1764 - accuracy: 0.9470 - val_loss: 2.1347 - val_accuracy: 0.5620\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.1960 - accuracy: 0.9370 - val_loss: 2.1369 - val_accuracy: 0.5600\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.2008 - accuracy: 0.9320 - val_loss: 2.1777 - val_accuracy: 0.5680\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.1800 - accuracy: 0.9390 - val_loss: 2.1925 - val_accuracy: 0.5580\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.1931 - accuracy: 0.9340 - val_loss: 2.1862 - val_accuracy: 0.5620\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.1931 - accuracy: 0.9400 - val_loss: 2.1583 - val_accuracy: 0.5640\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.1997 - accuracy: 0.9350 - val_loss: 2.1282 - val_accuracy: 0.5600\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.1859 - accuracy: 0.9380 - val_loss: 2.1227 - val_accuracy: 0.5600\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.1637 - accuracy: 0.9490 - val_loss: 2.1593 - val_accuracy: 0.5700\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.1698 - accuracy: 0.9435 - val_loss: 2.1818 - val_accuracy: 0.5780\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.1639 - accuracy: 0.9455 - val_loss: 2.2057 - val_accuracy: 0.5760\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.1671 - accuracy: 0.9460 - val_loss: 2.2672 - val_accuracy: 0.5660\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.1650 - accuracy: 0.9420 - val_loss: 2.2734 - val_accuracy: 0.5660\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.1778 - accuracy: 0.9410 - val_loss: 2.2599 - val_accuracy: 0.5680\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.1530 - accuracy: 0.9430 - val_loss: 2.2915 - val_accuracy: 0.5600\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 441us/step - loss: 0.1766 - accuracy: 0.9435 - val_loss: 2.2898 - val_accuracy: 0.5620\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.1782 - accuracy: 0.9430 - val_loss: 2.2451 - val_accuracy: 0.5680\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.1537 - accuracy: 0.9530 - val_loss: 2.2283 - val_accuracy: 0.5600\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.1562 - accuracy: 0.9550 - val_loss: 2.2338 - val_accuracy: 0.5580\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.1458 - accuracy: 0.9550 - val_loss: 2.2573 - val_accuracy: 0.5600\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.1605 - accuracy: 0.9495 - val_loss: 2.2931 - val_accuracy: 0.5660\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.1435 - accuracy: 0.9500 - val_loss: 2.3294 - val_accuracy: 0.5580\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.1399 - accuracy: 0.9540 - val_loss: 2.3448 - val_accuracy: 0.5600\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.1541 - accuracy: 0.9455 - val_loss: 2.3665 - val_accuracy: 0.5540\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.1709 - accuracy: 0.9425 - val_loss: 2.3434 - val_accuracy: 0.5500\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.1595 - accuracy: 0.9485 - val_loss: 2.3621 - val_accuracy: 0.5600\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.1260 - accuracy: 0.9630 - val_loss: 2.4032 - val_accuracy: 0.5580\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.1422 - accuracy: 0.9530 - val_loss: 2.4307 - val_accuracy: 0.5500\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.1526 - accuracy: 0.9525 - val_loss: 2.4235 - val_accuracy: 0.5580\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.1391 - accuracy: 0.9560 - val_loss: 2.3844 - val_accuracy: 0.5600\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.1324 - accuracy: 0.9550 - val_loss: 2.3918 - val_accuracy: 0.5660\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.1252 - accuracy: 0.9595 - val_loss: 2.3673 - val_accuracy: 0.5720\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.1416 - accuracy: 0.9560 - val_loss: 2.3547 - val_accuracy: 0.5700\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.1284 - accuracy: 0.9575 - val_loss: 2.3985 - val_accuracy: 0.5640\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.1480 - accuracy: 0.9540 - val_loss: 2.4290 - val_accuracy: 0.5520\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.1102 - accuracy: 0.9615 - val_loss: 2.4119 - val_accuracy: 0.5580\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.1335 - accuracy: 0.9565 - val_loss: 2.4254 - val_accuracy: 0.5580\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.1114 - accuracy: 0.9665 - val_loss: 2.4605 - val_accuracy: 0.5520\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.1379 - accuracy: 0.9525 - val_loss: 2.4970 - val_accuracy: 0.5620\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.1504 - accuracy: 0.9555 - val_loss: 2.4575 - val_accuracy: 0.5660\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.1406 - accuracy: 0.9510 - val_loss: 2.4240 - val_accuracy: 0.5700\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.1500 - accuracy: 0.9545 - val_loss: 2.4180 - val_accuracy: 0.5580\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.1230 - accuracy: 0.9640 - val_loss: 2.3877 - val_accuracy: 0.5580\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.1201 - accuracy: 0.9605 - val_loss: 2.4067 - val_accuracy: 0.5560\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.1263 - accuracy: 0.9600 - val_loss: 2.4004 - val_accuracy: 0.5500\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.1252 - accuracy: 0.9610 - val_loss: 2.4042 - val_accuracy: 0.5480\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.1277 - accuracy: 0.9635 - val_loss: 2.4067 - val_accuracy: 0.5460\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.1115 - accuracy: 0.9715 - val_loss: 2.4069 - val_accuracy: 0.5480\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.1180 - accuracy: 0.9605 - val_loss: 2.4446 - val_accuracy: 0.5520\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.1328 - accuracy: 0.9585 - val_loss: 2.4409 - val_accuracy: 0.5580\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.1183 - accuracy: 0.9600 - val_loss: 2.4565 - val_accuracy: 0.5620\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.1096 - accuracy: 0.9640 - val_loss: 2.4670 - val_accuracy: 0.5540\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.1025 - accuracy: 0.9680 - val_loss: 2.4798 - val_accuracy: 0.5600\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.1276 - accuracy: 0.9605 - val_loss: 2.4491 - val_accuracy: 0.5640\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0972 - accuracy: 0.9680 - val_loss: 2.4134 - val_accuracy: 0.5620\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.1098 - accuracy: 0.9675 - val_loss: 2.3966 - val_accuracy: 0.5500\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.1090 - accuracy: 0.9635 - val_loss: 2.4313 - val_accuracy: 0.5580\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.1333 - accuracy: 0.9590 - val_loss: 2.4735 - val_accuracy: 0.5580\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.1027 - accuracy: 0.9700 - val_loss: 2.4945 - val_accuracy: 0.5660\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.1102 - accuracy: 0.9665 - val_loss: 2.4979 - val_accuracy: 0.5560\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.1381 - accuracy: 0.9535 - val_loss: 2.4652 - val_accuracy: 0.5620\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0884 - accuracy: 0.9725 - val_loss: 2.4874 - val_accuracy: 0.5600\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.1308 - accuracy: 0.9590 - val_loss: 2.4796 - val_accuracy: 0.5640\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.1099 - accuracy: 0.9625 - val_loss: 2.4609 - val_accuracy: 0.5720\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.1012 - accuracy: 0.9675 - val_loss: 2.4727 - val_accuracy: 0.5600\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0945 - accuracy: 0.9715 - val_loss: 2.5038 - val_accuracy: 0.5440\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.1381 - accuracy: 0.9620 - val_loss: 2.5113 - val_accuracy: 0.5580\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 347us/step - loss: 0.0867 - accuracy: 0.9715 - val_loss: 2.4824 - val_accuracy: 0.5580\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.1089 - accuracy: 0.9625 - val_loss: 2.5171 - val_accuracy: 0.5600\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.1058 - accuracy: 0.9700 - val_loss: 2.5303 - val_accuracy: 0.5660\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0983 - accuracy: 0.9695 - val_loss: 2.5628 - val_accuracy: 0.5640\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.1092 - accuracy: 0.9690 - val_loss: 2.5709 - val_accuracy: 0.5620\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.1213 - accuracy: 0.9600 - val_loss: 2.5592 - val_accuracy: 0.5680\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.1026 - accuracy: 0.9680 - val_loss: 2.5809 - val_accuracy: 0.5880\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0893 - accuracy: 0.9725 - val_loss: 2.5867 - val_accuracy: 0.5720\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0889 - accuracy: 0.9700 - val_loss: 2.5813 - val_accuracy: 0.5620\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0886 - accuracy: 0.9735 - val_loss: 2.5832 - val_accuracy: 0.5540\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0846 - accuracy: 0.9725 - val_loss: 2.5853 - val_accuracy: 0.5680\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0898 - accuracy: 0.9695 - val_loss: 2.5742 - val_accuracy: 0.5520\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0942 - accuracy: 0.9700 - val_loss: 2.6054 - val_accuracy: 0.5540\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0939 - accuracy: 0.9690 - val_loss: 2.5855 - val_accuracy: 0.5520\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0965 - accuracy: 0.9715 - val_loss: 2.5960 - val_accuracy: 0.5640\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0814 - accuracy: 0.9745 - val_loss: 2.5969 - val_accuracy: 0.5540\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0991 - accuracy: 0.9675 - val_loss: 2.6460 - val_accuracy: 0.5540\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0820 - accuracy: 0.9775 - val_loss: 2.6447 - val_accuracy: 0.5540\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0772 - accuracy: 0.9765 - val_loss: 2.6272 - val_accuracy: 0.5560\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0865 - accuracy: 0.9760 - val_loss: 2.6214 - val_accuracy: 0.5540\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0896 - accuracy: 0.9775 - val_loss: 2.6284 - val_accuracy: 0.5600\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0883 - accuracy: 0.9770 - val_loss: 2.6242 - val_accuracy: 0.5580\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0875 - accuracy: 0.9695 - val_loss: 2.6416 - val_accuracy: 0.5540\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0922 - accuracy: 0.9730 - val_loss: 2.6143 - val_accuracy: 0.5560\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.0674 - accuracy: 0.9760 - val_loss: 2.6211 - val_accuracy: 0.5560\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0814 - accuracy: 0.9730 - val_loss: 2.6057 - val_accuracy: 0.5500\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0711 - accuracy: 0.9805 - val_loss: 2.6345 - val_accuracy: 0.5440\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0822 - accuracy: 0.9755 - val_loss: 2.6216 - val_accuracy: 0.5560\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0806 - accuracy: 0.9765 - val_loss: 2.6205 - val_accuracy: 0.5480\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0846 - accuracy: 0.9745 - val_loss: 2.6349 - val_accuracy: 0.5520\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0732 - accuracy: 0.9720 - val_loss: 2.6306 - val_accuracy: 0.5540\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0897 - accuracy: 0.9705 - val_loss: 2.6571 - val_accuracy: 0.5460\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0740 - accuracy: 0.9760 - val_loss: 2.7027 - val_accuracy: 0.5620\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0636 - accuracy: 0.9790 - val_loss: 2.6852 - val_accuracy: 0.5540\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0922 - accuracy: 0.9700 - val_loss: 2.6637 - val_accuracy: 0.5580\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0890 - accuracy: 0.9700 - val_loss: 2.6523 - val_accuracy: 0.5640\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0699 - accuracy: 0.9795 - val_loss: 2.6194 - val_accuracy: 0.5760\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 344us/step - loss: 0.0824 - accuracy: 0.9745 - val_loss: 2.6342 - val_accuracy: 0.5620\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0731 - accuracy: 0.9780 - val_loss: 2.6673 - val_accuracy: 0.5580\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 2.6603 - val_accuracy: 0.5580\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0863 - accuracy: 0.9700 - val_loss: 2.6700 - val_accuracy: 0.5620\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0896 - accuracy: 0.9755 - val_loss: 2.6861 - val_accuracy: 0.5660\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0795 - accuracy: 0.9740 - val_loss: 2.6851 - val_accuracy: 0.5600\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0656 - accuracy: 0.9770 - val_loss: 2.6795 - val_accuracy: 0.5600\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.0612 - accuracy: 0.9795 - val_loss: 2.6680 - val_accuracy: 0.5600\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.0615 - accuracy: 0.9805 - val_loss: 2.6813 - val_accuracy: 0.5560\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0696 - accuracy: 0.9780 - val_loss: 2.7255 - val_accuracy: 0.5540\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0609 - accuracy: 0.9815 - val_loss: 2.7240 - val_accuracy: 0.5460\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0734 - accuracy: 0.9780 - val_loss: 2.7168 - val_accuracy: 0.5500\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0755 - accuracy: 0.9745 - val_loss: 2.7453 - val_accuracy: 0.5540\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0837 - accuracy: 0.9730 - val_loss: 2.7286 - val_accuracy: 0.5560\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.97 - 1s 423us/step - loss: 0.0851 - accuracy: 0.9775 - val_loss: 2.7482 - val_accuracy: 0.5560\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0775 - accuracy: 0.9750 - val_loss: 2.7552 - val_accuracy: 0.5580\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0788 - accuracy: 0.9760 - val_loss: 2.7387 - val_accuracy: 0.5580\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0822 - accuracy: 0.9740 - val_loss: 2.7422 - val_accuracy: 0.5620\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0760 - accuracy: 0.9760 - val_loss: 2.7812 - val_accuracy: 0.5580\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0884 - accuracy: 0.9770 - val_loss: 2.8380 - val_accuracy: 0.5560\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.0828 - accuracy: 0.9755 - val_loss: 2.9209 - val_accuracy: 0.5500\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0902 - accuracy: 0.9735 - val_loss: 2.8353 - val_accuracy: 0.5540\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0806 - accuracy: 0.9760 - val_loss: 2.6897 - val_accuracy: 0.5540\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0612 - accuracy: 0.9805 - val_loss: 2.6415 - val_accuracy: 0.5480\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0610 - accuracy: 0.9820 - val_loss: 2.6924 - val_accuracy: 0.5520\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0644 - accuracy: 0.9780 - val_loss: 2.7159 - val_accuracy: 0.5520\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0700 - accuracy: 0.9770 - val_loss: 2.6924 - val_accuracy: 0.5420\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 2.6655 - val_accuracy: 0.5440\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.0677 - accuracy: 0.9765 - val_loss: 2.6924 - val_accuracy: 0.5560\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0878 - accuracy: 0.9775 - val_loss: 2.7193 - val_accuracy: 0.5620\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0663 - accuracy: 0.9815 - val_loss: 2.7279 - val_accuracy: 0.5580\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0492 - accuracy: 0.9835 - val_loss: 2.7722 - val_accuracy: 0.5560\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0567 - accuracy: 0.9810 - val_loss: 2.7622 - val_accuracy: 0.5620\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0823 - accuracy: 0.9755 - val_loss: 2.7835 - val_accuracy: 0.5660\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0659 - accuracy: 0.9795 - val_loss: 2.7674 - val_accuracy: 0.5680\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.98 - 1s 411us/step - loss: 0.0473 - accuracy: 0.9870 - val_loss: 2.8029 - val_accuracy: 0.5540\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0662 - accuracy: 0.9790 - val_loss: 2.8292 - val_accuracy: 0.5620\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0601 - accuracy: 0.9800 - val_loss: 2.8948 - val_accuracy: 0.5660\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0733 - accuracy: 0.9770 - val_loss: 2.8821 - val_accuracy: 0.5680\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0670 - accuracy: 0.9800 - val_loss: 2.8198 - val_accuracy: 0.5540\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0664 - accuracy: 0.9790 - val_loss: 2.8367 - val_accuracy: 0.5600\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0570 - accuracy: 0.9855 - val_loss: 2.8380 - val_accuracy: 0.5520\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0650 - accuracy: 0.9780 - val_loss: 2.8218 - val_accuracy: 0.5520\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0764 - accuracy: 0.9775 - val_loss: 2.7831 - val_accuracy: 0.5660\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0638 - accuracy: 0.9830 - val_loss: 2.7653 - val_accuracy: 0.5740\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0537 - accuracy: 0.9835 - val_loss: 2.7460 - val_accuracy: 0.5800\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0635 - accuracy: 0.9820 - val_loss: 2.7475 - val_accuracy: 0.5720\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0731 - accuracy: 0.9805 - val_loss: 2.7467 - val_accuracy: 0.5740\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0630 - accuracy: 0.9830 - val_loss: 2.7411 - val_accuracy: 0.5700\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0730 - accuracy: 0.9785 - val_loss: 2.7337 - val_accuracy: 0.5640\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.0779 - accuracy: 0.9765 - val_loss: 2.7254 - val_accuracy: 0.5680\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0720 - accuracy: 0.9785 - val_loss: 2.7012 - val_accuracy: 0.5700\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0643 - accuracy: 0.9775 - val_loss: 2.7152 - val_accuracy: 0.5680\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0583 - accuracy: 0.9820 - val_loss: 2.7284 - val_accuracy: 0.5760\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0541 - accuracy: 0.9800 - val_loss: 2.7375 - val_accuracy: 0.5800\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0769 - accuracy: 0.9785 - val_loss: 2.7601 - val_accuracy: 0.5780\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 2s 756us/step - loss: 0.0489 - accuracy: 0.9865 - val_loss: 2.7612 - val_accuracy: 0.5720\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 2.7661 - val_accuracy: 0.5740\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0615 - accuracy: 0.9800 - val_loss: 2.7508 - val_accuracy: 0.5780\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0654 - accuracy: 0.9830 - val_loss: 2.7620 - val_accuracy: 0.5740\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0464 - accuracy: 0.9855 - val_loss: 2.7696 - val_accuracy: 0.5720\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0554 - accuracy: 0.9815 - val_loss: 2.7527 - val_accuracy: 0.5700\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0776 - accuracy: 0.9805 - val_loss: 2.7442 - val_accuracy: 0.5760\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0537 - accuracy: 0.9815 - val_loss: 2.7482 - val_accuracy: 0.5700\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0622 - accuracy: 0.9820 - val_loss: 2.7224 - val_accuracy: 0.5740\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0517 - accuracy: 0.9855 - val_loss: 2.7538 - val_accuracy: 0.5720\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 2.7524 - val_accuracy: 0.5720\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0649 - accuracy: 0.9820 - val_loss: 2.7627 - val_accuracy: 0.5760\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 421us/step - loss: 0.0625 - accuracy: 0.9845 - val_loss: 2.7594 - val_accuracy: 0.5740\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0568 - accuracy: 0.9820 - val_loss: 2.7728 - val_accuracy: 0.5760\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0517 - accuracy: 0.9850 - val_loss: 2.7713 - val_accuracy: 0.5740\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0532 - accuracy: 0.9850 - val_loss: 2.7439 - val_accuracy: 0.5720\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0702 - accuracy: 0.9840 - val_loss: 2.7681 - val_accuracy: 0.5780\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0633 - accuracy: 0.9815 - val_loss: 2.7679 - val_accuracy: 0.5720\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0501 - accuracy: 0.9810 - val_loss: 2.7898 - val_accuracy: 0.5740\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 451us/step - loss: 0.0511 - accuracy: 0.9860 - val_loss: 2.7845 - val_accuracy: 0.5740\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 457us/step - loss: 0.0516 - accuracy: 0.9855 - val_loss: 2.7603 - val_accuracy: 0.5760\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0578 - accuracy: 0.9845 - val_loss: 2.7762 - val_accuracy: 0.5780\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.0759 - accuracy: 0.9740 - val_loss: 2.7715 - val_accuracy: 0.5760\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0476 - accuracy: 0.9865 - val_loss: 2.7769 - val_accuracy: 0.5760\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0445 - accuracy: 0.9845 - val_loss: 2.7705 - val_accuracy: 0.5760\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0579 - accuracy: 0.9835 - val_loss: 2.7803 - val_accuracy: 0.5780\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0468 - accuracy: 0.9870 - val_loss: 2.7607 - val_accuracy: 0.5740\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0540 - accuracy: 0.9840 - val_loss: 2.7753 - val_accuracy: 0.5740\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0643 - accuracy: 0.9810 - val_loss: 2.7822 - val_accuracy: 0.5760\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0463 - accuracy: 0.9835 - val_loss: 2.7834 - val_accuracy: 0.5780\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0606 - accuracy: 0.9810 - val_loss: 2.7896 - val_accuracy: 0.5820\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0536 - accuracy: 0.9850 - val_loss: 2.7901 - val_accuracy: 0.5780\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0459 - accuracy: 0.9845 - val_loss: 2.7983 - val_accuracy: 0.5800\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0456 - accuracy: 0.9835 - val_loss: 2.7980 - val_accuracy: 0.5760\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0569 - accuracy: 0.9810 - val_loss: 2.7849 - val_accuracy: 0.5780\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0548 - accuracy: 0.9815 - val_loss: 2.7822 - val_accuracy: 0.5760\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0450 - accuracy: 0.9855 - val_loss: 2.7919 - val_accuracy: 0.5740\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 2.8066 - val_accuracy: 0.5760\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 458us/step - loss: 0.0443 - accuracy: 0.9835 - val_loss: 2.8107 - val_accuracy: 0.5740\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 446us/step - loss: 0.0559 - accuracy: 0.9825 - val_loss: 2.8084 - val_accuracy: 0.5740\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0570 - accuracy: 0.9840 - val_loss: 2.7917 - val_accuracy: 0.5740\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0408 - accuracy: 0.9870 - val_loss: 2.8149 - val_accuracy: 0.5760\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0490 - accuracy: 0.9870 - val_loss: 2.8143 - val_accuracy: 0.5700\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0409 - accuracy: 0.9900 - val_loss: 2.8202 - val_accuracy: 0.5720\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0582 - accuracy: 0.9815 - val_loss: 2.8199 - val_accuracy: 0.5760\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0432 - accuracy: 0.9895 - val_loss: 2.8238 - val_accuracy: 0.5700\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0541 - accuracy: 0.9840 - val_loss: 2.8333 - val_accuracy: 0.5720\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0289 - accuracy: 0.9925 - val_loss: 2.8352 - val_accuracy: 0.5720\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0539 - accuracy: 0.9850 - val_loss: 2.8527 - val_accuracy: 0.5780\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0598 - accuracy: 0.9830 - val_loss: 2.8266 - val_accuracy: 0.5760\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0474 - accuracy: 0.9870 - val_loss: 2.8447 - val_accuracy: 0.5760\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 2.8415 - val_accuracy: 0.5780\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0507 - accuracy: 0.9850 - val_loss: 2.8116 - val_accuracy: 0.5720\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0589 - accuracy: 0.9830 - val_loss: 2.8164 - val_accuracy: 0.5740\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0480 - accuracy: 0.9885 - val_loss: 2.8205 - val_accuracy: 0.5720\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0462 - accuracy: 0.9865 - val_loss: 2.8434 - val_accuracy: 0.5700\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0581 - accuracy: 0.9860 - val_loss: 2.8518 - val_accuracy: 0.5760\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0374 - accuracy: 0.9900 - val_loss: 2.8401 - val_accuracy: 0.5720\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0364 - accuracy: 0.9900 - val_loss: 2.8426 - val_accuracy: 0.5680\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0462 - accuracy: 0.9860 - val_loss: 2.8232 - val_accuracy: 0.5720\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0516 - accuracy: 0.9855 - val_loss: 2.8376 - val_accuracy: 0.5700\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 2.8490 - val_accuracy: 0.5660\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0443 - accuracy: 0.9855 - val_loss: 2.8362 - val_accuracy: 0.5680\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 421us/step - loss: 0.0467 - accuracy: 0.9860 - val_loss: 2.8393 - val_accuracy: 0.5660\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0634 - accuracy: 0.9790 - val_loss: 2.8561 - val_accuracy: 0.5660\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0411 - accuracy: 0.9855 - val_loss: 2.8500 - val_accuracy: 0.5720\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0420 - accuracy: 0.9885 - val_loss: 2.8614 - val_accuracy: 0.5740\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0485 - accuracy: 0.9870 - val_loss: 2.8437 - val_accuracy: 0.5680\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0427 - accuracy: 0.9830 - val_loss: 2.8507 - val_accuracy: 0.5700\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0391 - accuracy: 0.9875 - val_loss: 2.8525 - val_accuracy: 0.5660\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0524 - accuracy: 0.9840 - val_loss: 2.8560 - val_accuracy: 0.5660\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0573 - accuracy: 0.9825 - val_loss: 2.8464 - val_accuracy: 0.5640\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0433 - accuracy: 0.9895 - val_loss: 2.8537 - val_accuracy: 0.5720\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0498 - accuracy: 0.9885 - val_loss: 2.8299 - val_accuracy: 0.5620\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0348 - accuracy: 0.9880 - val_loss: 2.8350 - val_accuracy: 0.5660\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.0443 - accuracy: 0.9850 - val_loss: 2.8422 - val_accuracy: 0.5680\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0319 - accuracy: 0.9915 - val_loss: 2.8419 - val_accuracy: 0.5660\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 473us/step - loss: 0.0430 - accuracy: 0.9870 - val_loss: 2.8422 - val_accuracy: 0.5680\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 489us/step - loss: 0.0451 - accuracy: 0.9855 - val_loss: 2.8501 - val_accuracy: 0.5700\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 454us/step - loss: 0.0385 - accuracy: 0.9900 - val_loss: 2.8487 - val_accuracy: 0.5680\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0378 - accuracy: 0.9895 - val_loss: 2.8473 - val_accuracy: 0.5660\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0514 - accuracy: 0.9855 - val_loss: 2.8452 - val_accuracy: 0.5660\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 440us/step - loss: 0.0356 - accuracy: 0.9915 - val_loss: 2.8502 - val_accuracy: 0.5680\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0440 - accuracy: 0.9860 - val_loss: 2.8562 - val_accuracy: 0.5740\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0330 - accuracy: 0.9905 - val_loss: 2.8651 - val_accuracy: 0.5720\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 452us/step - loss: 0.0493 - accuracy: 0.9820 - val_loss: 2.8570 - val_accuracy: 0.5740\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0482 - accuracy: 0.9835 - val_loss: 2.8527 - val_accuracy: 0.5700\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0405 - accuracy: 0.9890 - val_loss: 2.8555 - val_accuracy: 0.5700\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0437 - accuracy: 0.9870 - val_loss: 2.8581 - val_accuracy: 0.5680\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0477 - accuracy: 0.9860 - val_loss: 2.8587 - val_accuracy: 0.5740\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0445 - accuracy: 0.9875 - val_loss: 2.8484 - val_accuracy: 0.5720\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0402 - accuracy: 0.9895 - val_loss: 2.8737 - val_accuracy: 0.5700\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0548 - accuracy: 0.9830 - val_loss: 2.8771 - val_accuracy: 0.5700\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 448us/step - loss: 0.0454 - accuracy: 0.9855 - val_loss: 2.8689 - val_accuracy: 0.5720\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0447 - accuracy: 0.9890 - val_loss: 2.8877 - val_accuracy: 0.5740\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 446us/step - loss: 0.0277 - accuracy: 0.9910 - val_loss: 2.8784 - val_accuracy: 0.5740\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0485 - accuracy: 0.9870 - val_loss: 2.9021 - val_accuracy: 0.5720\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 451us/step - loss: 0.0478 - accuracy: 0.9860 - val_loss: 2.8871 - val_accuracy: 0.5700\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 461us/step - loss: 0.0342 - accuracy: 0.9885 - val_loss: 2.9053 - val_accuracy: 0.5700\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0530 - accuracy: 0.9860 - val_loss: 2.8758 - val_accuracy: 0.5660\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 2.8880 - val_accuracy: 0.5720\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0518 - accuracy: 0.9870 - val_loss: 2.9117 - val_accuracy: 0.5740\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0365 - accuracy: 0.9910 - val_loss: 2.9228 - val_accuracy: 0.5760\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0400 - accuracy: 0.9855 - val_loss: 2.9036 - val_accuracy: 0.5760\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0572 - accuracy: 0.9830 - val_loss: 2.8935 - val_accuracy: 0.5740\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0395 - accuracy: 0.9885 - val_loss: 2.8919 - val_accuracy: 0.5720\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0498 - accuracy: 0.9845 - val_loss: 2.9004 - val_accuracy: 0.5720\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0504 - accuracy: 0.9860 - val_loss: 2.8966 - val_accuracy: 0.5720\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 2.9027 - val_accuracy: 0.5680\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0401 - accuracy: 0.9870 - val_loss: 2.9021 - val_accuracy: 0.5680\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0546 - accuracy: 0.9810 - val_loss: 2.9140 - val_accuracy: 0.5740\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0366 - accuracy: 0.9915 - val_loss: 2.8942 - val_accuracy: 0.5760\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0397 - accuracy: 0.9860 - val_loss: 2.8852 - val_accuracy: 0.5780\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0367 - accuracy: 0.9905 - val_loss: 2.8914 - val_accuracy: 0.5760\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0377 - accuracy: 0.9890 - val_loss: 2.9207 - val_accuracy: 0.5780\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 462us/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 2.9257 - val_accuracy: 0.5720\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 2.9139 - val_accuracy: 0.5760\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0451 - accuracy: 0.9840 - val_loss: 2.9167 - val_accuracy: 0.5780\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0538 - accuracy: 0.9850 - val_loss: 2.9092 - val_accuracy: 0.5680\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0466 - accuracy: 0.9875 - val_loss: 2.9006 - val_accuracy: 0.5680\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0396 - accuracy: 0.9885 - val_loss: 2.8675 - val_accuracy: 0.5640\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0558 - accuracy: 0.9830 - val_loss: 2.8965 - val_accuracy: 0.5640\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0399 - accuracy: 0.9865 - val_loss: 2.8791 - val_accuracy: 0.5700\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0517 - accuracy: 0.9845 - val_loss: 2.9116 - val_accuracy: 0.5700\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0362 - accuracy: 0.9880 - val_loss: 2.9032 - val_accuracy: 0.5640\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0390 - accuracy: 0.9875 - val_loss: 2.9128 - val_accuracy: 0.5660\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0278 - accuracy: 0.9915 - val_loss: 2.9059 - val_accuracy: 0.5680\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 2.9210 - val_accuracy: 0.5680\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 2.9466 - val_accuracy: 0.5640\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 2.9272 - val_accuracy: 0.5660\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0294 - accuracy: 0.9900 - val_loss: 2.9304 - val_accuracy: 0.5660\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0524 - accuracy: 0.9860 - val_loss: 2.9384 - val_accuracy: 0.5660\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 2.9413 - val_accuracy: 0.5640\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 435us/step - loss: 0.0508 - accuracy: 0.9855 - val_loss: 2.9260 - val_accuracy: 0.5660\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0301 - accuracy: 0.9905 - val_loss: 2.9285 - val_accuracy: 0.5620\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0358 - accuracy: 0.9890 - val_loss: 2.9214 - val_accuracy: 0.5640\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0585 - accuracy: 0.9830 - val_loss: 2.9285 - val_accuracy: 0.5660\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0462 - accuracy: 0.9840 - val_loss: 2.9430 - val_accuracy: 0.5640\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0474 - accuracy: 0.9860 - val_loss: 2.9567 - val_accuracy: 0.5700\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0353 - accuracy: 0.9890 - val_loss: 2.9527 - val_accuracy: 0.5680\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0371 - accuracy: 0.9880 - val_loss: 2.9451 - val_accuracy: 0.5660\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0420 - accuracy: 0.9875 - val_loss: 2.9468 - val_accuracy: 0.5660\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0415 - accuracy: 0.9865 - val_loss: 2.9344 - val_accuracy: 0.5660\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0409 - accuracy: 0.9865 - val_loss: 2.9402 - val_accuracy: 0.5600\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0405 - accuracy: 0.9850 - val_loss: 2.9498 - val_accuracy: 0.5620\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0398 - accuracy: 0.9880 - val_loss: 2.9176 - val_accuracy: 0.5720\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0317 - accuracy: 0.9910 - val_loss: 2.9218 - val_accuracy: 0.5680\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0503 - accuracy: 0.9815 - val_loss: 2.9314 - val_accuracy: 0.5700\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0421 - accuracy: 0.9885 - val_loss: 2.9528 - val_accuracy: 0.5620\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0337 - accuracy: 0.9905 - val_loss: 2.9441 - val_accuracy: 0.5660\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0387 - accuracy: 0.9905 - val_loss: 2.9547 - val_accuracy: 0.5680\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 2.9567 - val_accuracy: 0.5660\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0282 - accuracy: 0.9930 - val_loss: 2.9430 - val_accuracy: 0.5700\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0430 - accuracy: 0.9915 - val_loss: 2.9539 - val_accuracy: 0.5680\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 438us/step - loss: 0.0371 - accuracy: 0.9900 - val_loss: 2.9648 - val_accuracy: 0.5640\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 2.9471 - val_accuracy: 0.5660\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0444 - accuracy: 0.9875 - val_loss: 2.9485 - val_accuracy: 0.5680\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0489 - accuracy: 0.9855 - val_loss: 2.9602 - val_accuracy: 0.5700\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0240 - accuracy: 0.9930 - val_loss: 2.9534 - val_accuracy: 0.5640\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0466 - accuracy: 0.9875 - val_loss: 2.9447 - val_accuracy: 0.5700\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0335 - accuracy: 0.9895 - val_loss: 2.9521 - val_accuracy: 0.5660\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0487 - accuracy: 0.9845 - val_loss: 2.9592 - val_accuracy: 0.5660\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 2.9490 - val_accuracy: 0.5680\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 2.9363 - val_accuracy: 0.5620\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0455 - accuracy: 0.9855 - val_loss: 2.9456 - val_accuracy: 0.5620\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 2.9390 - val_accuracy: 0.5640\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 437us/step - loss: 0.0397 - accuracy: 0.9870 - val_loss: 2.9126 - val_accuracy: 0.5700\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0354 - accuracy: 0.9865 - val_loss: 2.9150 - val_accuracy: 0.5740\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0523 - accuracy: 0.9860 - val_loss: 2.9492 - val_accuracy: 0.5720\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0458 - accuracy: 0.9845 - val_loss: 2.9329 - val_accuracy: 0.5740\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0291 - accuracy: 0.9915 - val_loss: 2.9507 - val_accuracy: 0.5760\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0557 - accuracy: 0.9855 - val_loss: 2.9518 - val_accuracy: 0.5780\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0478 - accuracy: 0.9890 - val_loss: 2.9480 - val_accuracy: 0.5740\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 461us/step - loss: 0.0534 - accuracy: 0.9875 - val_loss: 2.9449 - val_accuracy: 0.5680\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 425us/step - loss: 0.0308 - accuracy: 0.9900 - val_loss: 2.9399 - val_accuracy: 0.5640\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 2.9430 - val_accuracy: 0.5620\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0483 - accuracy: 0.9870 - val_loss: 2.9208 - val_accuracy: 0.5640\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0435 - accuracy: 0.9870 - val_loss: 2.9394 - val_accuracy: 0.5660\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0481 - accuracy: 0.9895 - val_loss: 2.9692 - val_accuracy: 0.5780\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0311 - accuracy: 0.9905 - val_loss: 2.9537 - val_accuracy: 0.5700\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0316 - accuracy: 0.9910 - val_loss: 2.9562 - val_accuracy: 0.5760\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0390 - accuracy: 0.9885 - val_loss: 2.9509 - val_accuracy: 0.5700\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0393 - accuracy: 0.9905 - val_loss: 2.9552 - val_accuracy: 0.5760\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 2.9727 - val_accuracy: 0.5740\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0428 - accuracy: 0.9850 - val_loss: 2.9575 - val_accuracy: 0.5700\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0285 - accuracy: 0.9900 - val_loss: 2.9809 - val_accuracy: 0.5700\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0425 - accuracy: 0.9860 - val_loss: 2.9670 - val_accuracy: 0.5660\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0440 - accuracy: 0.9845 - val_loss: 2.9762 - val_accuracy: 0.5720\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 442us/step - loss: 0.0407 - accuracy: 0.9850 - val_loss: 2.9596 - val_accuracy: 0.5640\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 2.9585 - val_accuracy: 0.5660\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0312 - accuracy: 0.9925 - val_loss: 2.9470 - val_accuracy: 0.5660\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0365 - accuracy: 0.9890 - val_loss: 2.9461 - val_accuracy: 0.5640\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 453us/step - loss: 0.0424 - accuracy: 0.9895 - val_loss: 2.9574 - val_accuracy: 0.5660\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0441 - accuracy: 0.9880 - val_loss: 2.9432 - val_accuracy: 0.5700\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0464 - accuracy: 0.9865 - val_loss: 2.9817 - val_accuracy: 0.5640\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0345 - accuracy: 0.9880 - val_loss: 2.9825 - val_accuracy: 0.5660\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0518 - accuracy: 0.9890 - val_loss: 2.9775 - val_accuracy: 0.5640\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0373 - accuracy: 0.9910 - val_loss: 2.9727 - val_accuracy: 0.5700\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0400 - accuracy: 0.9855 - val_loss: 2.9845 - val_accuracy: 0.5740\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0339 - accuracy: 0.9905 - val_loss: 2.9633 - val_accuracy: 0.5680\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 691us/step - loss: 0.0447 - accuracy: 0.9865 - val_loss: 2.9785 - val_accuracy: 0.5700\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0507 - accuracy: 0.9830 - val_loss: 2.9515 - val_accuracy: 0.5700\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0509 - accuracy: 0.9850 - val_loss: 2.9494 - val_accuracy: 0.5680\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0357 - accuracy: 0.9880 - val_loss: 2.9597 - val_accuracy: 0.5720\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 2.9624 - val_accuracy: 0.5720\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0367 - accuracy: 0.9885 - val_loss: 2.9486 - val_accuracy: 0.5660\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0373 - accuracy: 0.9895 - val_loss: 2.9646 - val_accuracy: 0.5700\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0406 - accuracy: 0.9865 - val_loss: 2.9539 - val_accuracy: 0.5720\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 450us/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 2.9593 - val_accuracy: 0.5700\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0341 - accuracy: 0.9880 - val_loss: 2.9683 - val_accuracy: 0.5660\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0347 - accuracy: 0.9880 - val_loss: 2.9756 - val_accuracy: 0.5680\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 449us/step - loss: 0.0398 - accuracy: 0.9860 - val_loss: 2.9607 - val_accuracy: 0.5680\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0368 - accuracy: 0.9895 - val_loss: 2.9708 - val_accuracy: 0.5680\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0454 - accuracy: 0.9875 - val_loss: 2.9716 - val_accuracy: 0.5700\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0435 - accuracy: 0.9860 - val_loss: 2.9737 - val_accuracy: 0.5680\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 2.9566 - val_accuracy: 0.5680\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0332 - accuracy: 0.9865 - val_loss: 2.9364 - val_accuracy: 0.5700\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0427 - accuracy: 0.9875 - val_loss: 2.9449 - val_accuracy: 0.5720\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0372 - accuracy: 0.9890 - val_loss: 2.9590 - val_accuracy: 0.5680\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.0382 - accuracy: 0.9870 - val_loss: 2.9588 - val_accuracy: 0.5660\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0413 - accuracy: 0.9865 - val_loss: 2.9654 - val_accuracy: 0.5680\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0455 - accuracy: 0.9875 - val_loss: 2.9524 - val_accuracy: 0.5620\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0461 - accuracy: 0.9840 - val_loss: 2.9552 - val_accuracy: 0.5660\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0342 - accuracy: 0.9905 - val_loss: 2.9360 - val_accuracy: 0.5640\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0351 - accuracy: 0.9895 - val_loss: 2.9348 - val_accuracy: 0.5700\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0347 - accuracy: 0.9910 - val_loss: 2.9351 - val_accuracy: 0.5620\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0356 - accuracy: 0.9885 - val_loss: 2.9558 - val_accuracy: 0.5660\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0310 - accuracy: 0.9900 - val_loss: 2.9592 - val_accuracy: 0.5680\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 490us/step - loss: 0.0561 - accuracy: 0.9850 - val_loss: 2.9515 - val_accuracy: 0.5660\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 425us/step - loss: 0.0430 - accuracy: 0.9865 - val_loss: 2.9625 - val_accuracy: 0.5680\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 459us/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 2.9615 - val_accuracy: 0.5640\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 451us/step - loss: 0.0484 - accuracy: 0.9900 - val_loss: 2.9711 - val_accuracy: 0.5700\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0323 - accuracy: 0.9910 - val_loss: 2.9686 - val_accuracy: 0.5680\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0364 - accuracy: 0.9885 - val_loss: 2.9811 - val_accuracy: 0.5680\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0402 - accuracy: 0.9865 - val_loss: 2.9727 - val_accuracy: 0.5660\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0385 - accuracy: 0.9895 - val_loss: 2.9777 - val_accuracy: 0.5660\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0366 - accuracy: 0.9865 - val_loss: 2.9627 - val_accuracy: 0.5660\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0350 - accuracy: 0.9885 - val_loss: 2.9656 - val_accuracy: 0.5640\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0428 - accuracy: 0.9870 - val_loss: 2.9784 - val_accuracy: 0.5660\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0310 - accuracy: 0.9920 - val_loss: 2.9898 - val_accuracy: 0.5680\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0330 - accuracy: 0.9890 - val_loss: 2.9817 - val_accuracy: 0.5660\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0392 - accuracy: 0.9900 - val_loss: 2.9739 - val_accuracy: 0.5680\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0569 - accuracy: 0.9820 - val_loss: 2.9736 - val_accuracy: 0.5660\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0365 - accuracy: 0.9875 - val_loss: 2.9803 - val_accuracy: 0.5680\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0276 - accuracy: 0.9895 - val_loss: 2.9629 - val_accuracy: 0.5680\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0261 - accuracy: 0.9915 - val_loss: 2.9634 - val_accuracy: 0.5660\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0325 - accuracy: 0.9895 - val_loss: 2.9555 - val_accuracy: 0.5660\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0360 - accuracy: 0.9905 - val_loss: 2.9581 - val_accuracy: 0.5700\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0417 - accuracy: 0.9895 - val_loss: 2.9589 - val_accuracy: 0.5660\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 417us/step - loss: 0.0299 - accuracy: 0.9915 - val_loss: 2.9567 - val_accuracy: 0.5660\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 437us/step - loss: 0.0375 - accuracy: 0.9895 - val_loss: 2.9711 - val_accuracy: 0.5680\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0432 - accuracy: 0.9870 - val_loss: 2.9638 - val_accuracy: 0.5620\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0353 - accuracy: 0.9865 - val_loss: 2.9583 - val_accuracy: 0.5660\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 2.9684 - val_accuracy: 0.5680\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0356 - accuracy: 0.9910 - val_loss: 2.9630 - val_accuracy: 0.5660\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0469 - accuracy: 0.9860 - val_loss: 2.9623 - val_accuracy: 0.5680\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0332 - accuracy: 0.9885 - val_loss: 2.9721 - val_accuracy: 0.5660\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0410 - accuracy: 0.9885 - val_loss: 2.9776 - val_accuracy: 0.5700\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0415 - accuracy: 0.9845 - val_loss: 2.9592 - val_accuracy: 0.5680\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0354 - accuracy: 0.9890 - val_loss: 2.9689 - val_accuracy: 0.5660\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0288 - accuracy: 0.9930 - val_loss: 2.9828 - val_accuracy: 0.5680\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0424 - accuracy: 0.9875 - val_loss: 2.9910 - val_accuracy: 0.5680\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0275 - accuracy: 0.9915 - val_loss: 2.9903 - val_accuracy: 0.5720\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0368 - accuracy: 0.9900 - val_loss: 2.9724 - val_accuracy: 0.5680\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0273 - accuracy: 0.9925 - val_loss: 2.9750 - val_accuracy: 0.5680\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0293 - accuracy: 0.9895 - val_loss: 2.9855 - val_accuracy: 0.5700\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0453 - accuracy: 0.9875 - val_loss: 2.9671 - val_accuracy: 0.5740\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0476 - accuracy: 0.9890 - val_loss: 2.9762 - val_accuracy: 0.5700\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0532 - accuracy: 0.9880 - val_loss: 2.9753 - val_accuracy: 0.5660\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0382 - accuracy: 0.9895 - val_loss: 2.9564 - val_accuracy: 0.5720\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0253 - accuracy: 0.9940 - val_loss: 2.9781 - val_accuracy: 0.5720\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0469 - accuracy: 0.9870 - val_loss: 2.9639 - val_accuracy: 0.5640\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0270 - accuracy: 0.9925 - val_loss: 2.9714 - val_accuracy: 0.5660\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0455 - accuracy: 0.9855 - val_loss: 2.9668 - val_accuracy: 0.5660\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0421 - accuracy: 0.9875 - val_loss: 2.9714 - val_accuracy: 0.5700\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 2.9655 - val_accuracy: 0.5700\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 2.9695 - val_accuracy: 0.5740\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0686 - accuracy: 0.9835 - val_loss: 2.9610 - val_accuracy: 0.5640\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0501 - accuracy: 0.9855 - val_loss: 2.9586 - val_accuracy: 0.5680\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0376 - accuracy: 0.9885 - val_loss: 2.9702 - val_accuracy: 0.5620\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0337 - accuracy: 0.9880 - val_loss: 2.9612 - val_accuracy: 0.5680\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0539 - accuracy: 0.9855 - val_loss: 2.9667 - val_accuracy: 0.5640\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0459 - accuracy: 0.9895 - val_loss: 2.9720 - val_accuracy: 0.5700\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0333 - accuracy: 0.9895 - val_loss: 2.9764 - val_accuracy: 0.5680\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0364 - accuracy: 0.9895 - val_loss: 2.9593 - val_accuracy: 0.5660\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0391 - accuracy: 0.9845 - val_loss: 2.9772 - val_accuracy: 0.5660\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0509 - accuracy: 0.9830 - val_loss: 2.9697 - val_accuracy: 0.5700\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0418 - accuracy: 0.9860 - val_loss: 2.9523 - val_accuracy: 0.5640\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0335 - accuracy: 0.9925 - val_loss: 2.9832 - val_accuracy: 0.5660\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 445us/step - loss: 0.0488 - accuracy: 0.9835 - val_loss: 2.9687 - val_accuracy: 0.5640\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 431us/step - loss: 0.0375 - accuracy: 0.9865 - val_loss: 2.9654 - val_accuracy: 0.5660\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 404us/step - loss: 0.0290 - accuracy: 0.9910 - val_loss: 2.9716 - val_accuracy: 0.5620\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0434 - accuracy: 0.9870 - val_loss: 2.9639 - val_accuracy: 0.5640\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0365 - accuracy: 0.9885 - val_loss: 2.9698 - val_accuracy: 0.5640\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0400 - accuracy: 0.9870 - val_loss: 2.9684 - val_accuracy: 0.5680\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 2.9609 - val_accuracy: 0.5680\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0385 - accuracy: 0.9905 - val_loss: 2.9674 - val_accuracy: 0.5660\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0439 - accuracy: 0.9870 - val_loss: 2.9805 - val_accuracy: 0.5660\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0395 - accuracy: 0.9865 - val_loss: 2.9809 - val_accuracy: 0.5660\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0339 - accuracy: 0.9890 - val_loss: 2.9807 - val_accuracy: 0.5680\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0481 - accuracy: 0.9850 - val_loss: 2.9829 - val_accuracy: 0.5700\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 2.9823 - val_accuracy: 0.5680\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0423 - accuracy: 0.9875 - val_loss: 2.9735 - val_accuracy: 0.5680\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0229 - accuracy: 0.9935 - val_loss: 2.9738 - val_accuracy: 0.5660\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0433 - accuracy: 0.9870 - val_loss: 2.9718 - val_accuracy: 0.5660\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0450 - accuracy: 0.9845 - val_loss: 2.9754 - val_accuracy: 0.5660\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 2.9826 - val_accuracy: 0.5660\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0384 - accuracy: 0.9840 - val_loss: 2.9564 - val_accuracy: 0.5660\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0350 - accuracy: 0.9905 - val_loss: 2.9865 - val_accuracy: 0.5660\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0342 - accuracy: 0.9885 - val_loss: 2.9755 - val_accuracy: 0.5660\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0385 - accuracy: 0.9915 - val_loss: 2.9979 - val_accuracy: 0.5680\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 441us/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 2.9897 - val_accuracy: 0.5720\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0464 - accuracy: 0.9855 - val_loss: 2.9669 - val_accuracy: 0.5640\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0311 - accuracy: 0.9880 - val_loss: 2.9596 - val_accuracy: 0.5660\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 2.9786 - val_accuracy: 0.5660\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0302 - accuracy: 0.9925 - val_loss: 2.9774 - val_accuracy: 0.5640\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0557 - accuracy: 0.9855 - val_loss: 2.9648 - val_accuracy: 0.5660\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0367 - accuracy: 0.9885 - val_loss: 2.9717 - val_accuracy: 0.5680\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0364 - accuracy: 0.9880 - val_loss: 2.9821 - val_accuracy: 0.5620\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 425us/step - loss: 0.0339 - accuracy: 0.9900 - val_loss: 2.9680 - val_accuracy: 0.5620\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 420us/step - loss: 0.0245 - accuracy: 0.9935 - val_loss: 2.9802 - val_accuracy: 0.5640\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0440 - accuracy: 0.9890 - val_loss: 2.9796 - val_accuracy: 0.5660\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0372 - accuracy: 0.9875 - val_loss: 2.9851 - val_accuracy: 0.5660\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0360 - accuracy: 0.9880 - val_loss: 2.9868 - val_accuracy: 0.5660\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0342 - accuracy: 0.9910 - val_loss: 2.9961 - val_accuracy: 0.5680\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0337 - accuracy: 0.9870 - val_loss: 2.9727 - val_accuracy: 0.5680\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0299 - accuracy: 0.9895 - val_loss: 2.9761 - val_accuracy: 0.5660\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0442 - accuracy: 0.9870 - val_loss: 2.9805 - val_accuracy: 0.5660\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 446us/step - loss: 0.0371 - accuracy: 0.9890 - val_loss: 2.9881 - val_accuracy: 0.5660\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0364 - accuracy: 0.9890 - val_loss: 2.9841 - val_accuracy: 0.5660\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0292 - accuracy: 0.9910 - val_loss: 2.9840 - val_accuracy: 0.5660\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0349 - accuracy: 0.9865 - val_loss: 2.9714 - val_accuracy: 0.5660\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0347 - accuracy: 0.9900 - val_loss: 2.9910 - val_accuracy: 0.5660\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 2.9859 - val_accuracy: 0.5640\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 2.9813 - val_accuracy: 0.5660\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0482 - accuracy: 0.9850 - val_loss: 2.9716 - val_accuracy: 0.5660\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 2.9784 - val_accuracy: 0.5660\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0338 - accuracy: 0.9910 - val_loss: 2.9973 - val_accuracy: 0.5660\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0308 - accuracy: 0.9910 - val_loss: 2.9755 - val_accuracy: 0.5660\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0320 - accuracy: 0.9925 - val_loss: 2.9790 - val_accuracy: 0.5660\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0348 - accuracy: 0.9875 - val_loss: 2.9764 - val_accuracy: 0.5660\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0303 - accuracy: 0.9915 - val_loss: 2.9613 - val_accuracy: 0.5700\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0455 - accuracy: 0.9910 - val_loss: 2.9700 - val_accuracy: 0.5680\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0457 - accuracy: 0.9845 - val_loss: 2.9739 - val_accuracy: 0.5680\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 411us/step - loss: 0.0461 - accuracy: 0.9895 - val_loss: 2.9772 - val_accuracy: 0.5660\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0273 - accuracy: 0.9930 - val_loss: 2.9859 - val_accuracy: 0.5660\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 2.9877 - val_accuracy: 0.5640\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0448 - accuracy: 0.9920 - val_loss: 2.9720 - val_accuracy: 0.5620\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0374 - accuracy: 0.9900 - val_loss: 2.9846 - val_accuracy: 0.5660\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0276 - accuracy: 0.9900 - val_loss: 2.9847 - val_accuracy: 0.5640\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 421us/step - loss: 0.0476 - accuracy: 0.9880 - val_loss: 2.9978 - val_accuracy: 0.5680\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 447us/step - loss: 0.0470 - accuracy: 0.9880 - val_loss: 2.9822 - val_accuracy: 0.5660\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0389 - accuracy: 0.9875 - val_loss: 2.9937 - val_accuracy: 0.5660\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 2.9733 - val_accuracy: 0.5640\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 2.9692 - val_accuracy: 0.5660\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0468 - accuracy: 0.9850 - val_loss: 2.9701 - val_accuracy: 0.5660\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0419 - accuracy: 0.9855 - val_loss: 2.9715 - val_accuracy: 0.5680\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0375 - accuracy: 0.9905 - val_loss: 2.9657 - val_accuracy: 0.5680\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0312 - accuracy: 0.9895 - val_loss: 2.9747 - val_accuracy: 0.5700\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0334 - accuracy: 0.9880 - val_loss: 2.9573 - val_accuracy: 0.5640\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0343 - accuracy: 0.9900 - val_loss: 2.9723 - val_accuracy: 0.5680\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0305 - accuracy: 0.9890 - val_loss: 2.9854 - val_accuracy: 0.5660\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0284 - accuracy: 0.9930 - val_loss: 2.9762 - val_accuracy: 0.5640\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0348 - accuracy: 0.9910 - val_loss: 2.9628 - val_accuracy: 0.5640\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 2.9542 - val_accuracy: 0.5660\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0324 - accuracy: 0.9885 - val_loss: 2.9830 - val_accuracy: 0.5680\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0308 - accuracy: 0.9880 - val_loss: 2.9818 - val_accuracy: 0.5680\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0397 - accuracy: 0.9875 - val_loss: 2.9736 - val_accuracy: 0.5640\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 2.9897 - val_accuracy: 0.5680\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0316 - accuracy: 0.9900 - val_loss: 2.9585 - val_accuracy: 0.5640\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 445us/step - loss: 0.0358 - accuracy: 0.9930 - val_loss: 2.9755 - val_accuracy: 0.5660\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 443us/step - loss: 0.0295 - accuracy: 0.9900 - val_loss: 2.9641 - val_accuracy: 0.5660\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0354 - accuracy: 0.9870 - val_loss: 2.9740 - val_accuracy: 0.5660\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0292 - accuracy: 0.9905 - val_loss: 2.9634 - val_accuracy: 0.5660\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0313 - accuracy: 0.9890 - val_loss: 2.9616 - val_accuracy: 0.5640\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0358 - accuracy: 0.9880 - val_loss: 2.9962 - val_accuracy: 0.5660\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0447 - accuracy: 0.9845 - val_loss: 2.9984 - val_accuracy: 0.5660\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0395 - accuracy: 0.9895 - val_loss: 2.9771 - val_accuracy: 0.5680\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0374 - accuracy: 0.9885 - val_loss: 2.9711 - val_accuracy: 0.5660\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0505 - accuracy: 0.9870 - val_loss: 2.9901 - val_accuracy: 0.5660\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0349 - accuracy: 0.9890 - val_loss: 2.9899 - val_accuracy: 0.5660\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0340 - accuracy: 0.9895 - val_loss: 2.9814 - val_accuracy: 0.5680\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0335 - accuracy: 0.9875 - val_loss: 2.9889 - val_accuracy: 0.5640\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0415 - accuracy: 0.9885 - val_loss: 2.9847 - val_accuracy: 0.5640\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0392 - accuracy: 0.9870 - val_loss: 2.9846 - val_accuracy: 0.5640\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0304 - accuracy: 0.9915 - val_loss: 2.9847 - val_accuracy: 0.5680\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0382 - accuracy: 0.9895 - val_loss: 2.9806 - val_accuracy: 0.5640\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0279 - accuracy: 0.9925 - val_loss: 2.9923 - val_accuracy: 0.5660\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0213 - accuracy: 0.9925 - val_loss: 2.9732 - val_accuracy: 0.5660\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0382 - accuracy: 0.9855 - val_loss: 2.9792 - val_accuracy: 0.5660\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 447us/step - loss: 0.0347 - accuracy: 0.9880 - val_loss: 2.9835 - val_accuracy: 0.5640\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 424us/step - loss: 0.0340 - accuracy: 0.9905 - val_loss: 2.9704 - val_accuracy: 0.5660\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 2.9887 - val_accuracy: 0.5680\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0302 - accuracy: 0.9885 - val_loss: 2.9873 - val_accuracy: 0.5660\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0319 - accuracy: 0.9905 - val_loss: 2.9961 - val_accuracy: 0.5680\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0464 - accuracy: 0.9840 - val_loss: 2.9837 - val_accuracy: 0.5700\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0355 - accuracy: 0.9885 - val_loss: 2.9677 - val_accuracy: 0.5640\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0401 - accuracy: 0.9850 - val_loss: 2.9618 - val_accuracy: 0.5620\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0355 - accuracy: 0.9870 - val_loss: 2.9896 - val_accuracy: 0.5620\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0348 - accuracy: 0.9890 - val_loss: 2.9872 - val_accuracy: 0.5640\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 581us/step - loss: 0.0461 - accuracy: 0.9875 - val_loss: 3.1538 - val_accuracy: 0.5560\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0561 - accuracy: 0.9845 - val_loss: 3.1612 - val_accuracy: 0.5680\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0411 - accuracy: 0.9855 - val_loss: 3.2383 - val_accuracy: 0.5560\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0503 - accuracy: 0.9855 - val_loss: 3.2377 - val_accuracy: 0.5500\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0479 - accuracy: 0.9855 - val_loss: 3.2666 - val_accuracy: 0.5560\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0501 - accuracy: 0.9850 - val_loss: 3.2489 - val_accuracy: 0.5520\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0530 - accuracy: 0.9815 - val_loss: 3.1644 - val_accuracy: 0.5580\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0499 - accuracy: 0.9830 - val_loss: 3.1289 - val_accuracy: 0.5580\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0583 - accuracy: 0.9830 - val_loss: 3.1609 - val_accuracy: 0.5540\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 412us/step - loss: 0.0466 - accuracy: 0.9855 - val_loss: 3.1706 - val_accuracy: 0.5560\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 407us/step - loss: 0.0695 - accuracy: 0.9800 - val_loss: 3.1294 - val_accuracy: 0.5440\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0438 - accuracy: 0.9900 - val_loss: 3.0896 - val_accuracy: 0.5520\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 327us/step - loss: 0.0407 - accuracy: 0.9865 - val_loss: 3.0706 - val_accuracy: 0.5640\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.0491 - accuracy: 0.9855 - val_loss: 3.1040 - val_accuracy: 0.5620\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0724 - accuracy: 0.9800 - val_loss: 3.0459 - val_accuracy: 0.5580\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0497 - accuracy: 0.9840 - val_loss: 3.0278 - val_accuracy: 0.5540\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0556 - accuracy: 0.9835 - val_loss: 2.9882 - val_accuracy: 0.5640\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0421 - accuracy: 0.9895 - val_loss: 2.9814 - val_accuracy: 0.5640\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0438 - accuracy: 0.9885 - val_loss: 3.0270 - val_accuracy: 0.5560\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0547 - accuracy: 0.9800 - val_loss: 3.0294 - val_accuracy: 0.5600\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0434 - accuracy: 0.9870 - val_loss: 3.0567 - val_accuracy: 0.5540\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 326us/step - loss: 0.0396 - accuracy: 0.9880 - val_loss: 3.1049 - val_accuracy: 0.5480\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0458 - accuracy: 0.9870 - val_loss: 3.0962 - val_accuracy: 0.5500\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0408 - accuracy: 0.9885 - val_loss: 3.1283 - val_accuracy: 0.5600\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0480 - accuracy: 0.9865 - val_loss: 3.1519 - val_accuracy: 0.5640\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.0469 - accuracy: 0.9865 - val_loss: 3.1232 - val_accuracy: 0.5580\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 0.0453 - accuracy: 0.9840 - val_loss: 3.0848 - val_accuracy: 0.5680\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0338 - accuracy: 0.9870 - val_loss: 3.0857 - val_accuracy: 0.5680\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0502 - accuracy: 0.9815 - val_loss: 3.0578 - val_accuracy: 0.5660\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 3.0339 - val_accuracy: 0.5620\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0383 - accuracy: 0.9910 - val_loss: 3.0134 - val_accuracy: 0.5580\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0415 - accuracy: 0.9845 - val_loss: 3.0092 - val_accuracy: 0.5620\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0474 - accuracy: 0.9870 - val_loss: 2.9978 - val_accuracy: 0.5640\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0428 - accuracy: 0.9880 - val_loss: 3.0256 - val_accuracy: 0.5660\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.0469 - accuracy: 0.9840 - val_loss: 3.0574 - val_accuracy: 0.5600\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.0486 - accuracy: 0.9850 - val_loss: 3.0944 - val_accuracy: 0.5540\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0311 - accuracy: 0.9890 - val_loss: 3.1225 - val_accuracy: 0.5560\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0610 - accuracy: 0.9790 - val_loss: 3.0521 - val_accuracy: 0.5600\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0440 - accuracy: 0.9860 - val_loss: 3.0358 - val_accuracy: 0.5620\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 327us/step - loss: 0.0641 - accuracy: 0.9845 - val_loss: 3.0420 - val_accuracy: 0.5600\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0362 - accuracy: 0.9885 - val_loss: 3.0738 - val_accuracy: 0.5600\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0458 - accuracy: 0.9860 - val_loss: 3.0879 - val_accuracy: 0.5680\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 346us/step - loss: 0.0479 - accuracy: 0.9850 - val_loss: 3.0342 - val_accuracy: 0.5620\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0411 - accuracy: 0.9870 - val_loss: 3.0271 - val_accuracy: 0.5640\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.0487 - accuracy: 0.9900 - val_loss: 3.0152 - val_accuracy: 0.5660\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0450 - accuracy: 0.9850 - val_loss: 3.0141 - val_accuracy: 0.5660\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 3.0000 - val_accuracy: 0.5620\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 333us/step - loss: 0.0436 - accuracy: 0.9855 - val_loss: 3.0206 - val_accuracy: 0.5520\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 3.0107 - val_accuracy: 0.5480\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0381 - accuracy: 0.9895 - val_loss: 2.9800 - val_accuracy: 0.5460\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0495 - accuracy: 0.9870 - val_loss: 3.0512 - val_accuracy: 0.5500\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 343us/step - loss: 0.0497 - accuracy: 0.9855 - val_loss: 3.0802 - val_accuracy: 0.5640\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0565 - accuracy: 0.9835 - val_loss: 3.1218 - val_accuracy: 0.5700\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0512 - accuracy: 0.9850 - val_loss: 3.1475 - val_accuracy: 0.5600\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 416us/step - loss: 0.0379 - accuracy: 0.9910 - val_loss: 3.0998 - val_accuracy: 0.5580\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 3.0681 - val_accuracy: 0.5700\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0235 - accuracy: 0.9920 - val_loss: 3.0738 - val_accuracy: 0.5600\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0505 - accuracy: 0.9820 - val_loss: 3.1133 - val_accuracy: 0.5620\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0450 - accuracy: 0.9850 - val_loss: 3.1196 - val_accuracy: 0.5680\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 3.1617 - val_accuracy: 0.5680\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0478 - accuracy: 0.9875 - val_loss: 3.1561 - val_accuracy: 0.5640\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0319 - accuracy: 0.9895 - val_loss: 3.1158 - val_accuracy: 0.5620\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0326 - accuracy: 0.9875 - val_loss: 3.0613 - val_accuracy: 0.5660\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0439 - accuracy: 0.9870 - val_loss: 3.0586 - val_accuracy: 0.5620\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0536 - accuracy: 0.9840 - val_loss: 3.0018 - val_accuracy: 0.5660\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0575 - accuracy: 0.9850 - val_loss: 2.9547 - val_accuracy: 0.5680\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0467 - accuracy: 0.9875 - val_loss: 2.9867 - val_accuracy: 0.5640\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0441 - accuracy: 0.9890 - val_loss: 3.0012 - val_accuracy: 0.5640\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0484 - accuracy: 0.9850 - val_loss: 3.0124 - val_accuracy: 0.5500\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0435 - accuracy: 0.9880 - val_loss: 3.0623 - val_accuracy: 0.5580\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0321 - accuracy: 0.9915 - val_loss: 3.0860 - val_accuracy: 0.5540\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.0450 - accuracy: 0.9860 - val_loss: 3.0989 - val_accuracy: 0.5540\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0476 - accuracy: 0.9870 - val_loss: 3.1349 - val_accuracy: 0.5540\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0359 - accuracy: 0.9890 - val_loss: 3.1674 - val_accuracy: 0.5580\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0441 - accuracy: 0.9850 - val_loss: 3.1156 - val_accuracy: 0.5620\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0545 - accuracy: 0.9845 - val_loss: 3.0501 - val_accuracy: 0.5600\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0315 - accuracy: 0.9900 - val_loss: 3.0505 - val_accuracy: 0.5580\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0600 - accuracy: 0.9840 - val_loss: 3.0577 - val_accuracy: 0.5680\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0349 - accuracy: 0.9880 - val_loss: 3.0424 - val_accuracy: 0.5580\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0526 - accuracy: 0.9840 - val_loss: 2.9856 - val_accuracy: 0.5540\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0563 - accuracy: 0.9850 - val_loss: 2.9199 - val_accuracy: 0.5600\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0472 - accuracy: 0.9890 - val_loss: 2.9401 - val_accuracy: 0.5580\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0430 - accuracy: 0.9870 - val_loss: 2.8872 - val_accuracy: 0.5520\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 335us/step - loss: 0.0291 - accuracy: 0.9900 - val_loss: 2.8939 - val_accuracy: 0.5500\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0449 - accuracy: 0.9865 - val_loss: 2.8964 - val_accuracy: 0.5560\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0499 - accuracy: 0.9865 - val_loss: 2.9919 - val_accuracy: 0.5560\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0469 - accuracy: 0.9885 - val_loss: 2.9895 - val_accuracy: 0.5540\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 2.9633 - val_accuracy: 0.5580\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0544 - accuracy: 0.9805 - val_loss: 3.0109 - val_accuracy: 0.5640\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0525 - accuracy: 0.9855 - val_loss: 3.0234 - val_accuracy: 0.5660\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0487 - accuracy: 0.9870 - val_loss: 3.0575 - val_accuracy: 0.5620\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0441 - accuracy: 0.9895 - val_loss: 3.0574 - val_accuracy: 0.5640\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0374 - accuracy: 0.9880 - val_loss: 2.9897 - val_accuracy: 0.5680\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0498 - accuracy: 0.9870 - val_loss: 2.9534 - val_accuracy: 0.5620\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0493 - accuracy: 0.9860 - val_loss: 2.9687 - val_accuracy: 0.5620\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0567 - accuracy: 0.9850 - val_loss: 2.9692 - val_accuracy: 0.5640\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0358 - accuracy: 0.9880 - val_loss: 3.0164 - val_accuracy: 0.5680\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0405 - accuracy: 0.9895 - val_loss: 3.0127 - val_accuracy: 0.5640\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 406us/step - loss: 0.0365 - accuracy: 0.9885 - val_loss: 3.0307 - val_accuracy: 0.5500\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 3.0323 - val_accuracy: 0.5620\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0338 - accuracy: 0.9910 - val_loss: 3.0226 - val_accuracy: 0.5700\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0384 - accuracy: 0.9875 - val_loss: 2.9973 - val_accuracy: 0.5660\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0404 - accuracy: 0.9875 - val_loss: 2.9762 - val_accuracy: 0.5680\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 2.9857 - val_accuracy: 0.5720\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 345us/step - loss: 0.0295 - accuracy: 0.9900 - val_loss: 2.9802 - val_accuracy: 0.5740\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.0370 - accuracy: 0.9870 - val_loss: 2.9990 - val_accuracy: 0.5780\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 341us/step - loss: 0.0321 - accuracy: 0.9900 - val_loss: 3.0433 - val_accuracy: 0.5700\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0336 - accuracy: 0.9905 - val_loss: 3.0575 - val_accuracy: 0.5680\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0277 - accuracy: 0.9920 - val_loss: 3.1200 - val_accuracy: 0.5680\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.0340 - accuracy: 0.9890 - val_loss: 3.1318 - val_accuracy: 0.5780\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0336 - accuracy: 0.9895 - val_loss: 3.1077 - val_accuracy: 0.5740\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0331 - accuracy: 0.9890 - val_loss: 3.0462 - val_accuracy: 0.5780\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0296 - accuracy: 0.9930 - val_loss: 3.0304 - val_accuracy: 0.5760\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0425 - accuracy: 0.9870 - val_loss: 3.0187 - val_accuracy: 0.5700\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 340us/step - loss: 0.0339 - accuracy: 0.9910 - val_loss: 2.9856 - val_accuracy: 0.5680\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0314 - accuracy: 0.9905 - val_loss: 2.9845 - val_accuracy: 0.5720\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 379us/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 3.0078 - val_accuracy: 0.5640\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0373 - accuracy: 0.9935 - val_loss: 3.0264 - val_accuracy: 0.5660\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0383 - accuracy: 0.9920 - val_loss: 3.0251 - val_accuracy: 0.5680\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0380 - accuracy: 0.9885 - val_loss: 2.9807 - val_accuracy: 0.5680\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 3.0414 - val_accuracy: 0.5600\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0252 - accuracy: 0.9935 - val_loss: 3.1048 - val_accuracy: 0.5540\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0347 - accuracy: 0.9890 - val_loss: 3.1695 - val_accuracy: 0.5600\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0279 - accuracy: 0.9930 - val_loss: 3.2058 - val_accuracy: 0.5620\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0482 - accuracy: 0.9905 - val_loss: 3.1864 - val_accuracy: 0.5720\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0274 - accuracy: 0.9910 - val_loss: 3.1358 - val_accuracy: 0.5680\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0256 - accuracy: 0.9905 - val_loss: 3.0934 - val_accuracy: 0.5720\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 337us/step - loss: 0.0435 - accuracy: 0.9895 - val_loss: 3.0876 - val_accuracy: 0.5720\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0387 - accuracy: 0.9900 - val_loss: 3.0740 - val_accuracy: 0.5760\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0467 - accuracy: 0.9850 - val_loss: 3.1122 - val_accuracy: 0.5740\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 3.1643 - val_accuracy: 0.5700\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0433 - accuracy: 0.9875 - val_loss: 3.2135 - val_accuracy: 0.5620\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 410us/step - loss: 0.0305 - accuracy: 0.9905 - val_loss: 3.2080 - val_accuracy: 0.5680\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 3.1894 - val_accuracy: 0.5680\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0296 - accuracy: 0.9920 - val_loss: 3.1860 - val_accuracy: 0.5720\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0401 - accuracy: 0.9910 - val_loss: 3.2031 - val_accuracy: 0.5760\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 331us/step - loss: 0.0250 - accuracy: 0.9925 - val_loss: 3.2490 - val_accuracy: 0.5720\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0422 - accuracy: 0.9890 - val_loss: 3.1837 - val_accuracy: 0.5820\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0348 - accuracy: 0.9895 - val_loss: 3.1911 - val_accuracy: 0.5760\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0438 - accuracy: 0.9880 - val_loss: 3.2212 - val_accuracy: 0.5760\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0370 - accuracy: 0.9905 - val_loss: 3.2319 - val_accuracy: 0.5740\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 3.2260 - val_accuracy: 0.5560\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 348us/step - loss: 0.0272 - accuracy: 0.9930 - val_loss: 3.2184 - val_accuracy: 0.5560\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0277 - accuracy: 0.9955 - val_loss: 3.2074 - val_accuracy: 0.5520\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0484 - accuracy: 0.9850 - val_loss: 3.2028 - val_accuracy: 0.5540\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 321us/step - loss: 0.0252 - accuracy: 0.9935 - val_loss: 3.2462 - val_accuracy: 0.5480\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0342 - accuracy: 0.9915 - val_loss: 3.2391 - val_accuracy: 0.5420\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.0394 - accuracy: 0.9880 - val_loss: 3.2315 - val_accuracy: 0.5480\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 317us/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 3.2159 - val_accuracy: 0.5440\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 334us/step - loss: 0.0264 - accuracy: 0.9930 - val_loss: 3.2124 - val_accuracy: 0.5560\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0380 - accuracy: 0.9860 - val_loss: 3.2043 - val_accuracy: 0.5500\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0310 - accuracy: 0.9905 - val_loss: 3.1854 - val_accuracy: 0.5560\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 3.1664 - val_accuracy: 0.5660\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0548 - accuracy: 0.9870 - val_loss: 3.1342 - val_accuracy: 0.5660\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.0283 - accuracy: 0.9925 - val_loss: 3.1674 - val_accuracy: 0.5540\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 326us/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 3.1432 - val_accuracy: 0.5500\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 349us/step - loss: 0.0334 - accuracy: 0.9900 - val_loss: 3.1161 - val_accuracy: 0.5600\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0215 - accuracy: 0.9930 - val_loss: 3.1378 - val_accuracy: 0.5600\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0403 - accuracy: 0.9880 - val_loss: 3.1358 - val_accuracy: 0.5600\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0294 - accuracy: 0.9925 - val_loss: 3.1085 - val_accuracy: 0.5640\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 349us/step - loss: 0.0491 - accuracy: 0.9885 - val_loss: 3.0932 - val_accuracy: 0.5660\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 318us/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 3.1222 - val_accuracy: 0.5540\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 3.1103 - val_accuracy: 0.5600\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0225 - accuracy: 0.9935 - val_loss: 3.1500 - val_accuracy: 0.5700\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0326 - accuracy: 0.9920 - val_loss: 3.2134 - val_accuracy: 0.5740\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 349us/step - loss: 0.0334 - accuracy: 0.9905 - val_loss: 3.2035 - val_accuracy: 0.5780\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0333 - accuracy: 0.9890 - val_loss: 3.2014 - val_accuracy: 0.5740\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 3.1574 - val_accuracy: 0.5760\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 3.1605 - val_accuracy: 0.5700\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0319 - accuracy: 0.9890 - val_loss: 3.1179 - val_accuracy: 0.5700\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 3.0324 - val_accuracy: 0.5720\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0418 - accuracy: 0.9895 - val_loss: 3.0177 - val_accuracy: 0.5700\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 321us/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 3.0237 - val_accuracy: 0.5720\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0415 - accuracy: 0.9890 - val_loss: 3.0063 - val_accuracy: 0.5700\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 3.0414 - val_accuracy: 0.5620\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0501 - accuracy: 0.9860 - val_loss: 3.0087 - val_accuracy: 0.5700\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 322us/step - loss: 0.0299 - accuracy: 0.9895 - val_loss: 3.0176 - val_accuracy: 0.5800\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 318us/step - loss: 0.0335 - accuracy: 0.9910 - val_loss: 3.0193 - val_accuracy: 0.5820\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 3.0935 - val_accuracy: 0.5680\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 3.1600 - val_accuracy: 0.5660\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 339us/step - loss: 0.0306 - accuracy: 0.9900 - val_loss: 3.1654 - val_accuracy: 0.5640\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.0368 - accuracy: 0.9920 - val_loss: 3.1468 - val_accuracy: 0.5720\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.0307 - accuracy: 0.9915 - val_loss: 3.1492 - val_accuracy: 0.5760\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0284 - accuracy: 0.9915 - val_loss: 3.1550 - val_accuracy: 0.5740\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0225 - accuracy: 0.9935 - val_loss: 3.2021 - val_accuracy: 0.5780\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0298 - accuracy: 0.9910 - val_loss: 3.2327 - val_accuracy: 0.5820\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0318 - accuracy: 0.9880 - val_loss: 3.2405 - val_accuracy: 0.5760\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0276 - accuracy: 0.9925 - val_loss: 3.2003 - val_accuracy: 0.5680\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0331 - accuracy: 0.9915 - val_loss: 3.2142 - val_accuracy: 0.5660\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0304 - accuracy: 0.9915 - val_loss: 3.1989 - val_accuracy: 0.5620\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 318us/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 3.2355 - val_accuracy: 0.5640\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 338us/step - loss: 0.0386 - accuracy: 0.9875 - val_loss: 3.3034 - val_accuracy: 0.5680\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0288 - accuracy: 0.9920 - val_loss: 3.3332 - val_accuracy: 0.5680\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0274 - accuracy: 0.9915 - val_loss: 3.3307 - val_accuracy: 0.5600\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 321us/step - loss: 0.0350 - accuracy: 0.9910 - val_loss: 3.3302 - val_accuracy: 0.5600\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 3.2972 - val_accuracy: 0.5600\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0357 - accuracy: 0.9915 - val_loss: 3.2825 - val_accuracy: 0.5560\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 3.2261 - val_accuracy: 0.5600\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 342us/step - loss: 0.0414 - accuracy: 0.9900 - val_loss: 3.1356 - val_accuracy: 0.5740\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.0306 - accuracy: 0.9885 - val_loss: 3.1084 - val_accuracy: 0.5680\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 1s 600us/step - loss: 0.0266 - accuracy: 0.9900 - val_loss: 3.1045 - val_accuracy: 0.5660\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0304 - accuracy: 0.9940 - val_loss: 3.1296 - val_accuracy: 0.5740\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 3.1418 - val_accuracy: 0.5700\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 3.1642 - val_accuracy: 0.5740\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0353 - accuracy: 0.9910 - val_loss: 3.1606 - val_accuracy: 0.5760\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0325 - accuracy: 0.9910 - val_loss: 3.1318 - val_accuracy: 0.5720\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 3.1202 - val_accuracy: 0.5780\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 1s 441us/step - loss: 0.0337 - accuracy: 0.9900 - val_loss: 3.1298 - val_accuracy: 0.5760\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 1s 409us/step - loss: 0.0313 - accuracy: 0.9925 - val_loss: 3.1013 - val_accuracy: 0.5720\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0296 - accuracy: 0.9905 - val_loss: 3.1162 - val_accuracy: 0.5740\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0232 - accuracy: 0.9940 - val_loss: 3.1151 - val_accuracy: 0.5720\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0276 - accuracy: 0.9930 - val_loss: 3.1077 - val_accuracy: 0.5700\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0254 - accuracy: 0.9925 - val_loss: 3.1181 - val_accuracy: 0.5720\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0298 - accuracy: 0.9925 - val_loss: 3.1188 - val_accuracy: 0.5720\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0310 - accuracy: 0.9905 - val_loss: 3.1196 - val_accuracy: 0.5700\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0485 - accuracy: 0.9895 - val_loss: 3.1352 - val_accuracy: 0.5680\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0242 - accuracy: 0.9930 - val_loss: 3.1360 - val_accuracy: 0.5680\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0357 - accuracy: 0.9910 - val_loss: 3.1076 - val_accuracy: 0.5740\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0342 - accuracy: 0.9920 - val_loss: 3.1254 - val_accuracy: 0.5720\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0221 - accuracy: 0.9935 - val_loss: 3.1230 - val_accuracy: 0.5720\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 3.1267 - val_accuracy: 0.5780\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0218 - accuracy: 0.9955 - val_loss: 3.1036 - val_accuracy: 0.5780\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0206 - accuracy: 0.9895 - val_loss: 3.1043 - val_accuracy: 0.5760\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0254 - accuracy: 0.9910 - val_loss: 3.0995 - val_accuracy: 0.5780\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0272 - accuracy: 0.9925 - val_loss: 3.1281 - val_accuracy: 0.5760\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 3.1069 - val_accuracy: 0.5740\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0193 - accuracy: 0.9945 - val_loss: 3.0892 - val_accuracy: 0.5740\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 1s 439us/step - loss: 0.0238 - accuracy: 0.9935 - val_loss: 3.1040 - val_accuracy: 0.5760\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0182 - accuracy: 0.9935 - val_loss: 3.1182 - val_accuracy: 0.5800\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 1s 432us/step - loss: 0.0363 - accuracy: 0.9885 - val_loss: 3.1427 - val_accuracy: 0.5820\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0119 - accuracy: 0.9970 - val_loss: 3.1320 - val_accuracy: 0.5800\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0205 - accuracy: 0.9945 - val_loss: 3.1069 - val_accuracy: 0.5740\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0296 - accuracy: 0.9930 - val_loss: 3.1051 - val_accuracy: 0.5840\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0301 - accuracy: 0.9900 - val_loss: 3.1072 - val_accuracy: 0.5800\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0493 - accuracy: 0.9870 - val_loss: 3.0915 - val_accuracy: 0.5760\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0355 - accuracy: 0.9895 - val_loss: 3.1353 - val_accuracy: 0.5720\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0224 - accuracy: 0.9945 - val_loss: 3.1044 - val_accuracy: 0.5800\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0138 - accuracy: 0.9975 - val_loss: 3.1311 - val_accuracy: 0.5800\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 3.0832 - val_accuracy: 0.5840\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 3.1112 - val_accuracy: 0.5800\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 3.1258 - val_accuracy: 0.5760\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0397 - accuracy: 0.9920 - val_loss: 3.0851 - val_accuracy: 0.5800\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 1s 381us/step - loss: 0.0283 - accuracy: 0.9935 - val_loss: 3.1207 - val_accuracy: 0.5820\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0198 - accuracy: 0.9940 - val_loss: 3.1208 - val_accuracy: 0.5800\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 1s 370us/step - loss: 0.0345 - accuracy: 0.9885 - val_loss: 3.1219 - val_accuracy: 0.5720\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 3.0885 - val_accuracy: 0.5840\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 3.1041 - val_accuracy: 0.5780\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 3.1055 - val_accuracy: 0.5760\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0222 - accuracy: 0.9915 - val_loss: 3.1236 - val_accuracy: 0.5780\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 1s 437us/step - loss: 0.0199 - accuracy: 0.9955 - val_loss: 3.1352 - val_accuracy: 0.5800\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0273 - accuracy: 0.9935 - val_loss: 3.1324 - val_accuracy: 0.5800\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 3.1298 - val_accuracy: 0.5800\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0157 - accuracy: 0.9960 - val_loss: 3.1412 - val_accuracy: 0.5800\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0197 - accuracy: 0.9940 - val_loss: 3.1601 - val_accuracy: 0.5800\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0388 - accuracy: 0.9895 - val_loss: 3.1299 - val_accuracy: 0.5840\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0198 - accuracy: 0.9945 - val_loss: 3.1303 - val_accuracy: 0.5720\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0394 - accuracy: 0.9905 - val_loss: 3.1232 - val_accuracy: 0.5740\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0244 - accuracy: 0.9935 - val_loss: 3.1308 - val_accuracy: 0.5720\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0291 - accuracy: 0.9920 - val_loss: 3.1115 - val_accuracy: 0.5660\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 3.1001 - val_accuracy: 0.5680\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0228 - accuracy: 0.9935 - val_loss: 3.0995 - val_accuracy: 0.5700\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0130 - accuracy: 0.9960 - val_loss: 3.1217 - val_accuracy: 0.5700\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0196 - accuracy: 0.9965 - val_loss: 3.1471 - val_accuracy: 0.5720\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0234 - accuracy: 0.9950 - val_loss: 3.1334 - val_accuracy: 0.5740\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0181 - accuracy: 0.9950 - val_loss: 3.1355 - val_accuracy: 0.5760\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0269 - accuracy: 0.9910 - val_loss: 3.1174 - val_accuracy: 0.5760\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 3.1054 - val_accuracy: 0.5780\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0355 - accuracy: 0.9865 - val_loss: 3.1549 - val_accuracy: 0.5720\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 1s 422us/step - loss: 0.0240 - accuracy: 0.9945 - val_loss: 3.1650 - val_accuracy: 0.5720\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 3.1404 - val_accuracy: 0.5680\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0228 - accuracy: 0.9955 - val_loss: 3.1432 - val_accuracy: 0.5700\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 1s 423us/step - loss: 0.0175 - accuracy: 0.9945 - val_loss: 3.1345 - val_accuracy: 0.5780\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 3.1293 - val_accuracy: 0.5760\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 3.1258 - val_accuracy: 0.5720\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 3.1440 - val_accuracy: 0.5680\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 1s 418us/step - loss: 0.0150 - accuracy: 0.9960 - val_loss: 3.1354 - val_accuracy: 0.5700\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0243 - accuracy: 0.9945 - val_loss: 3.1172 - val_accuracy: 0.5700\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0269 - accuracy: 0.9930 - val_loss: 3.1452 - val_accuracy: 0.5680\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0248 - accuracy: 0.9920 - val_loss: 3.1244 - val_accuracy: 0.5680\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0256 - accuracy: 0.9930 - val_loss: 3.1363 - val_accuracy: 0.5700\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0241 - accuracy: 0.9920 - val_loss: 3.1440 - val_accuracy: 0.5700\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0174 - accuracy: 0.9935 - val_loss: 3.1722 - val_accuracy: 0.5740\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0197 - accuracy: 0.9950 - val_loss: 3.1695 - val_accuracy: 0.5740\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0218 - accuracy: 0.9925 - val_loss: 3.1737 - val_accuracy: 0.5700\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0190 - accuracy: 0.9955 - val_loss: 3.1446 - val_accuracy: 0.5660\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0159 - accuracy: 0.9955 - val_loss: 3.1334 - val_accuracy: 0.5680\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0221 - accuracy: 0.9950 - val_loss: 3.1805 - val_accuracy: 0.5680\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0241 - accuracy: 0.9935 - val_loss: 3.1440 - val_accuracy: 0.5740\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0269 - accuracy: 0.9910 - val_loss: 3.1810 - val_accuracy: 0.5760\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0224 - accuracy: 0.9960 - val_loss: 3.1591 - val_accuracy: 0.5740\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0217 - accuracy: 0.9935 - val_loss: 3.1344 - val_accuracy: 0.5740\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 3.1187 - val_accuracy: 0.5760\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0138 - accuracy: 0.9955 - val_loss: 3.1410 - val_accuracy: 0.5740\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 3.1458 - val_accuracy: 0.5720\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0259 - accuracy: 0.9960 - val_loss: 3.1515 - val_accuracy: 0.5760\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 3.1477 - val_accuracy: 0.5740\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 3.1327 - val_accuracy: 0.5720\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0136 - accuracy: 0.9940 - val_loss: 3.1704 - val_accuracy: 0.5700\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0218 - accuracy: 0.9935 - val_loss: 3.1682 - val_accuracy: 0.5700\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0145 - accuracy: 0.9935 - val_loss: 3.1934 - val_accuracy: 0.5720\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 3.2006 - val_accuracy: 0.5680\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0160 - accuracy: 0.9955 - val_loss: 3.2093 - val_accuracy: 0.5720\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0212 - accuracy: 0.9945 - val_loss: 3.1955 - val_accuracy: 0.5740\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0166 - accuracy: 0.9935 - val_loss: 3.2002 - val_accuracy: 0.5680\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 3.1770 - val_accuracy: 0.5700\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 3.1740 - val_accuracy: 0.5720\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0204 - accuracy: 0.9935 - val_loss: 3.1840 - val_accuracy: 0.5700\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 3.1640 - val_accuracy: 0.5760\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0131 - accuracy: 0.9965 - val_loss: 3.1888 - val_accuracy: 0.5720\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0248 - accuracy: 0.9945 - val_loss: 3.1900 - val_accuracy: 0.5740\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0259 - accuracy: 0.9925 - val_loss: 3.1804 - val_accuracy: 0.5780\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 1s 434us/step - loss: 0.0200 - accuracy: 0.9950 - val_loss: 3.2032 - val_accuracy: 0.5720\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 1s 419us/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 3.1903 - val_accuracy: 0.5720\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 1s 375us/step - loss: 0.0152 - accuracy: 0.9935 - val_loss: 3.1760 - val_accuracy: 0.5780\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0173 - accuracy: 0.9965 - val_loss: 3.1913 - val_accuracy: 0.5700\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0217 - accuracy: 0.9940 - val_loss: 3.2082 - val_accuracy: 0.5680\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 3.2193 - val_accuracy: 0.5680\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0214 - accuracy: 0.9935 - val_loss: 3.2329 - val_accuracy: 0.5680\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0256 - accuracy: 0.9940 - val_loss: 3.2300 - val_accuracy: 0.5700\n",
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0150 - accuracy: 0.9965 - val_loss: 3.2157 - val_accuracy: 0.5700\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0175 - accuracy: 0.9965 - val_loss: 3.2060 - val_accuracy: 0.5660\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0087 - accuracy: 0.9990 - val_loss: 3.2477 - val_accuracy: 0.5700\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0258 - accuracy: 0.9910 - val_loss: 3.2263 - val_accuracy: 0.5760\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 3.2585 - val_accuracy: 0.5740\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0223 - accuracy: 0.9955 - val_loss: 3.2183 - val_accuracy: 0.5740\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 3.2111 - val_accuracy: 0.5780\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0218 - accuracy: 0.9950 - val_loss: 3.2211 - val_accuracy: 0.5760\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0252 - accuracy: 0.9915 - val_loss: 3.2525 - val_accuracy: 0.5760\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0237 - accuracy: 0.9965 - val_loss: 3.2514 - val_accuracy: 0.5720\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0173 - accuracy: 0.9935 - val_loss: 3.2333 - val_accuracy: 0.5780\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0288 - accuracy: 0.9940 - val_loss: 3.2413 - val_accuracy: 0.5760\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 1s 430us/step - loss: 0.0174 - accuracy: 0.9950 - val_loss: 3.2136 - val_accuracy: 0.5760\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 1s 427us/step - loss: 0.0268 - accuracy: 0.9935 - val_loss: 3.2364 - val_accuracy: 0.5760\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 1s 433us/step - loss: 0.0164 - accuracy: 0.9940 - val_loss: 3.2482 - val_accuracy: 0.5780\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 0.0158 - accuracy: 0.9960 - val_loss: 3.2383 - val_accuracy: 0.5760\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 3.2058 - val_accuracy: 0.5800\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0134 - accuracy: 0.9955 - val_loss: 3.2417 - val_accuracy: 0.5820\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0180 - accuracy: 0.9955 - val_loss: 3.2322 - val_accuracy: 0.5760\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 1s 393us/step - loss: 0.0187 - accuracy: 0.9960 - val_loss: 3.2109 - val_accuracy: 0.5800\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0158 - accuracy: 0.9945 - val_loss: 3.2500 - val_accuracy: 0.5780\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0195 - accuracy: 0.9935 - val_loss: 3.2349 - val_accuracy: 0.5780\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0198 - accuracy: 0.9965 - val_loss: 3.2588 - val_accuracy: 0.5780\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 3.2510 - val_accuracy: 0.5740\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0264 - accuracy: 0.9925 - val_loss: 3.2235 - val_accuracy: 0.5760\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0212 - accuracy: 0.9950 - val_loss: 3.2469 - val_accuracy: 0.5720\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 1s 382us/step - loss: 0.0113 - accuracy: 0.9965 - val_loss: 3.2495 - val_accuracy: 0.5700\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 1s 390us/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 3.2116 - val_accuracy: 0.5740\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0249 - accuracy: 0.9940 - val_loss: 3.2263 - val_accuracy: 0.5740\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 3.2306 - val_accuracy: 0.5720\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 1s 380us/step - loss: 0.0163 - accuracy: 0.9965 - val_loss: 3.2330 - val_accuracy: 0.5660\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0230 - accuracy: 0.9925 - val_loss: 3.2366 - val_accuracy: 0.5680\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 3.2777 - val_accuracy: 0.5640\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 1s 399us/step - loss: 0.0128 - accuracy: 0.9955 - val_loss: 3.2861 - val_accuracy: 0.5620\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 3.2741 - val_accuracy: 0.5660\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0222 - accuracy: 0.9940 - val_loss: 3.2458 - val_accuracy: 0.5720\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 3.2215 - val_accuracy: 0.5740\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0185 - accuracy: 0.9960 - val_loss: 3.2450 - val_accuracy: 0.5720\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 3.2701 - val_accuracy: 0.5740\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0208 - accuracy: 0.9965 - val_loss: 3.2480 - val_accuracy: 0.5740\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0146 - accuracy: 0.9965 - val_loss: 3.2502 - val_accuracy: 0.5720\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0175 - accuracy: 0.9950 - val_loss: 3.2565 - val_accuracy: 0.5700\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0135 - accuracy: 0.9960 - val_loss: 3.2667 - val_accuracy: 0.5700\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 1s 357us/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 3.2555 - val_accuracy: 0.5720\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0284 - accuracy: 0.9905 - val_loss: 3.2398 - val_accuracy: 0.5740\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0239 - accuracy: 0.9935 - val_loss: 3.2372 - val_accuracy: 0.5740\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0210 - accuracy: 0.9945 - val_loss: 3.2445 - val_accuracy: 0.5740\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0113 - accuracy: 0.9955 - val_loss: 3.2586 - val_accuracy: 0.5740\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.0155 - accuracy: 0.9955 - val_loss: 3.2277 - val_accuracy: 0.5720\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 3.2281 - val_accuracy: 0.5760\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 3.2454 - val_accuracy: 0.5760\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 3.2496 - val_accuracy: 0.5720\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 3.2553 - val_accuracy: 0.5720\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 3.2285 - val_accuracy: 0.5700\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 1s 408us/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 3.2273 - val_accuracy: 0.5680\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 1s 429us/step - loss: 0.0173 - accuracy: 0.9960 - val_loss: 3.2379 - val_accuracy: 0.5720\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0252 - accuracy: 0.9925 - val_loss: 3.2716 - val_accuracy: 0.5680\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0271 - accuracy: 0.9940 - val_loss: 3.2581 - val_accuracy: 0.5660\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 3.2135 - val_accuracy: 0.5680\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0140 - accuracy: 0.9945 - val_loss: 3.2377 - val_accuracy: 0.5680\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.0234 - accuracy: 0.9925 - val_loss: 3.2271 - val_accuracy: 0.5700\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.0146 - accuracy: 0.9960 - val_loss: 3.2679 - val_accuracy: 0.5700\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 1s 378us/step - loss: 0.0289 - accuracy: 0.9935 - val_loss: 3.2660 - val_accuracy: 0.5660\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0177 - accuracy: 0.9955 - val_loss: 3.2620 - val_accuracy: 0.5680\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0145 - accuracy: 0.9945 - val_loss: 3.2575 - val_accuracy: 0.5680\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 1s 352us/step - loss: 0.0257 - accuracy: 0.9920 - val_loss: 3.2675 - val_accuracy: 0.5660\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.0268 - accuracy: 0.9925 - val_loss: 3.2669 - val_accuracy: 0.5640\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0272 - accuracy: 0.9940 - val_loss: 3.2437 - val_accuracy: 0.5600\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 1s 376us/step - loss: 0.0206 - accuracy: 0.9930 - val_loss: 3.2521 - val_accuracy: 0.5600\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.0209 - accuracy: 0.9955 - val_loss: 3.2404 - val_accuracy: 0.5660\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0199 - accuracy: 0.9950 - val_loss: 3.2372 - val_accuracy: 0.5620\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 3.2578 - val_accuracy: 0.5600\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0213 - accuracy: 0.9945 - val_loss: 3.2307 - val_accuracy: 0.5620\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 3.2381 - val_accuracy: 0.5600\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 1s 405us/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 3.2283 - val_accuracy: 0.5580\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 1s 436us/step - loss: 0.0177 - accuracy: 0.9935 - val_loss: 3.2492 - val_accuracy: 0.5580\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 3.2380 - val_accuracy: 0.5600\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0099 - accuracy: 0.9960 - val_loss: 3.2222 - val_accuracy: 0.5600\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 1s 385us/step - loss: 0.0182 - accuracy: 0.9935 - val_loss: 3.2395 - val_accuracy: 0.5600\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0125 - accuracy: 0.9965 - val_loss: 3.2509 - val_accuracy: 0.5620\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0232 - accuracy: 0.9945 - val_loss: 3.2380 - val_accuracy: 0.5600\n",
      "Train on 2000 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 1s 598us/step - loss: 0.0266 - accuracy: 0.9940 - val_loss: 3.2320 - val_accuracy: 0.5660\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 1s 365us/step - loss: 0.0184 - accuracy: 0.9970 - val_loss: 3.2394 - val_accuracy: 0.5580\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0185 - accuracy: 0.9930 - val_loss: 3.2333 - val_accuracy: 0.5620\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 3.2293 - val_accuracy: 0.5640\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 3.2457 - val_accuracy: 0.5640\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 3.2369 - val_accuracy: 0.5640\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 3.2458 - val_accuracy: 0.5620\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0107 - accuracy: 0.9960 - val_loss: 3.2444 - val_accuracy: 0.5620\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0129 - accuracy: 0.9975 - val_loss: 3.2121 - val_accuracy: 0.5620\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0284 - accuracy: 0.9935 - val_loss: 3.2379 - val_accuracy: 0.5620\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 1s 355us/step - loss: 0.0147 - accuracy: 0.9945 - val_loss: 3.2571 - val_accuracy: 0.5600\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 3.2722 - val_accuracy: 0.5600\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 1s 428us/step - loss: 0.0132 - accuracy: 0.9970 - val_loss: 3.2245 - val_accuracy: 0.5640\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 1s 414us/step - loss: 0.0194 - accuracy: 0.9930 - val_loss: 3.2505 - val_accuracy: 0.5620\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0135 - accuracy: 0.9965 - val_loss: 3.2113 - val_accuracy: 0.5640\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 1s 391us/step - loss: 0.0174 - accuracy: 0.9935 - val_loss: 3.2201 - val_accuracy: 0.5620\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 3.2262 - val_accuracy: 0.5580\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 1s 377us/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 3.2649 - val_accuracy: 0.5600\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0200 - accuracy: 0.9945 - val_loss: 3.2290 - val_accuracy: 0.5620\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 3.2557 - val_accuracy: 0.5640\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0165 - accuracy: 0.9960 - val_loss: 3.2394 - val_accuracy: 0.5660\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 0.0148 - accuracy: 0.9955 - val_loss: 3.2671 - val_accuracy: 0.5600\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0251 - accuracy: 0.9925 - val_loss: 3.2666 - val_accuracy: 0.5640\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 0.0156 - accuracy: 0.9945 - val_loss: 3.2446 - val_accuracy: 0.5620\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0128 - accuracy: 0.9950 - val_loss: 3.2237 - val_accuracy: 0.5640\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0102 - accuracy: 0.9975 - val_loss: 3.2339 - val_accuracy: 0.5640\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0119 - accuracy: 0.9970 - val_loss: 3.2319 - val_accuracy: 0.5640\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 1s 367us/step - loss: 0.0179 - accuracy: 0.9945 - val_loss: 3.2386 - val_accuracy: 0.5620\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 1s 401us/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 3.2554 - val_accuracy: 0.5620\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.0178 - accuracy: 0.9960 - val_loss: 3.2378 - val_accuracy: 0.5640\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 3.2459 - val_accuracy: 0.5640\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.0225 - accuracy: 0.9940 - val_loss: 3.2385 - val_accuracy: 0.5640\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 1s 413us/step - loss: 0.0172 - accuracy: 0.9965 - val_loss: 3.2540 - val_accuracy: 0.5660\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 1s 426us/step - loss: 0.0133 - accuracy: 0.9960 - val_loss: 3.2591 - val_accuracy: 0.5640\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 1s 384us/step - loss: 0.0166 - accuracy: 0.9955 - val_loss: 3.2823 - val_accuracy: 0.5640\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 1s 398us/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 3.2419 - val_accuracy: 0.5640\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 3.2236 - val_accuracy: 0.5660\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 3.2348 - val_accuracy: 0.5620\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 1s 360us/step - loss: 0.0161 - accuracy: 0.9950 - val_loss: 3.2377 - val_accuracy: 0.5640\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 3.2463 - val_accuracy: 0.5620\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 1s 388us/step - loss: 0.0302 - accuracy: 0.9920 - val_loss: 3.2589 - val_accuracy: 0.5640\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 1s 386us/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 3.2383 - val_accuracy: 0.5620\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 1s 366us/step - loss: 0.0236 - accuracy: 0.9915 - val_loss: 3.2315 - val_accuracy: 0.5600\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 1s 359us/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 3.2211 - val_accuracy: 0.5620\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 3.2083 - val_accuracy: 0.5640\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.0131 - accuracy: 0.9945 - val_loss: 3.2333 - val_accuracy: 0.5640\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 1s 351us/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 3.2517 - val_accuracy: 0.5620\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 1s 358us/step - loss: 0.0179 - accuracy: 0.9950 - val_loss: 3.2434 - val_accuracy: 0.5640\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 1s 395us/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 3.2457 - val_accuracy: 0.5640\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.0218 - accuracy: 0.9935 - val_loss: 3.2457 - val_accuracy: 0.5620\n",
      "--- 856.139769077301 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "start_time = time.time()\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.01),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat, validation_data=[test_x, test_label_cat],epochs=50, batch_size=300)\n",
    "# print('lr=0.01, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.fit(train_x,train_label_cat,validation_data=[test_x,test_label_cat],epochs=200, batch_size=250)\n",
    "# print('lr=0.001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat,validation_data=[test_x,test_label_cat],epochs=200, batch_size=200)\n",
    "# print('lr=0.0001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.00001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat, validation_data=[test_x,test_label_cat],epochs=200, batch_size=200)\n",
    "# print('lr=0.000001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.fit(train_x,train_label_cat,validation_data=[test_x, test_label_cat],epochs=200, batch_size=250)\n",
    "# print('lr=0.001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.0001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat,validation_data=[test_x, test_label_cat],epochs=200, batch_size=200)\n",
    "# print('lr=0.0001, train complete')\n",
    "model.compile(loss='categorical_crossentropy',optimizer = Adam(lr=0.00001),metrics=['accuracy'])\n",
    "model.fit(train_x, train_label_cat, validation_data=[test_x, test_label_cat],epochs=50, batch_size=200)\n",
    "# print('lr=0.0001, train complete')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy Curves')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(model_nn.history['accuracy'],'r',linewidth=1.0)\n",
    "plt.legend(['Training Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 56.2 percent\n",
      "testing model takes 0.206 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred = model.predict(test_x)\n",
    "pred_list = [] \n",
    "for i in range(len(pred)):\n",
    "    arr = pred[i]\n",
    "    idx = np.argwhere(arr == np.max(arr))\n",
    "    pred_list.append(idx[0][0])\n",
    "tst_labl = np.argmax(test_label_cat, axis=-1)\n",
    "accuracy = accuracy_score(pred_list, tst_labl)\n",
    "print(\"Test accuracy is %s percent\" % round(accuracy*100,3))\n",
    "print(\"testing model takes %s seconds\" % round((time.time() - start),3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
